{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"","text":"<p>Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.</p> <p>Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties.</p> <p>Rook integrates deeply into cloud native environments leveraging extension points and providing a seamless experience for scheduling, lifecycle management, resource management, security, monitoring, and user experience.</p> <p>The Ceph operator was declared stable in December 2018 in the Rook v0.9 release, providing a production storage platform for several years already.</p>","title":"Rook"},{"location":"#quick-start-guide","text":"<p>Starting Ceph in your cluster is as simple as a few <code>kubectl</code> commands. See our Quickstart guide to get started with the Ceph operator!</p>","title":"Quick Start Guide"},{"location":"#designs","text":"<p>Ceph is a highly scalable distributed storage solution for block storage, object storage, and shared filesystems with years of production deployments. See the Ceph overview.</p> <p>For detailed design documentation, see also the design docs.</p>","title":"Designs"},{"location":"#need-help-be-sure-to-join-the-rook-slack","text":"<p>If you have any questions along the way, please don't hesitate to ask us in our Slack channel. You can sign up for our Slack here.</p>","title":"Need help? Be sure to join the Rook Slack"},{"location":"ceph-configuration/","text":"<p>For most any Ceph cluster, the user will want to--and may need to--change some Ceph configurations. These changes often may be warranted in order to alter performance to meet SLAs or to update default data resiliency settings.</p>  <p>WARNING: Modify Ceph settings carefully, and review the Ceph configuration documentation before making any changes. Changing the settings could result in unhealthy daemons or even data loss if used incorrectly.</p>","title":"Configuration"},{"location":"ceph-configuration/#required-configurations","text":"<p>Rook and Ceph both strive to make configuration as easy as possible, but there are some configuration options which users are well advised to consider for any production cluster.</p>","title":"Required configurations"},{"location":"ceph-configuration/#default-pg-and-pgp-counts","text":"<p>The number of PGs and PGPs can be configured on a per-pool basis, but it is highly advised to set default values that are appropriate for your Ceph cluster. Appropriate values depend on the number of OSDs the user expects to have backing each pool. The Ceph OSD and Pool config docs provide detailed information about how to tune these parameters: <code>osd_pool_default_pg_num</code> and <code>osd_pool_default_pgp_num</code>.</p> <p>Nautilus introduced the PG auto-scaler mgr module capable of automatically managing PG and PGP values for pools. Please see Ceph New in Nautilus: PG merging and autotuning for more information about this module.</p> <p>In Octopus (v15.2.x) and newer, module <code>pg_autoscaler</code> is enabled by default without the above-mentioned setting.</p> <p>To disable this module, in the CephCluster CR:</p> <pre><code>spec:\n  mgr:\n    modules:\n    - name: pg_autoscaler\n      enabled: false\n</code></pre>  <p>With that setting, the autoscaler will be enabled for all new pools. If you do not desire to have the autoscaler enabled for all new pools, you will need to use the Rook toolbox to enable the module and enable the autoscaling on individual pools.</p> <p>The autoscaler is not enabled for the existing pools after enabling the module. So if you want to enable the autoscaling for these existing pools, they must be configured from the toolbox.</p>","title":"Default PG and PGP counts"},{"location":"ceph-configuration/#specifying-configuration-options","text":"","title":"Specifying configuration options"},{"location":"ceph-configuration/#toolbox-ceph-cli","text":"<p>The most recommended way of configuring Ceph is to set Ceph's configuration directly. The first method for doing so is to use Ceph's CLI from the Rook-Ceph toolbox pod. Using the toolbox pod is detailed here. From the toolbox, the user can change Ceph configurations, enable manager modules, create users and pools, and much more.</p>","title":"Toolbox + Ceph CLI"},{"location":"ceph-configuration/#ceph-dashboard","text":"<p>The Ceph Dashboard, examined in more detail here, is another way of setting some of Ceph's configuration directly. Configuration by the Ceph dashboard is recommended with the same priority as configuration via the Ceph CLI (above).</p>","title":"Ceph Dashboard"},{"location":"ceph-configuration/#advanced-configuration-via-cephconf-override-configmap","text":"<p>Setting configs via Ceph's CLI requires that at least one mon be available for the configs to be set, and setting configs via dashboard requires at least one mgr to be available. Ceph may also have a small number of very advanced settings that aren't able to be modified easily via CLI or dashboard. The least recommended method for configuring Ceph is intended as a last-resort fallback in situations like these. This is covered in detail here.</p>","title":"Advanced configuration via ceph.conf override ConfigMap"},{"location":"CRDs/ceph-client-crd/","text":"<p>Rook allows creation and updating clients through the custom resource definitions (CRDs). For more information about user management and capabilities see the Ceph docs.</p>","title":"Ceph Client CRD"},{"location":"CRDs/ceph-client-crd/#use-case","text":"<p>Use Client CRD in case you want to integrate Rook with with applications that are using LibRBD directly. For example for OpenStack deployment with Ceph backend use Client CRD to create OpenStack services users.</p> <p>The Client CRD is not needed for Flex or CSI driver users. The drivers create the needed users automatically.</p>","title":"Use Case"},{"location":"CRDs/ceph-client-crd/#creating-ceph-user","text":"<p>To get you started, here is a simple example of a CRD to configure a Ceph client with capabilities.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephClient\nmetadata:\n  name: glance\n  namespace: rook-ceph\nspec:\n  caps:\n    mon: 'profile rbd'\n    osd: 'profile rbd pool=images'\n---\napiVersion: ceph.rook.io/v1\nkind: CephClient\nmetadata:\n  name: cinder\n  namespace: rook-ceph\nspec:\n  caps:\n    mon: 'profile rbd'\n    osd: 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images'\n</code></pre>","title":"Creating Ceph User"},{"location":"CRDs/ceph-client-crd/#prerequisites","text":"<p>This guide assumes you have created a Rook cluster as explained in the main Quickstart guide</p>","title":"Prerequisites"},{"location":"CRDs/ceph-cluster-crd/","text":"<p>Rook allows creation and customization of storage clusters through the custom resource definitions (CRDs). There are primarily three different modes in which to create your cluster.</p> <ol> <li>Specify host paths and raw devices</li> <li>Dynamically provision storage underneath Rook by specifying the storage class Rook should use to consume storage via PVCs</li> <li>Create a Stretch cluster that distributes Ceph mons across three zones, while storage (OSDs) is only configured in two zones</li> </ol> <p>Following is an example for each of these approaches. More examples are included later in this doc.</p>","title":"Ceph Cluster CRD"},{"location":"CRDs/ceph-cluster-crd/#host-based-cluster","text":"<p>To get you started, here is a simple example of a CRD to configure a Ceph cluster with all nodes and all devices. The Ceph persistent data is stored directly on a host path (Ceph Mons) and on raw devices (Ceph OSDs).</p>  <p>NOTE: In addition to your CephCluster object, you need to create the namespace, service accounts, and RBAC rules for the namespace you are going to create the CephCluster in. These resources are defined in the example <code>common.yaml</code>.</p>  <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    # see the \"Cluster Settings\" section below for more details on which image of ceph to run\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n  storage:\n    useAllNodes: true\n    useAllDevices: true\n    onlyApplyOSDPlacement: false\n</code></pre>","title":"Host-based Cluster"},{"location":"CRDs/ceph-cluster-crd/#pvc-based-cluster","text":"<p>In a \"PVC-based cluster\", the Ceph persistent data is stored on volumes requested from a storage class of your choice. This type of cluster is recommended in a cloud environment where volumes can be dynamically created and also in clusters where a local PV provisioner is available.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    # see the \"Cluster Settings\" section below for more details on which image of ceph to run\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n    volumeClaimTemplate:\n      spec:\n        storageClassName: gp2\n        resources:\n          requests:\n            storage: 10Gi\n  storage:\n   storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      encrypted: false\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)\n          storageClassName: gp2\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n    onlyApplyOSDPlacement: false\n</code></pre>  <p>For a more advanced scenario, such as adding a dedicated device you can refer to the dedicated metadata device for OSD on PVC section.</p>","title":"PVC-based Cluster"},{"location":"CRDs/ceph-cluster-crd/#stretch-cluster","text":"<p>For environments that only have two failure domains available where data can be replicated, consider the case where one failure domain is down and the data is still fully available in the remaining failure domain. To support this scenario, Ceph has recently integrated support for \"stretch\" clusters.</p> <p>Rook requires three zones. Two zones (A and B) will each run all types of Rook pods, which we call the \"data\" zones. Two mons run in each of the two data zones, while two replicas of the data are in each zone for a total of four data replicas. The third zone (arbiter) runs a single mon. No other Rook or Ceph daemons need to be run in the arbiter zone.</p> <p>For this example, we assume the desired failure domain is a zone. Another failure domain can also be specified with a known topology node label which is already being used for OSD failure domains.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  dataDirHostPath: /var/lib/rook\n  mon:\n    # Five mons must be created for stretch mode\n    count: 5\n    allowMultiplePerNode: false\n    stretchCluster:\n      failureDomainLabel: topology.kubernetes.io/zone\n      subFailureDomain: host\n      zones:\n      - name: a\n        arbiter: true\n      - name: b\n      - name: c\n  cephVersion:\n    # Stretch cluster is supported in Ceph Pacific or newer.\n    image: quay.io/ceph/ceph:v16.2.7\n    allowUnsupported: true\n  # Either storageClassDeviceSets or the storage section can be specified for creating OSDs.\n  # This example uses all devices for simplicity.\n  storage:\n    useAllNodes: true\n    useAllDevices: true\n    deviceFilter: \"\"\n  # OSD placement is expected to include the non-arbiter zones\n  placement:\n    osd:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: topology.kubernetes.io/zone\n              operator: In\n              values:\n              - b\n              - c\n</code></pre>  <p>For more details, see the Stretch Cluster design doc.</p>","title":"Stretch Cluster"},{"location":"CRDs/ceph-cluster-crd/#settings","text":"<p>Settings can be specified at the global level to apply to the cluster as a whole, while other settings can be specified at more fine-grained levels.  If any setting is unspecified, a suitable default will be used automatically.</p>","title":"Settings"},{"location":"CRDs/ceph-cluster-crd/#cluster-metadata","text":"<ul> <li><code>name</code>: The name that will be used internally for the Ceph cluster. Most commonly the name is the same as the namespace since multiple clusters are not supported in the same namespace.</li> <li><code>namespace</code>: The Kubernetes namespace that will be created for the Rook cluster. The services, pods, and other resources created by the operator will be added to this namespace. The common scenario is to create a single Rook cluster. If multiple clusters are created, they must not have conflicting devices or host paths.</li> </ul>","title":"Cluster metadata"},{"location":"CRDs/ceph-cluster-crd/#cluster-settings","text":"<ul> <li><code>external</code>:</li> <li><code>enable</code>: if <code>true</code>, the cluster will not be managed by Rook but via an external entity. This mode is intended to connect to an existing cluster. In this case, Rook will only consume the external cluster. However, Rook will be able to deploy various daemons in Kubernetes such as object gateways, mds and nfs if an image is provided and will refuse otherwise. If this setting is enabled all the other options will be ignored except <code>cephVersion.image</code> and <code>dataDirHostPath</code>. See external cluster configuration. If <code>cephVersion.image</code> is left blank, Rook will refuse the creation of extra CRs like object, file and nfs.</li> <li><code>cephVersion</code>: The version information for launching the ceph daemons.</li> <li><code>image</code>: The image used for running the ceph daemons. For example, <code>quay.io/ceph/ceph:v15.2.12</code> or <code>v16.2.7</code>. For more details read the container images section.   For the latest ceph images, see the Ceph DockerHub.   To ensure a consistent version of the image is running across all nodes in the cluster, it is recommended to use a very specific image version.   Tags also exist that would give the latest version, but they are only recommended for test environments. For example, the tag <code>v15</code> will be updated each time a new Octopus build is released.   Using the <code>v15</code> or similar tag is not recommended in production because it may lead to inconsistent versions of the image running across different nodes in the cluster.</li> <li><code>allowUnsupported</code>: If <code>true</code>, allow an unsupported major version of the Ceph release. Currently <code>octopus</code> and <code>pacific</code> are supported. Future versions such as <code>quincy</code> would require this to be set to <code>true</code>. Should be set to <code>false</code> in production.</li> <li><code>dataDirHostPath</code>: The path on the host (hostPath) where config and data should be stored for each of the services. If the directory does not exist, it will be created. Because this directory persists on the host, it will remain after pods are deleted. Following paths and any of their subpaths must not be used: <code>/etc/ceph</code>, <code>/rook</code> or <code>/var/log/ceph</code>.</li> <li>On Minikube environments, use <code>/data/rook</code>. Minikube boots into a tmpfs but it provides some directories where files can be persisted across reboots. Using one of these directories will ensure that Rook's data and configuration files are persisted and that enough storage space is available.</li> <li>WARNING: For test scenarios, if you delete a cluster and start a new cluster on the same hosts, the path used by <code>dataDirHostPath</code> must be deleted. Otherwise, stale keys and other config will remain from the previous cluster and the new mons will fail to start. If this value is empty, each pod will get an ephemeral directory to store their config files that is tied to the lifetime of the pod running on that node. More details can be found in the Kubernetes empty dir docs.</li> <li><code>skipUpgradeChecks</code>: if set to true Rook won't perform any upgrade checks on Ceph daemons during an upgrade. Use this at YOUR OWN RISK, only if you know what you're doing. To understand Rook's upgrade process of Ceph, read the upgrade doc.</li> <li><code>continueUpgradeAfterChecksEvenIfNotHealthy</code>: if set to true Rook will continue the OSD daemon upgrade process even if the PGs are not clean, or continue with the MDS upgrade even the file system is not healthy.</li> <li><code>dashboard</code>: Settings for the Ceph dashboard. To view the dashboard in your browser see the dashboard guide.</li> <li><code>enabled</code>: Whether to enable the dashboard to view cluster status</li> <li><code>urlPrefix</code>: Allows to serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)</li> <li><code>port</code>: Allows to change the default port where the dashboard is served</li> <li><code>ssl</code>: Whether to serve the dashboard via SSL, ignored on Ceph versions older than <code>13.2.2</code></li> <li><code>monitoring</code>: Settings for monitoring Ceph using Prometheus. To enable monitoring on your cluster see the monitoring guide.</li> <li><code>enabled</code>: Whether to enable prometheus based monitoring for this cluster</li> <li><code>externalMgrEndpoints</code>: external cluster manager endpoints</li> <li><code>externalMgrPrometheusPort</code>: external prometheus manager module port. See external cluster configuration for more details.</li> <li><code>rulesNamespace</code>: Namespace to deploy prometheusRule. If empty, namespace of the cluster will be used.       Recommended:<ul> <li>If you have a single Rook Ceph cluster, set the <code>rulesNamespace</code> to the same namespace as the cluster or keep it empty.</li> <li>If you have multiple Rook Ceph clusters in the same Kubernetes cluster, choose the same namespace to set <code>rulesNamespace</code> for all the clusters (ideally, namespace with prometheus deployed). Otherwise, you will get duplicate alerts with duplicate alert definitions.</li> </ul> </li> <li><code>network</code>: For the network settings for the cluster, refer to the network configuration settings</li> <li><code>mon</code>: contains mon related options mon settings For more details on the mons and when to choose a number other than <code>3</code>, see the mon health doc.</li> <li><code>mgr</code>: manager top level section</li> <li><code>count</code>: set number of ceph managers between <code>1</code> to <code>2</code>. The default value is 2.     If there are two managers, it is important for all mgr services point to the active mgr and not the passive mgr. Therefore, Rook will     automatically update all services (in the cluster namespace) that have a label <code>app=rook-ceph-mgr</code> with a selector pointing to the     active mgr. This commonly applies to services for the dashboard or the prometheus metrics collector.</li> <li><code>modules</code>: is the list of Ceph manager modules to enable</li> <li><code>crashCollector</code>: The settings for crash collector daemon(s).</li> <li><code>disable</code>: is set to <code>true</code>, the crash collector will not run on any node where a Ceph daemon runs</li> <li><code>daysToRetain</code>: specifies the number of days to keep crash entries in the Ceph cluster. By default the entries are kept indefinitely.</li> <li><code>logCollector</code>: The settings for log collector daemon.</li> <li><code>enabled</code>: if set to <code>true</code>, the log collector will run as a side-car next to each Ceph daemon. The Ceph configuration option <code>log_to_file</code> will be turned on, meaning Ceph daemons will log on files in addition to still logging to container's stdout. These logs will be rotated. (default: false)</li> <li><code>periodicity</code>: how often to rotate daemon's log. (default: 24h). Specified with a time suffix which may be 'h' for hours or 'd' for days. Rotating too often will slightly impact the daemon's performance since the signal briefly interrupts the program.</li> <li><code>annotations</code>: annotations configuration settings</li> <li><code>labels</code>: labels configuration settings</li> <li><code>placement</code>: placement configuration settings</li> <li><code>resources</code>: resources configuration settings</li> <li><code>priorityClassNames</code>: priority class names configuration settings</li> <li><code>storage</code>: Storage selection and configuration that will be used across the cluster.  Note that these settings can be overridden for specific nodes.</li> <li><code>useAllNodes</code>: <code>true</code> or <code>false</code>, indicating if all nodes in the cluster should be used for storage according to the cluster level storage selection and configuration values.   If individual nodes are specified under the <code>nodes</code> field, then <code>useAllNodes</code> must be set to <code>false</code>.</li> <li><code>nodes</code>: Names of individual nodes in the cluster that should have their storage included in accordance with either the cluster level configuration specified above or any node specific overrides described in the next section below. <code>useAllNodes</code> must be set to <code>false</code> to use specific nodes and their config.   See node settings below.</li> <li><code>config</code>: Config settings applied to all OSDs on the node unless overridden by <code>devices</code>. See the config settings below.</li> <li>storage selection settings</li> <li>Storage Class Device Sets</li> <li><code>onlyApplyOSDPlacement</code>: Whether the placement specific for OSDs is merged with the <code>all</code> placement. If <code>false</code>, the OSD placement will be merged with the <code>all</code> placement. If true, the <code>OSD placement will be applied</code> and the <code>all</code> placement will be ignored. The placement for OSDs is computed from several different places depending on the type of OSD:<ul> <li>For non-PVCs: <code>placement.all</code> and <code>placement.osd</code></li> <li>For PVCs: <code>placement.all</code> and inside the storageClassDeviceSets from the <code>placement</code> or <code>preparePlacement</code></li> </ul> </li> <li><code>disruptionManagement</code>: The section for configuring management of daemon disruptions</li> <li><code>managePodBudgets</code>: if <code>true</code>, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically via the strategy outlined in the design. The operator will block eviction of OSDs by default and unblock them safely when drains are detected.</li> <li><code>osdMaintenanceTimeout</code>: is a duration in minutes that determines how long an entire failureDomain like <code>region/zone/host</code> will be held in <code>noout</code> (in addition to the default DOWN/OUT interval) when it is draining. This is only relevant when  <code>managePodBudgets</code> is <code>true</code>. The default value is <code>30</code> minutes.</li> <li><code>manageMachineDisruptionBudgets</code>: if <code>true</code>, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy. Only available on OpenShift.</li> <li><code>machineDisruptionBudgetNamespace</code>: the namespace in which to watch the MachineDisruptionBudgets.</li> <li><code>removeOSDsIfOutAndSafeToRemove</code>: If <code>true</code> the operator will remove the OSDs that are down and whose data has been restored to other OSDs. In Ceph terms, the OSDs are <code>out</code> and <code>safe-to-destroy</code> when they are removed.</li> <li><code>cleanupPolicy</code>: cleanup policy settings</li> <li><code>security</code>: security page for key management configuration</li> </ul>","title":"Cluster Settings"},{"location":"CRDs/ceph-cluster-crd/#ceph-container-images","text":"<p>Official releases of Ceph Container images are available from Docker Hub.</p> <p>These are general purpose Ceph container with all necessary daemons and dependencies installed.</p>    TAG MEANING     vRELNUM Latest release in this series (e.g., v15 = Octopus)   vRELNUM.Y Latest stable release in this stable series (e.g., v15.2)   vRELNUM.Y.Z A specific release (e.g., v15.2.5)   vRELNUM.Y.Z-YYYYMMDD A specific build (e.g., v15.2.11-20200419)    <p>A specific will contain a specific release of Ceph as well as security fixes from the Operating System.</p>","title":"Ceph container images"},{"location":"CRDs/ceph-cluster-crd/#mon-settings","text":"<ul> <li><code>count</code>: Set the number of mons to be started. The number must be between <code>1</code> and <code>9</code>. The recommended value is most commonly <code>3</code>.   For highest availability, an odd number of mons should be specified.   For higher durability in case of mon loss, an even number can be specified although availability may be lower.   To maintain quorum a majority of mons must be up. For example, if there are three mons, two must be up.   If there are four mons, three must be up. If there are two mons, both must be up.   If quorum is lost, see the disaster recovery guide to restore quorum from a single mon.</li> <li><code>allowMultiplePerNode</code>: Whether to allow the placement of multiple mons on a single node. Default is <code>false</code> for production. Should only be set to <code>true</code> in test environments.</li> <li><code>volumeClaimTemplate</code>: A <code>PersistentVolumeSpec</code> used by Rook to create PVCs   for monitor storage. This field is optional, and when not provided, HostPath   volume mounts are used.  The current set of fields from template that are used   are <code>storageClassName</code> and the <code>storage</code> resource request and limit. The   default storage size request for new PVCs is <code>10Gi</code>. Ensure that associated   storage class is configured to use <code>volumeBindingMode: WaitForFirstConsumer</code>.   This setting only applies to new monitors that are created when the requested   number of monitors increases, or when a monitor fails and is recreated. An example CRD configuration is provided below.</li> <li><code>stretchCluster</code>: The stretch cluster settings that define the zones (or other failure domain labels) across which to configure the cluster.</li> <li><code>failureDomainLabel</code>: The label that is expected on each node where the cluster is expected to be deployed. The labels must be found     in the list of well-known topology labels.</li> <li><code>subFailureDomain</code>: With a zone, the data replicas must be spread across OSDs in the subFailureDomain. The default is <code>host</code>.</li> <li><code>zones</code>: The failure domain names where the Mons and OSDs are expected to be deployed. There must be three zones specified in the list.     This element is always named <code>zone</code> even if a non-default <code>failureDomainLabel</code> is specified. The elements have two values:<ul> <li><code>name</code>: The name of the zone, which is the value of the domain label.</li> <li><code>arbiter</code>: Whether the zone is expected to be the arbiter zone which only runs a single mon. Exactly one zone must be labeled <code>true</code>.   The two zones that are not the arbiter zone are expected to have OSDs deployed.</li> </ul> </li> </ul> <p>If these settings are changed in the CRD the operator will update the number of mons during a periodic check of the mon health, which by default is every 45 seconds.</p> <p>To change the defaults that the operator uses to determine the mon health and whether to failover a mon, refer to the health settings. The intervals should be small enough that you have confidence the mons will maintain quorum, while also being long enough to ignore network blips where mons are failed over too often.</p>","title":"Mon Settings"},{"location":"CRDs/ceph-cluster-crd/#mgr-settings","text":"<p>You can use the cluster CR to enable or disable any manager module. This can be configured like so:</p> <pre><code>mgr:\n  modules:\n  - name: &lt;name of the module&gt;\n    enabled: true\n</code></pre>  <p>Some modules will have special configuration to ensure the module is fully functional after being enabled. Specifically:</p> <ul> <li><code>pg_autoscaler</code>: Rook will configure all new pools with PG autoscaling by setting: <code>osd_pool_default_pg_autoscale_mode = on</code></li> </ul>","title":"Mgr Settings"},{"location":"CRDs/ceph-cluster-crd/#network-configuration-settings","text":"<p>If not specified, the default SDN will be used. Configure the network that will be enabled for the cluster and services.</p> <ul> <li><code>provider</code>: Specifies the network provider that will be used to connect the network interface. You can choose between <code>host</code>, and <code>multus</code>.</li> <li><code>selectors</code>: List the network selector(s) that will be used associated by a key.</li> <li><code>ipFamily</code>: Specifies the network stack Ceph daemons should listen on.</li> <li><code>dualStack</code>: Specifies that Ceph daemon should listen on both IPv4 and IPv6 network stacks.</li> <li><code>connections</code>: Settings for network connections using Ceph's msgr2 protocol</li> <li><code>encryption</code>: Settings for encryption on the wire to Ceph daemons<ul> <li><code>enabled</code>: Whether to encrypt the data in transit across the wire to prevent eavesdropping the data on the network.   The default is false. When encryption is enabled, all communication between clients and Ceph daemons, or between   Ceph daemons will be encrypted. When encryption is not enabled, clients still establish a strong initial authentication   and data integrity is still validated with a crc check.   IMPORTANT: Encryption requires the 5.11 kernel for the latest nbd and cephfs drivers. Alternatively for testing only,   set \"mounter: rbd-nbd\" in the rbd storage class, or \"mounter: fuse\" in the cephfs storage class.   The nbd and fuse drivers are not recommended in production since restarting the csi driver pod will disconnect the volumes.</li> </ul> </li> <li><code>compression</code>:<ul> <li><code>enabled</code>: Whether to compress the data in transit across the wire. The default is false.   Requires Ceph Quincy (v17) or newer. Also see the kernel requirements above for encryption.</li> </ul> </li> </ul>  <p>NOTE: Changing networking configuration after a Ceph cluster has been deployed is NOT supported and will result in a non-functioning cluster.</p>","title":"Network Configuration Settings"},{"location":"CRDs/ceph-cluster-crd/#host-networking","text":"<p>To use host networking, set <code>provider: host</code>.</p>","title":"Host Networking"},{"location":"CRDs/ceph-cluster-crd/#multus","text":"<p>Rook supports addition of public and cluster network for ceph using Multus</p> <p>The selector keys are required to be <code>public</code> and <code>cluster</code> where each represent:</p> <ul> <li><code>public</code>: client communications with the cluster (reads/writes)</li> <li><code>cluster</code>: internal Ceph replication network</li> </ul> <p>If you want to learn more, please read * Ceph Networking reference. * Multus documentation</p> <p>Based on the configuration, the operator will do the following:</p> <ol> <li>If only the <code>public</code> selector is specified, all communication will happen on that network <pre><code>  network:\n    provider: multus\n    selectors:\n      public: rook-ceph/rook-public-nw\n</code></pre> </li> <li>If only the <code>cluster</code> selector is specified, the internal cluster traffic* will happen on that network. All other traffic to mons, OSDs, and other daemons will be on the default network. <pre><code>  network:\n    provider: multus\n    selectors:\n      cluster: rook-ceph/rook-cluster-nw\n</code></pre> </li> <li>If both <code>public</code> and <code>cluster</code> selectors are specified the first one will run all the communication network and the second the internal cluster network* <pre><code>  network:\n    provider: multus\n    selectors:\n      public: rook-ceph/rook-public-nw\n      cluster: rook-ceph/rook-cluster-nw\n</code></pre> </li> </ol> <p>* Internal cluster traffic includes OSD heartbeats, data replication, and data recovery</p> <p>Only OSD pods will have both Public and Cluster networks attached. The rest of the Ceph component pods and CSI pods will only have the Public network attached. Rook Ceph Operator will not have any networks attached as it proxies the required commands via a sidecar container in the mgr pod.</p> <p>In order to work, each selector value must match a <code>NetworkAttachmentDefinition</code> object name in Multus.</p> <p>For <code>multus</code> network provider, an already working cluster with Multus networking is required. Network attachment definition that later will be attached to the cluster needs to be created before the Cluster CRD. The Network attachment definitions should be using whereabouts cni. If Rook cannot find the provided Network attachment definition it will fail running the Ceph OSD pods. You can add the Multus network attachment selection annotation selecting the created network attachment definition on <code>selectors</code>.</p> <p>A valid NetworkAttachmentDefinition will look like following:</p> <pre><code>apiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: rook-public-nw\nspec:\n  config: '{\n      \"cniVersion\": \"0.3.0\",\n      \"name\": \"public-nad\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens5\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"whereabouts\",\n        \"range\": \"192.168.1.0/24\"\n      }\n    }'\n</code></pre>  <ul> <li>Ensure that <code>master</code> matches the network interface of the host that you want to use.</li> <li>Ipam type <code>whereabouts</code> is required because it makes sure that all the pods get a unique IP address from the multus network.</li> <li>The NetworkAttachmentDefinition should be referenced along with the namespace in which it is present like <code>public: &lt;namespace&gt;/&lt;name of NAD&gt;</code>.   e.g., the network attachment definition are in <code>default</code> namespace: <pre><code>  public: default/rook-public-nw\n  cluster: default/rook-cluster-nw\n</code></pre> </li> <li>This format is required in order to use the NetworkAttachmentDefinition across namespaces.</li> <li>In Openshift, to use a NetworkAttachmentDefinition (NAD) across namespaces, the NAD must be deployed in the <code>default</code> namespace. The NAD is then referenced with the namespace: <code>default/rook-public-nw</code></li> </ul>","title":"Multus"},{"location":"CRDs/ceph-cluster-crd/#known-limitations-with-multus","text":"<p>Daemons leveraging Kubernetes service IPs (Monitors, Managers, Rados Gateways) are not listening on the NAD specified in the <code>selectors</code>. Instead the daemon listens on the default network, however the NAD is attached to the container, allowing the daemon to communicate with the rest of the cluster. There is work in progress to fix this issue in the multus-service repository. At the time of writing it's unclear when this will be supported.</p>","title":"Known limitations with Multus"},{"location":"CRDs/ceph-cluster-crd/#known-issues-with-multus","text":"<p>When a CephFS/RBD volume is mounted in a Pod using cephcsi and then the CSI CephFS/RBD plugin is restarted or terminated (e.g. by restarting or deleting its DaemonSet), all operations on the volume become blocked, even after restarting the CSI pods. The only workaround is to restart the node where the cephcsi plugin pod was restarted. This issue is tracked here.</p>","title":"Known issues with Multus"},{"location":"CRDs/ceph-cluster-crd/#ipfamily","text":"<p>Provide single-stack IPv4 or IPv6 protocol to assign corresponding addresses to pods and services. This field is optional. Possible inputs are IPv6 and IPv4. Empty value will be treated as IPv4. Kubernetes version should be at least v1.13 to run IPv6. Dual-stack is supported as of ceph Pacific. To turn on dual stack see the network configuration section.</p>","title":"IPFamily"},{"location":"CRDs/ceph-cluster-crd/#node-settings","text":"<p>In addition to the cluster level settings specified above, each individual node can also specify configuration to override the cluster level settings and defaults. If a node does not specify any configuration then it will inherit the cluster level settings.</p> <ul> <li><code>name</code>: The name of the node, which should match its <code>kubernetes.io/hostname</code> label.</li> <li><code>config</code>: Config settings applied to all OSDs on the node unless overridden by <code>devices</code>. See the config settings below.</li> <li>storage selection settings</li> </ul> <p>When <code>useAllNodes</code> is set to <code>true</code>, Rook attempts to make Ceph cluster management as hands-off as possible while still maintaining reasonable data safety. If a usable node comes online, Rook will begin to use it automatically. To maintain a balance between hands-off usability and data safety, Nodes are removed from Ceph as OSD hosts only (1) if the node is deleted from Kubernetes itself or (2) if the node has its taints or affinities modified in such a way that the node is no longer usable by Rook. Any changes to taints or affinities, intentional or unintentional, may affect the data reliability of the Ceph cluster. In order to help protect against this somewhat, deletion of nodes by taint or affinity modifications must be \"confirmed\" by deleting the Rook-Ceph operator pod and allowing the operator deployment to restart the pod.</p> <p>For production clusters, we recommend that <code>useAllNodes</code> is set to <code>false</code> to prevent the Ceph cluster from suffering reduced data reliability unintentionally due to a user mistake. When <code>useAllNodes</code> is set to <code>false</code>, Rook relies on the user to be explicit about when nodes are added to or removed from the Ceph cluster. Nodes are only added to the Ceph cluster if the node is added to the Ceph cluster resource. Similarly, nodes are only removed if the node is removed from the Ceph cluster resource.</p>","title":"Node Settings"},{"location":"CRDs/ceph-cluster-crd/#node-updates","text":"<p>Nodes can be added and removed over time by updating the Cluster CRD, for example with <code>kubectl -n rook-ceph edit cephcluster rook-ceph</code>. This will bring up your default text editor and allow you to add and remove storage nodes from the cluster. This feature is only available when <code>useAllNodes</code> has been set to <code>false</code>.</p>","title":"Node Updates"},{"location":"CRDs/ceph-cluster-crd/#storage-selection-settings","text":"<p>Below are the settings for host-based cluster. This type of cluster can specify devices for OSDs, both at the cluster and individual node level, for selecting which storage resources will be included in the cluster.</p> <ul> <li><code>useAllDevices</code>: <code>true</code> or <code>false</code>, indicating whether all devices found on nodes in the cluster should be automatically consumed by OSDs. Not recommended unless you have a very controlled environment where you will not risk formatting of devices with existing data. When <code>true</code>, all devices/partitions will be used. Is overridden by <code>deviceFilter</code> if specified.</li> <li><code>deviceFilter</code>: A regular expression for short kernel names of devices (e.g. <code>sda</code>) that allows selection of devices to be consumed by OSDs.  If individual devices have been specified for a node then this filter will be ignored.  This field uses golang regular expression syntax. For example:</li> <li><code>sdb</code>: Only selects the <code>sdb</code> device if found</li> <li><code>^sd.</code>: Selects all devices starting with <code>sd</code></li> <li><code>^sd[a-d]</code>: Selects devices starting with <code>sda</code>, <code>sdb</code>, <code>sdc</code>, and <code>sdd</code> if found</li> <li><code>^s</code>: Selects all devices that start with <code>s</code></li> <li><code>^[^r]</code>: Selects all devices that do not start with <code>r</code></li> <li><code>devicePathFilter</code>: A regular expression for device paths (e.g. <code>/dev/disk/by-path/pci-0:1:2:3-scsi-1</code>) that allows selection of devices to be consumed by OSDs.  If individual devices or <code>deviceFilter</code> have been specified for a node then this filter will be ignored.  This field uses golang regular expression syntax. For example:</li> <li><code>^/dev/sd.</code>: Selects all devices starting with <code>sd</code></li> <li><code>^/dev/disk/by-path/pci-.*</code>: Selects all devices which are connected to PCI bus</li> <li><code>devices</code>: A list of individual device names belonging to this node to include in the storage cluster.</li> <li><code>name</code>: The name of the device (e.g., <code>sda</code>), or full udev path (e.g. <code>/dev/disk/by-id/ata-ST4000DM004-XXXX</code> - this will not change after reboots).</li> <li><code>config</code>: Device-specific config settings. See the config settings below</li> </ul> <p>Host-based cluster only supports raw device and partition. Be sure to see the Ceph quickstart doc prerequisites for additional considerations.</p> <p>Below are the settings for a PVC-based cluster.</p> <ul> <li><code>storageClassDeviceSets</code>: Explained in Storage Class Device Sets</li> </ul>","title":"Storage Selection Settings"},{"location":"CRDs/ceph-cluster-crd/#storage-class-device-sets","text":"<p>The following are the settings for Storage Class Device Sets which can be configured to create OSDs that are backed by block mode PVs.</p> <ul> <li><code>name</code>: A name for the set.</li> <li><code>count</code>: The number of devices in the set.</li> <li><code>resources</code>: The CPU and RAM requests/limits for the devices. (Optional)</li> <li><code>placement</code>: The placement criteria for the devices. (Optional) Default is no placement criteria.</li> </ul> <p>The syntax is the same as for other placement configuration. It supports <code>nodeAffinity</code>, <code>podAffinity</code>, <code>podAntiAffinity</code> and <code>tolerations</code> keys.</p> <p>It is recommended to configure the placement such that the OSDs will be as evenly spread across nodes as possible. At a minimum, anti-affinity should be added so at least one OSD will be placed on each available nodes.</p> <p>However, if there are more OSDs than nodes, this anti-affinity will not be effective. Another placement scheme to consider is to add labels to the nodes in such a way that the OSDs can be grouped on those nodes, create multiple storageClassDeviceSets, and add node affinity to each of the device sets that will place the OSDs in those sets of nodes.</p> <p>Rook will automatically add required nodeAffinity to the OSD daemons to match the topology labels that are found   on the nodes where the OSD prepare jobs ran. To ensure data durability, the OSDs are required to run in the same   topology that the Ceph CRUSH map expects. For example, if the nodes are labeled with rack topology labels, the   OSDs will be constrained to a certain rack. Without the topology labels, Rook will not constrain the OSDs beyond   what is required by the PVs, for example to run in the zone where provisioned. See the OSD Topology   section for the related labels.</p> <ul> <li><code>preparePlacement</code>: The placement criteria for the preparation of the OSD devices. Creating OSDs is a two-step process and the prepare job may require different placement than the OSD daemons. If the <code>preparePlacement</code> is not specified, the <code>placement</code> will instead be applied for consistent placement for the OSD prepare jobs and OSD deployments. The <code>preparePlacement</code> is only useful for <code>portable</code> OSDs in the device sets. OSDs that are not portable will be tied to the host where the OSD prepare job initially runs.</li> <li>For example, provisioning may require topology spread constraints across zones, but the OSD daemons may require constraints across hosts within the zones.</li> <li><code>portable</code>: If <code>true</code>, the OSDs will be allowed to move between nodes during failover. This requires a storage class that supports portability (e.g. <code>aws-ebs</code>, but not the local storage provisioner). If <code>false</code>, the OSDs will be assigned to a node permanently. Rook will configure Ceph's CRUSH map to support the portability.</li> <li><code>tuneDeviceClass</code>: For example, Ceph cannot detect AWS volumes as HDDs from the storage class \"gp2\", so you can improve Ceph performance by setting this to true.</li> <li><code>tuneFastDeviceClass</code>: For example, Ceph cannot detect Azure disks as SSDs from the storage class \"managed-premium\", so you can improve Ceph performance by setting this to true..</li> <li><code>volumeClaimTemplates</code>: A list of PVC templates to use for provisioning the underlying storage devices.</li> <li><code>resources.requests.storage</code>: The desired capacity for the underlying storage devices.</li> <li><code>storageClassName</code>: The StorageClass to provision PVCs from. Default would be to use the cluster-default StorageClass. This StorageClass should provide a raw block device, multipath device, or logical volume. Other types are not supported. If you want to use logical volume, please see known issue of OSD on LV-backed PVC</li> <li><code>volumeMode</code>: The volume mode to be set for the PVC. Which should be Block</li> <li><code>accessModes</code>: The access mode for the PVC to be bound by OSD.</li> <li><code>schedulerName</code>: Scheduler name for OSD pod placement. (Optional)</li> <li><code>encrypted</code>: whether to encrypt all the OSDs in a given storageClassDeviceSet</li> </ul>","title":"Storage Class Device Sets"},{"location":"CRDs/ceph-cluster-crd/#osd-configuration-settings","text":"<p>The following storage selection settings are specific to Ceph and do not apply to other backends. All variables are key-value pairs represented as strings.</p> <ul> <li><code>metadataDevice</code>: Name of a device to use for the metadata of OSDs on each node.  Performance can be improved by using a low latency device (such as SSD or NVMe) as the metadata device, while other spinning platter (HDD) devices on a node are used to store data. Provisioning will fail if the user specifies a <code>metadataDevice</code> but that device is not used as a metadata device by Ceph. Notably, <code>ceph-volume</code> will not use a device of the same device class (HDD, SSD, NVMe) as OSD devices for metadata, resulting in this failure.</li> <li><code>databaseSizeMB</code>:  The size in MB of a bluestore database. Include quotes around the size.</li> <li><code>walSizeMB</code>:  The size in MB of a bluestore write ahead log (WAL). Include quotes around the size.</li> <li><code>deviceClass</code>: The CRUSH device class to use for this selection of storage devices. (By default, if a device's class has not already been set, OSDs will automatically set a device's class to either <code>hdd</code>, <code>ssd</code>, or <code>nvme</code>  based on the hardware properties exposed by the Linux kernel.) These storage classes can then be used to select the devices backing a storage pool by specifying them as the value of the pool spec's <code>deviceClass</code> field.</li> <li><code>initialWeight</code>: The initial OSD weight in TiB units. By default, this value is derived from OSD's capacity.</li> <li><code>primaryAffinity</code>: The primary-affinity value of an OSD, within range <code>[0, 1]</code> (default: <code>1</code>).</li> <li><code>osdsPerDevice</code>**: The number of OSDs to create on each device. High performance devices such as NVMe can handle running multiple OSDs. If desired, this can be overridden for each node and each device.</li> <li><code>encryptedDevice</code>**: Encrypt OSD volumes using dmcrypt (\"true\" or \"false\"). By default this option is disabled. See encryption for more information on encryption in Ceph.</li> <li><code>crushRoot</code>: The value of the <code>root</code> CRUSH map label. The default is <code>default</code>. Generally, you should not need to change this. However, if any of your topology labels may have the value <code>default</code>, you need to change <code>crushRoot</code> to avoid conflicts, since CRUSH map values need to be unique.</li> </ul>","title":"OSD Configuration Settings"},{"location":"CRDs/ceph-cluster-crd/#annotations-and-labels","text":"<p>Annotations and Labels can be specified so that the Rook components will have those annotations / labels added to them.</p> <p>You can set annotations / labels for Rook components for the list of key value pairs:</p> <ul> <li><code>all</code>: Set annotations / labels for all components except <code>clusterMetadata</code>.</li> <li><code>mgr</code>: Set annotations / labels for MGRs</li> <li><code>mon</code>: Set annotations / labels for mons</li> <li><code>osd</code>: Set annotations / labels for OSDs</li> <li><code>prepareosd</code>: Set annotations / labels for OSD Prepare Jobs</li> <li><code>monitoring</code>: Set annotations / labels for service monitor</li> <li><code>crashcollector</code>: Set annotations / labels for crash collectors</li> <li><code>clusterMetadata</code>: Set annotations  only to <code>rook-ceph-mon-endpoints</code> configmap and the  <code>rook-ceph-mon</code> and <code>rook-ceph-admin-keyring</code> secrets. These annotations will not be merged with the <code>all</code> annotations. The common usage is for backing up these critical resources with <code>kubed</code>. Note the clusterMetadata annotation will not be merged with the <code>all</code> annotation. When other keys are set, <code>all</code> will be merged together with the specific component.</li> </ul>","title":"Annotations and Labels"},{"location":"CRDs/ceph-cluster-crd/#placement-configuration-settings","text":"<p>Placement configuration for the cluster services. It includes the following keys: <code>mgr</code>, <code>mon</code>, <code>arbiter</code>, <code>osd</code>, <code>cleanup</code>, and <code>all</code>. Each service will have its placement configuration generated by merging the generic configuration under <code>all</code> with the most specific one (which will override any attributes).</p> <p>In stretch clusters, if the <code>arbiter</code> placement is specified, that placement will only be applied to the arbiter. Neither will the <code>arbiter</code> placement be merged with the <code>all</code> placement to allow the arbiter to be fully independent of other daemon placement. The remaining mons will still use the <code>mon</code> and/or <code>all</code> sections.</p> <p>NOTE: Placement of OSD pods is controlled using the Storage Class Device Set, not the general <code>placement</code> configuration.</p> <p>A Placement configuration is specified (according to the kubernetes PodSpec) as:</p> <ul> <li><code>nodeAffinity</code>: kubernetes NodeAffinity</li> <li><code>podAffinity</code>: kubernetes PodAffinity</li> <li><code>podAntiAffinity</code>: kubernetes PodAntiAffinity</li> <li><code>tolerations</code>: list of kubernetes Toleration</li> <li><code>topologySpreadConstraints</code>: kubernetes TopologySpreadConstraints</li> </ul> <p>If you use <code>labelSelector</code> for <code>osd</code> pods, you must write two rules both for <code>rook-ceph-osd</code> and <code>rook-ceph-osd-prepare</code> like the example configuration. It comes from the design that there are these two pods for an OSD. For more detail, see the osd design doc and the related issue.</p> <p>The Rook Ceph operator creates a Job called <code>rook-ceph-detect-version</code> to detect the full Ceph version used by the given <code>cephVersion.image</code>. The placement from the <code>mon</code> section is used for the Job except for the <code>PodAntiAffinity</code> field.</p>","title":"Placement Configuration Settings"},{"location":"CRDs/ceph-cluster-crd/#cluster-wide-resources-configuration-settings","text":"<p>Resources should be specified so that the Rook components are handled after Kubernetes Pod Quality of Service classes. This allows to keep Rook components running when for example a node runs out of memory and the Rook components are not killed depending on their Quality of Service class.</p> <p>You can set resource requests/limits for Rook components through the Resource Requirements/Limits structure in the following keys:</p> <ul> <li><code>mon</code>: Set resource requests/limits for mons</li> <li><code>osd</code>: Set resource requests/limits for OSDs.   This key applies for all OSDs regardless of their device classes. In case of need to apply resource requests/limits for OSDs with particular device class use specific osd keys below. If the memory resource is declared Rook will automatically set the OSD configuration <code>osd_memory_target</code> to the same value. This aims to ensure that the actual OSD memory consumption is consistent with the OSD pods' resource declaration.</li> <li><code>osd-&lt;deviceClass&gt;</code>: Set resource requests/limits for OSDs on a specific device class. Rook will automatically detect <code>hdd</code>, <code>ssd</code>, or <code>nvme</code> device classes. Custom device classes can also be set.</li> <li><code>mgr</code>: Set resource requests/limits for MGRs</li> <li><code>mgr-sidecar</code>: Set resource requests/limits for the MGR sidecar, which is only created when <code>mgr.count: 2</code>.   The sidecar requires very few resources since it only executes every 15 seconds to query Ceph for the active   mgr and update the mgr services if the active mgr changed.</li> <li><code>prepareosd</code>: Set resource requests/limits for OSD prepare job</li> <li><code>crashcollector</code>: Set resource requests/limits for crash. This pod runs wherever there is a Ceph pod running. It scrapes for Ceph daemon core dumps and sends them to the Ceph manager crash module so that core dumps are centralized and can be easily listed/accessed. You can read more about the Ceph Crash module.</li> <li><code>logcollector</code>: Set resource requests/limits for the log collector. When enabled, this container runs as side-car to each Ceph daemons.</li> <li><code>cleanup</code>: Set resource requests/limits for cleanup job, responsible for wiping cluster's data after uninstall</li> </ul> <p>In order to provide the best possible experience running Ceph in containers, Rook internally recommends minimum memory limits if resource limits are passed. If a user configures a limit or request value that is too low, Rook will still run the pod(s) and print a warning to the operator log.</p> <ul> <li><code>mon</code>: 1024MB</li> <li><code>mgr</code>: 512MB</li> <li><code>osd</code>: 2048MB</li> <li><code>prepareosd</code>: 50MB</li> <li><code>crashcollector</code>: 60MB</li> <li><code>mgr-sidecar</code>: 100MB limit, 40MB requests</li> </ul>  <p>HINT The resources for MDS daemons are not configured in the Cluster. Refer to the Ceph Filesystem CRD instead.</p>","title":"Cluster-wide Resources Configuration Settings"},{"location":"CRDs/ceph-cluster-crd/#resource-requirementslimits","text":"<p>For more information on resource requests/limits see the official Kubernetes documentation: Kubernetes - Managing Compute Resources for Containers</p> <ul> <li><code>requests</code>: Requests for cpu or memory.</li> <li><code>cpu</code>: Request for CPU (example: one CPU core <code>1</code>, 50% of one CPU core <code>500m</code>).</li> <li><code>memory</code>: Limit for Memory (example: one gigabyte of memory <code>1Gi</code>, half a gigabyte of memory <code>512Mi</code>).</li> <li><code>limits</code>: Limits for cpu or memory.</li> <li><code>cpu</code>: Limit for CPU (example: one CPU core <code>1</code>, 50% of one CPU core <code>500m</code>).</li> <li><code>memory</code>: Limit for Memory (example: one gigabyte of memory <code>1Gi</code>, half a gigabyte of memory <code>512Mi</code>).</li> </ul>","title":"Resource Requirements/Limits"},{"location":"CRDs/ceph-cluster-crd/#priority-class-names","text":"<p>Priority class names can be specified so that the Rook components will have those priority class names added to them.</p> <p>You can set priority class names for Rook components for the list of key value pairs:</p> <ul> <li><code>all</code>: Set priority class names for MGRs, Mons, OSDs, and crashcollectors.</li> <li><code>mgr</code>: Set priority class names for MGRs. Examples default to system-cluster-critical.</li> <li><code>mon</code>: Set priority class names for Mons. Examples default to system-node-critical.</li> <li><code>osd</code>: Set priority class names for OSDs. Examples default to system-node-critical.</li> <li><code>crashcollector</code>: Set priority class names for crashcollectors.</li> </ul> <p>The specific component keys will act as overrides to <code>all</code>.</p>","title":"Priority Class Names"},{"location":"CRDs/ceph-cluster-crd/#health-settings","text":"<p>Rook-Ceph will monitor the state of the CephCluster on various components by default. The following CRD settings are available:</p> <ul> <li><code>healthCheck</code>: main ceph cluster health monitoring section</li> </ul> <p>Currently three health checks are implemented:</p> <ul> <li><code>mon</code>: health check on the ceph monitors, basically check whether monitors are members of the quorum. If after a certain timeout a given monitor has not joined the quorum back it will be failed over and replace by a new monitor.</li> <li><code>osd</code>: health check on the ceph osds</li> <li><code>status</code>: ceph health status check, periodically check the Ceph health state and reflects it in the CephCluster CR status field.</li> </ul> <p>The liveness probe and startup probe of each daemon can also be controlled via <code>livenessProbe</code> and <code>startupProbe</code> respectively. The settings are valid for <code>mon</code>, <code>mgr</code> and <code>osd</code>. Here is a complete example for both <code>daemonHealth</code>, <code>livenessProbe</code>, and <code>startupProbe</code>:</p> <pre><code>healthCheck:\n  daemonHealth:\n    mon:\n      disabled: false\n      interval: 45s\n      timeout: 600s\n    osd:\n      disabled: false\n      interval: 60s\n    status:\n      disabled: false\n  livenessProbe:\n    mon:\n      disabled: false\n    mgr:\n      disabled: false\n    osd:\n      disabled: false\n  startupProbe:\n    mon:\n      disabled: false\n    mgr:\n      disabled: false\n    osd:\n      disabled: false\n</code></pre>  <p>The probe's timing values and thresholds (but not the probe itself) can also be overridden. For more info, refer to the Kubernetes documentation.</p> <p>For example, you could change the <code>mgr</code> probe by applying:</p> <pre><code>healthCheck:\n  startupProbe:\n    mgr:\n      disabled: false\n      probe:\n        initialDelaySeconds: 3\n        periodSeconds: 3\n        failureThreshold: 30\n  livenessProbe:\n    mgr:\n      disabled: false\n      probe:\n        initialDelaySeconds: 3\n        periodSeconds: 3\n</code></pre>  <p>Changing the liveness probe is an advanced operation and should rarely be necessary. If you want to change these settings then modify the desired settings.</p>","title":"Health settings"},{"location":"CRDs/ceph-cluster-crd/#status","text":"<p>The operator is regularly configuring and checking the health of the cluster. The results of the configuration and health checks can be seen in the <code>status</code> section of the CephCluster CR.</p> <pre><code>kubectl -n rook-ceph get CephCluster -o yaml\n</code></pre>  <pre><code>  ...\n  status:\n    ceph:\n      health: HEALTH_OK\n      lastChecked: \"2021-03-02T21:22:11Z\"\n      capacity:\n        bytesAvailable: 22530293760\n        bytesTotal: 25757220864\n        bytesUsed: 3226927104\n        lastUpdated: \"2021-03-02T21:22:11Z\"\n    message: Cluster created successfully\n    phase: Ready\n    state: Created\n    storage:\n      deviceClasses:\n      - name: hdd\n    version:\n      image: quay.io/ceph/ceph:v16.2.7\n      version: 16.2.6-0\n    conditions:\n    - lastHeartbeatTime: \"2021-03-02T21:22:11Z\"\n      lastTransitionTime: \"2021-03-02T21:21:09Z\"\n      message: Cluster created successfully\n      reason: ClusterCreated\n      status: \"True\"\n      type: Ready\n</code></pre>","title":"Status"},{"location":"CRDs/ceph-cluster-crd/#ceph-status","text":"<p>Ceph is constantly monitoring the health of the data plane and reporting back if there are any warnings or errors. If everything is healthy from Ceph's perspective, you will see <code>HEALTH_OK</code>.</p> <p>If Ceph reports any warnings or errors, the details will be printed to the status. If further troubleshooting is needed to resolve these issues, the toolbox will likely be needed where you can run <code>ceph</code> commands to find more details.</p> <p>The <code>capacity</code> of the cluster is reported, including bytes available, total, and used. The available space will be less that you may expect due to overhead in the OSDs.</p>","title":"Ceph Status"},{"location":"CRDs/ceph-cluster-crd/#conditions","text":"<p>The <code>conditions</code> represent the status of the Rook operator. - If the cluster is fully configured and the operator is stable, the <code>Ready</code> condition is raised with <code>ClusterCreated</code> reason and no other conditions. The cluster   will remain in the <code>Ready</code> condition after the first successful configuration since it   is expected the storage is consumable from this point on. If there are issues preventing   the storage layer from working, they are expected to show as Ceph health errors. - If the cluster is externally connected successfully, the <code>Ready</code> condition will have the reason <code>ClusterConnected</code>. - If the operator is currently being configured or the operator is checking for update,   there will be a <code>Progressing</code> condition. - If there was a failure, the condition(s) status will be <code>false</code> and the <code>message</code> will   give a summary of the error. See the operator log for more details.</p>","title":"Conditions"},{"location":"CRDs/ceph-cluster-crd/#other-status","text":"<p>There are several other properties for the overall status including: - <code>message</code>, <code>phase</code>, and <code>state</code>: A summary of the overall current state of the cluster, which   is somewhat duplicated from the conditions for backward compatibility. - <code>storage.deviceClasses</code>: The names of the types of storage devices that Ceph discovered   in the cluster. These types will be <code>ssd</code> or <code>hdd</code> unless they have been overridden   with the <code>crushDeviceClass</code> in the <code>storageClassDeviceSets</code>. - <code>version</code>: The version of the Ceph image currently deployed.</p>","title":"Other Status"},{"location":"CRDs/ceph-cluster-crd/#samples","text":"<p>Here are several samples for configuring Ceph clusters. Each of the samples must also include the namespace and corresponding access granted for management by the Ceph operator. See the common cluster resources below.</p>","title":"Samples"},{"location":"CRDs/ceph-cluster-crd/#storage-configuration-all-devices","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n  dashboard:\n    enabled: true\n  # cluster level storage configuration and selection\n  storage:\n    useAllNodes: true\n    useAllDevices: true\n    deviceFilter:\n    config:\n      metadataDevice:\n      databaseSizeMB: \"1024\" # this value can be removed for environments with normal sized disks (100 GB or larger)\n      journalSizeMB: \"1024\"  # this value can be removed for environments with normal sized disks (20 GB or larger)\n      osdsPerDevice: \"1\"\n</code></pre>","title":"Storage configuration: All devices"},{"location":"CRDs/ceph-cluster-crd/#storage-configuration-specific-devices","text":"<p>Individual nodes and their config can be specified so that only the named nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n  dashboard:\n    enabled: true\n  # cluster level storage configuration and selection\n  storage:\n    useAllNodes: false\n    useAllDevices: false\n    deviceFilter:\n    config:\n      metadataDevice:\n      databaseSizeMB: \"1024\" # this value can be removed for environments with normal sized disks (100 GB or larger)\n    nodes:\n    - name: \"172.17.4.201\"\n      devices:             # specific devices to use for storage can be specified for each node\n      - name: \"sdb\" # Whole storage device\n      - name: \"sdc1\" # One specific partition. Should not have a file system on it.\n      - name: \"/dev/disk/by-id/ata-ST4000DM004-XXXX\" # both device name and explicit udev links are supported\n      config:         # configuration can be specified at the node level which overrides the cluster level config\n    - name: \"172.17.4.301\"\n      deviceFilter: \"^sd.\"\n</code></pre>","title":"Storage Configuration: Specific devices"},{"location":"CRDs/ceph-cluster-crd/#node-affinity","text":"<p>To control where various services will be scheduled by kubernetes, use the placement configuration sections below. The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and tolerate taints with a key of 'storage-node'.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n  # enable the ceph dashboard for viewing cluster status\n  dashboard:\n    enabled: true\n  placement:\n    all:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: role\n              operator: In\n              values:\n              - storage-node\n      tolerations:\n      - key: storage-node\n        operator: Exists\n    mgr:\n      nodeAffinity:\n      tolerations:\n    mon:\n      nodeAffinity:\n      tolerations:\n    osd:\n      nodeAffinity:\n      tolerations:\n</code></pre>","title":"Node Affinity"},{"location":"CRDs/ceph-cluster-crd/#resource-requestslimits","text":"<p>To control how many resources the Rook components can request/use, you can set requests and limits in Kubernetes for them. You can override these requests/limits for OSDs per node when using <code>useAllNodes: false</code> in the <code>node</code> item in the <code>nodes</code> list.</p>  <p>WARNING: Before setting resource requests/limits, please take a look at the Ceph documentation for recommendations for each component: Ceph - Hardware Recommendations.</p>  <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n  # enable the ceph dashboard for viewing cluster status\n  dashboard:\n    enabled: true\n  # cluster level resource requests/limits configuration\n  resources:\n  storage:\n    useAllNodes: false\n    nodes:\n    - name: \"172.17.4.201\"\n      resources:\n        limits:\n          cpu: \"2\"\n          memory: \"4096Mi\"\n        requests:\n          cpu: \"2\"\n          memory: \"4096Mi\"\n</code></pre>","title":"Resource Requests/Limits"},{"location":"CRDs/ceph-cluster-crd/#osd-topology","text":"<p>The topology of the cluster is important in production environments where you want your data spread across failure domains. The topology can be controlled by adding labels to the nodes. When the labels are found on a node at first OSD deployment, Rook will add them to the desired level in the CRUSH map.</p> <p>The complete list of labels in hierarchy order from highest to lowest is:</p> <pre><code>topology.kubernetes.io/region\ntopology.kubernetes.io/zone\ntopology.rook.io/datacenter\ntopology.rook.io/room\ntopology.rook.io/pod\ntopology.rook.io/pdu\ntopology.rook.io/row\ntopology.rook.io/rack\ntopology.rook.io/chassis\n</code></pre>  <p>For example, if the following labels were added to a node:</p> <pre><code>kubectl label node mynode topology.kubernetes.io/zone=zone1\nkubectl label node mynode topology.rook.io/rack=zone1-rack1\n</code></pre>  <p>These labels would result in the following hierarchy for OSDs on that node (this command can be run in the Rook toolbox):</p> <pre><code>ceph osd tree\n</code></pre>   <pre><code>ID CLASS WEIGHT  TYPE NAME                 STATUS REWEIGHT PRI-AFF\n-1       0.01358 root default\n-5       0.01358     zone zone1\n-4       0.01358         rack rack1\n-3       0.01358             host mynode\n0   hdd 0.00679                 osd.0         up  1.00000 1.00000\n1   hdd 0.00679                 osd.1         up  1.00000 1.00000\n</code></pre>   <p>Ceph requires unique names at every level in the hierarchy (CRUSH map). For example, you cannot have two racks with the same name that are in different zones. Racks in different zones must be named uniquely.</p> <p>Note that the <code>host</code> is added automatically to the hierarchy by Rook. The host cannot be specified with a topology label. All topology labels are optional.</p>  <p>HINT When setting the node labels prior to <code>CephCluster</code> creation, these settings take immediate effect. However, applying this to an already deployed <code>CephCluster</code> requires removing each node from the cluster first and then re-adding it with new configuration to take effect. Do this node by node to keep your data safe! Check the result with <code>ceph osd tree</code> from the Rook Toolbox. The OSD tree should display the hierarchy for the nodes that already have been re-added.</p>  <p>To utilize the <code>failureDomain</code> based on the node labels, specify the corresponding option in the CephBlockPool</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  failureDomain: rack  # this matches the topology labels on nodes\n  replicated:\n    size: 3\n</code></pre>  <p>This configuration will split the replication of volumes across unique racks in the data center setup.</p>","title":"OSD Topology"},{"location":"CRDs/ceph-cluster-crd/#using-pvc-storage-for-monitors","text":"<p>In the CRD specification below three monitors are created each using a 10Gi PVC created by Rook using the <code>local-storage</code> storage class.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n    volumeClaimTemplate:\n      spec:\n        storageClassName: local-storage\n        resources:\n          requests:\n            storage: 10Gi\n  dashboard:\n    enabled: true\n  storage:\n    useAllNodes: true\n    useAllDevices: true\n    deviceFilter:\n    config:\n      metadataDevice:\n      databaseSizeMB: \"1024\" # this value can be removed for environments with normal sized disks (100 GB or larger)\n      journalSizeMB: \"1024\"  # this value can be removed for environments with normal sized disks (20 GB or larger)\n      osdsPerDevice: \"1\"\n</code></pre>","title":"Using PVC storage for monitors"},{"location":"CRDs/ceph-cluster-crd/#using-storageclassdevicesets","text":"<p>In the CRD specification below, 3 OSDs (having specific placement and resource values) and 3 mons with each using a 10Gi PVC, are created by Rook using the <code>local-storage</code> storage class.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  dataDirHostPath: /var/lib/rook\n  mon:\n    count: 3\n    allowMultiplePerNode: false\n    volumeClaimTemplate:\n      spec:\n        storageClassName: local-storage\n        resources:\n          requests:\n            storage: 10Gi\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7\n    allowUnsupported: false\n  dashboard:\n    enabled: true\n  network:\n    hostNetwork: false\n  storage:\n    storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      resources:\n        limits:\n          cpu: \"500m\"\n          memory: \"4Gi\"\n        requests:\n          cpu: \"500m\"\n          memory: \"4Gi\"\n      placement:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: \"rook.io/cluster\"\n                  operator: In\n                  values:\n                    - cluster1\n              topologyKey: \"topology.kubernetes.io/zone\"\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          storageClassName: local-storage\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n</code></pre>","title":"Using StorageClassDeviceSets"},{"location":"CRDs/ceph-cluster-crd/#dedicated-metadata-and-wal-device-for-osd-on-pvc","text":"<p>In the simplest case, Ceph OSD BlueStore consumes a single (primary) storage device. BlueStore is the engine used by the OSD to store data.</p> <p>The storage device is normally used as a whole, occupying the full device that is managed directly by BlueStore. It is also possible to deploy BlueStore across additional devices such as a DB device. This device can be used for storing BlueStore\u2019s internal metadata. BlueStore (or rather, the embedded RocksDB) will put as much metadata as it can on the DB device to improve performance. If the DB device fills up, metadata will spill back onto the primary device (where it would have been otherwise). Again, it is only helpful to provision a DB device if it is faster than the primary device.</p> <p>You can have multiple <code>volumeClaimTemplates</code> where each might either represent a device or a metadata device. So just taking the <code>storage</code> section this will give something like:</p> <pre><code>  storage:\n   storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)\n          storageClassName: gp2\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n      - metadata:\n          name: metadata\n        spec:\n          resources:\n            requests:\n              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing\n              storage: 5Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)\n          storageClassName: io1\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n</code></pre>   <p>NOTE: Note that Rook only supports three naming convention for a given template:</p>  <ul> <li>\"data\": represents the main OSD block device, where your data is being stored.</li> <li>\"metadata\": represents the metadata (including block.db and block.wal) device used to store the Ceph Bluestore database for an OSD.</li> <li>\"wal\": represents the block.wal device used to store the Ceph Bluestore database for an OSD. If this device is set, \"metadata\" device will refer specifically to block.db device. It is recommended to use a faster storage class for the metadata or wal device, with a slower device for the data. Otherwise, having a separate metadata device will not improve the performance.</li> </ul> <p>The bluestore partition has the following reference combinations supported by the ceph-volume utility:</p> <ul> <li>A single \"data\" device.</li> </ul> <pre><code>  storage:\n    storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)\n          storageClassName: gp2\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n</code></pre>  <ul> <li>A \"data\" device and a \"metadata\" device.</li> </ul> <pre><code>  storage:\n    storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)\n          storageClassName: gp2\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n      - metadata:\n          name: metadata\n        spec:\n          resources:\n            requests:\n              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing\n              storage: 5Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)\n          storageClassName: io1\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n</code></pre>  <ul> <li>A \"data\" device and a \"wal\" device. A WAL device can be used for BlueStore\u2019s internal journal or write-ahead log (block.wal), it is only useful to use a WAL device if the device is faster than the primary device (data device). There is no separate \"metadata\" device in this case, the data of main OSD block and block.db located in \"data\" device.</li> </ul> <pre><code>  storage:\n    storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)\n          storageClassName: gp2\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n      - metadata:\n          name: wal\n        spec:\n          resources:\n            requests:\n              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing\n              storage: 5Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)\n          storageClassName: io1\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n</code></pre>  <ul> <li>A \"data\" device, a \"metadata\" device and a \"wal\" device.</li> </ul> <pre><code>  storage:\n    storageClassDeviceSets:\n    - name: set1\n      count: 3\n      portable: false\n      volumeClaimTemplates:\n      - metadata:\n          name: data\n        spec:\n          resources:\n            requests:\n              storage: 10Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)\n          storageClassName: gp2\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n      - metadata:\n          name: metadata\n        spec:\n          resources:\n            requests:\n              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing\n              storage: 5Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)\n          storageClassName: io1\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n      - metadata:\n          name: wal\n        spec:\n          resources:\n            requests:\n              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing\n              storage: 5Gi\n          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)\n          storageClassName: io1\n          volumeMode: Block\n          accessModes:\n            - ReadWriteOnce\n</code></pre>  <p>To determine the size of the metadata block follow the official Ceph sizing guide.</p> <p>With the present configuration, each OSD will have its main block allocated a 10GB device as well a 5GB device to act as a bluestore database.</p>","title":"Dedicated metadata and wal device for OSD on PVC"},{"location":"CRDs/ceph-cluster-crd/#external-cluster","text":"<p>The minimum supported Ceph version for the External Cluster is Luminous 12.2.x.</p> <p>The features available from the external cluster will vary depending on the version of Ceph. The following table shows the minimum version of Ceph for some of the features:</p>    FEATURE CEPH VERSION     Dynamic provisioning RBD 12.2.X   Configure extra CRDs (object, file, nfs)1 13.2.3   Dynamic provisioning CephFS 14.2.3","title":"External cluster"},{"location":"CRDs/ceph-cluster-crd/#external-cluster-configuration","text":"<ul> <li> <p>Source cluster: The cluster providing the data, usually configured by cephadm</p> </li> <li> <p>Consumer cluster: The K8s cluster that will be consuming the external source cluster</p> </li> </ul>","title":"External Cluster configuration"},{"location":"CRDs/ceph-cluster-crd/#commands-on-the-source-ceph-cluster","text":"<p>In order to configure an external Ceph cluster with Rook, we need to extract some information in order to connect to that cluster.</p> <ol> <li>Run the python script create-external-cluster-resources.py for creating all users and keys.</li> </ol> <pre><code>python3 create-external-cluster-resources.py --rbd-data-pool-name &lt;pool_name&gt; --cephfs-filesystem-name &lt;filesystem-name&gt; --rgw-endpoint  &lt;rgw-endpoint&gt; --namespace &lt;namespace&gt; --rgw-pool-prefix &lt;rgw-pool-prefix&gt; --format bash\n</code></pre>  <ul> <li><code>--namespace</code>: Namespace where CephCluster will run in the consumer cluster, for example <code>rook-ceph-external</code></li> <li><code>--format bash</code>: The format of the output</li> <li><code>--rbd-data-pool-name</code>: The name of the RBD data pool</li> <li><code>--cephfs-filesystem-name</code>: (optional) The name of the filesystem</li> <li><code>--rgw-endpoint</code>: (optional) The RADOS Gateway endpoint in the format <code>&lt;IP&gt;:&lt;PORT&gt;</code></li> <li> <p><code>--rgw-pool-prefix</code>: (optional) The prefix of the RGW pools. If not specified, the default prefix is <code>default</code>.</p> </li> <li> <p>Copy the bash output.</p> </li> </ul> <p>Example Output: <pre><code>export ROOK_EXTERNAL_FSID=797f411a-aafe-11ec-a254-fa163e1539f5\nexport ROOK_EXTERNAL_USERNAME=client.healthchecker\nexport ROOK_EXTERNAL_CEPH_MON_DATA=ceph-rados-upstream-w4pdvq-node1-installer=10.0.210.83:6789\nexport ROOK_EXTERNAL_USER_SECRET=AQAdm0FilZDSJxAAMucfuu/j0ZYYP4Bia8Us+w==\nexport ROOK_EXTERNAL_DASHBOARD_LINK=https://10.0.210.83:8443/\nexport CSI_RBD_NODE_SECRET=AQC1iDxip45JDRAAVahaBhKz1z0WW98+ACLqMQ==\nexport CSI_RBD_PROVISIONER_SECRET=AQC1iDxiMM+LLhAA0PucjNZI8sG9Eh+pcvnWhQ==\nexport MONITORING_ENDPOINT=10.0.210.83\nexport MONITORING_ENDPOINT_PORT=9283\nexport RBD_POOL_NAME=replicated_2g\nexport RGW_POOL_PREFIX=default\n</code></pre> </p>","title":"Commands on the source Ceph cluster"},{"location":"CRDs/ceph-cluster-crd/#commands-on-the-k8s-consumer-cluster","text":"<ol> <li> <p>Deploy Rook-Ceph, create common.yaml, crds.yaml and operator.yaml manifests.</p> </li> <li> <p>Paste the above output from <code>create-external-cluster-resources.py</code> into your current shell to allow importing the source data.</p> </li> <li> <p>Run the import script. <pre><code>. import-external-cluster.sh\n</code></pre> </p> </li> <li> <p>Create common-external.yaml and cluster-external.yaml</p> </li> <li> <p>Verify the consumer cluster is connected to the source ceph cluster: <pre><code>kubectl -n rook-ceph-external  get CephCluster\n</code></pre> </p>  <pre><code>NAME                 DATADIRHOSTPATH   MONCOUNT   AGE    STATE       HEALTH\nrook-ceph-external   /var/lib/rook                162m   Connected   HEALTH_OK\n</code></pre>   </li> </ol>","title":"Commands on the K8s consumer cluster"},{"location":"CRDs/ceph-cluster-crd/#create-storageclass","text":"<p>Create a StorageClass based on the pool that was already created in the source Ceph cluster. In this example, the pool <code>replicated_2g</code> exists in the source cluster.</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\n</code></pre>   <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: rook-ceph-block-ext\n# Change \"rook-ceph\" provisioner prefix to match the operator namespace if needed\nprovisioner: rook-ceph.rbd.csi.ceph.com\nparameters:\n    # clusterID is the namespace where the rook cluster is running\n    clusterID: rook-ceph-external\n    # Ceph pool into which the RBD image shall be created\n    pool: replicated_2g\n\n    # RBD image format. Defaults to \"2\".\n    imageFormat: \"2\"\n\n    # RBD image features. Available for imageFormat: \"2\". CSI RBD currently supports only `layering` feature.\n    imageFeatures: layering\n\n    # The secrets contain Ceph admin credentials.\n    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external\n    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\n    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external\n    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\n    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external\n\n    # Specify the filesystem type of the volume. If not specified, csi-provisioner\n    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock\n    # in hyperconverged settings where the volume is mounted on the same node as the osds.\n    csi.storage.k8s.io/fstype: ext4\n\n# Delete the rbd volume when a PVC is deleted\nreclaimPolicy: Delete\nallowVolumeExpansion: true\nEOF\n</code></pre>   <p>You can now create a persistent volume based on this StorageClass.</p>","title":"Create StorageClass"},{"location":"CRDs/ceph-cluster-crd/#cephcluster-example-management","text":"<p>The following CephCluster CR represents a cluster that will perform management tasks on the external cluster. It will not only act as a consumer but will also allow the deployment of other CRDs such as CephFilesystem or CephObjectStore. You would need to inject the admin keyring for that.</p> <p>The corresponding YAML example:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph-external\n  namespace: rook-ceph-external\nspec:\n  external:\n    enable: true\n  dataDirHostPath: /var/lib/rook\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.7 # Should match external cluster version\n</code></pre>","title":"CephCluster example (management)"},{"location":"CRDs/ceph-cluster-crd/#deleting-a-cephcluster","text":"<p>During deletion of a CephCluster resource, Rook protects against accidental or premature destruction of user data by blocking deletion if there are any other Rook-Ceph Custom Resources that reference the CephCluster being deleted. Rook will warn about which other resources are blocking deletion in three ways until all blocking resources are deleted: 1. An event will be registered on the CephCluster resource 1. A status condition will be added to the CephCluster resource 1. An error will be added to the Rook-Ceph Operator log</p>","title":"Deleting a CephCluster"},{"location":"CRDs/ceph-cluster-crd/#cleanup-policy","text":"<p>Rook has the ability to cleanup resources and data that were deployed when a CephCluster is removed. The policy settings indicate which data should be forcibly deleted and in what way the data should be wiped. The <code>cleanupPolicy</code> has several fields:</p> <ul> <li><code>confirmation</code>: Only an empty string and <code>yes-really-destroy-data</code> are valid values for this field.   If this setting is empty, the cleanupPolicy settings will be ignored and Rook will not cleanup any resources during cluster removal.   To reinstall the cluster, the admin would then be required to follow the cleanup guide to delete the data on hosts.   If this setting is <code>yes-really-destroy-data</code>, the operator will automatically delete the data on hosts.   Because this cleanup policy is destructive, after the confirmation is set to <code>yes-really-destroy-data</code>   Rook will stop configuring the cluster as if the cluster is about to be destroyed.</li> <li><code>sanitizeDisks</code>: sanitizeDisks represents advanced settings that can be used to delete data on drives.</li> <li><code>method</code>: indicates if the entire disk should be sanitized or simply ceph's metadata. Possible choices are 'quick' (default) or 'complete'</li> <li><code>dataSource</code>: indicate where to get random bytes from to write on the disk. Possible choices are 'zero' (default) or 'random'.   Using random sources will consume entropy from the system and will take much more time then the zero source</li> <li><code>iteration</code>: overwrite N times instead of the default (1). Takes an integer value</li> <li><code>allowUninstallWithVolumes</code>: If set to true, then the cephCluster deletion doesn't wait for the PVCs to be deleted. Default is false.</li> </ul> <p>To automate activation of the cleanup, you can use the following command. WARNING: DATA WILL BE PERMANENTLY DELETED:</p> <pre><code>kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n</code></pre>  <p>Nothing will happen until the deletion of the CR is requested, so this can still be reverted. However, all new configuration by the operator will be blocked with this cleanup policy enabled.</p> <p>Rook waits for the deletion of PVs provisioned using the cephCluster before proceeding to delete the cephCluster. To force deletion of the cephCluster without waiting for the PVs to be deleted, you can set the allowUninstallWithVolumes to true under spec.CleanupPolicy.</p>   <ol> <li> <p>Configure an object store, shared filesystem, or NFS resources in the local cluster to connect to the external Ceph cluster\u00a0\u21a9</p> </li> </ol>","title":"Cleanup policy"},{"location":"CRDs/ceph-filesystem-crd/","text":"<p>Rook allows creation and customization of shared filesystems through the custom resource definitions (CRDs). The following settings are available for Ceph filesystems.</p>","title":"Ceph Shared Filesystem CRD"},{"location":"CRDs/ceph-filesystem-crd/#samples","text":"","title":"Samples"},{"location":"CRDs/ceph-filesystem-crd/#replicated","text":"<p>NOTE: This sample requires at least 1 OSD per node, with each OSD located on 3 different nodes.</p>  <p>Each OSD must be located on a different node, because both of the defined pools set the <code>failureDomain</code> to <code>host</code> and the <code>replicated.size</code> to <code>3</code>.</p> <p>The <code>failureDomain</code> can also be set to another location type (e.g. <code>rack</code>), if it has been added as a <code>location</code> in the Storage Selection Settings.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephFilesystem\nmetadata:\n  name: myfs\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    failureDomain: host\n    replicated:\n      size: 3\n  dataPools:\n    - name: replicated\n      failureDomain: host\n      replicated:\n        size: 3\n  preserveFilesystemOnDelete: true\n  metadataServer:\n    activeCount: 1\n    activeStandby: true\n    # A key/value list of annotations\n    annotations:\n    #  key: value\n    placement:\n    #  nodeAffinity:\n    #    requiredDuringSchedulingIgnoredDuringExecution:\n    #      nodeSelectorTerms:\n    #      - matchExpressions:\n    #        - key: role\n    #          operator: In\n    #          values:\n    #          - mds-node\n    #  tolerations:\n    #  - key: mds-node\n    #    operator: Exists\n    #  podAffinity:\n    #  podAntiAffinity:\n    #  topologySpreadConstraints:\n    resources:\n    #  limits:\n    #    cpu: \"500m\"\n    #    memory: \"1024Mi\"\n    #  requests:\n    #    cpu: \"500m\"\n    #    memory: \"1024Mi\"\n</code></pre>  <p>(These definitions can also be found in the <code>filesystem.yaml</code> file)</p>","title":"Replicated"},{"location":"CRDs/ceph-filesystem-crd/#erasure-coded","text":"<p>Erasure coded pools require the OSDs to use <code>bluestore</code> for the configured <code>storeType</code>. Additionally, erasure coded pools can only be used with <code>dataPools</code>. The <code>metadataPool</code> must use a replicated pool.</p>  <p>NOTE: This sample requires at least 3 bluestore OSDs, with each OSD located on a different node.</p>  <p>The OSDs must be located on different nodes, because the <code>failureDomain</code> will be set to <code>host</code> by default, and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephFilesystem\nmetadata:\n  name: myfs-ec\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    replicated:\n      size: 3\n  dataPools:\n    - name: default\n      replicated:\n        size: 3\n    - name: erasurecoded\n      erasureCoded:\n        dataChunks: 2\n        codingChunks: 1\n  metadataServer:\n    activeCount: 1\n    activeStandby: true\n</code></pre>  <p>IMPORTANT: For erasure coded pools, we have to create a replicated pool as the default data pool and an erasure-coded pool as a secondary pool.</p> <p>(These definitions can also be found in the <code>filesystem-ec.yaml</code> file. Also see an example in the <code>storageclass-ec.yaml</code> for how to configure the volume.)</p>","title":"Erasure Coded"},{"location":"CRDs/ceph-filesystem-crd/#mirroring","text":"<p>Ceph filesystem mirroring is a process of asynchronous replication of snapshots to a remote CephFS file system. Snapshots are synchronized by mirroring snapshot data followed by creating a snapshot with the same name (for a given directory on the remote file system) as the snapshot being synchronized. It is generally useful when planning for Disaster Recovery. Mirroring is for clusters that are geographically distributed and stretching a single cluster is not possible due to high latencies.</p> <p>The following will enable mirroring of the filesystem:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephFilesystem\nmetadata:\n  name: myfs\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    failureDomain: host\n    replicated:\n      size: 3\n  dataPools:\n    - name: replicated\n      failureDomain: host\n      replicated:\n        size: 3\n  preserveFilesystemOnDelete: true\n  metadataServer:\n    activeCount: 1\n    activeStandby: true\n  mirroring:\n    enabled: true\n    # list of Kubernetes Secrets containing the peer token\n    # for more details see: https://docs.ceph.com/en/latest/dev/cephfs-mirroring/#bootstrap-peers\n    peers:\n      secretNames:\n        - secondary-cluster-peer\n    # specify the schedule(s) on which snapshots should be taken\n    # see the official syntax here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules\n    snapshotSchedules:\n      - path: /\n        interval: 24h # daily snapshots\n        startTime: 11:55\n    # manage retention policies\n    # see syntax duration here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies\n    snapshotRetention:\n      - path: /\n        duration: \"h 24\"\n</code></pre>  <p>Once mirroring is enabled, Rook will by default create its own bootstrap peer token so that it can be used by another cluster. The bootstrap peer token can be found in a Kubernetes Secret. The name of the Secret is present in the Status field of the CephFilesystem CR:</p> <pre><code>status:\n  info:\n    fsMirrorBootstrapPeerSecretName: fs-peer-token-myfs\n</code></pre>  <p>This secret can then be fetched like so:</p> <pre><code>kubectl get secret -n rook-ceph fs-peer-token-myfs -o jsonpath='{.data.token}'|base64 -d\n</code></pre>   <pre><code>eyJmc2lkIjoiOTFlYWUwZGQtMDZiMS00ZDJjLTkxZjMtMTMxMWM5ZGYzODJiIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFEN1psOWZ3V1VGRHhBQWdmY0gyZi8xeUhYeGZDUTU5L1N0NEE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjEwMS4xOC4yMjM6MzMwMCx2MToxMC4xMDEuMTguMjIzOjY3ODldIn0=\n</code></pre>   <p>The secret must be decoded. The result will be another base64 encoded blob that you will import in the destination cluster:</p> <pre><code>external-cluster-console # ceph fs snapshot mirror peer_bootstrap import &lt;fs_name&gt; &lt;token file path&gt;\n</code></pre>  <p>See the official cephfs mirror documentation on how to add a bootstrap peer.</p>","title":"Mirroring"},{"location":"CRDs/ceph-filesystem-crd/#filesystem-settings","text":"","title":"Filesystem Settings"},{"location":"CRDs/ceph-filesystem-crd/#metadata","text":"<ul> <li><code>name</code>: The name of the filesystem to create, which will be reflected in the pool and other resource names.</li> <li><code>namespace</code>: The namespace of the Rook cluster where the filesystem is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-filesystem-crd/#pools","text":"<p>The pools allow all of the settings defined in the Pool CRD spec. For more details, see the Pool CRD settings. In the example above, there must be at least three hosts (size 3) and at least eight devices (6 data + 2 coding chunks) in the cluster.</p> <ul> <li><code>metadataPool</code>: The settings used to create the filesystem metadata pool. Must use replication.</li> <li><code>dataPools</code>: The settings to create the filesystem data pools. Optionally (and we highly recommend), a pool name can be specified with the <code>name</code> field to override the default generated name; see more below. If multiple pools are specified, Rook will add the pools to the filesystem. Assigning users or files to a pool is left as an exercise for the reader with the CephFS documentation. The data pools can use replication or erasure coding. If erasure coding pools are specified, the cluster must be running with bluestore enabled on the OSDs.</li> <li><code>name</code>: (optional, and highly recommended) Override the default generated name of the pool. The final pool name will consist of the filesystem name and pool name, e.g., <code>&lt;fsName&gt;-&lt;poolName&gt;</code>. We highly recommend to specify <code>name</code> to prevent issues that can arise from modifying the spec in a way that causes Rook to lose the original pool ordering.</li> <li><code>preserveFilesystemOnDelete</code>: If it is set to 'true' the filesystem will remain when the   CephFilesystem resource is deleted. This is a security measure to avoid loss of data if the   CephFilesystem resource is deleted accidentally. The default value is 'false'. This option   replaces <code>preservePoolsOnDelete</code> which should no longer be set.</li> <li>(deprecated) <code>preservePoolsOnDelete</code>: This option is replaced by the above <code>preserveFilesystemOnDelete</code>. For backwards compatibility and upgradeability, if this is set to   'true', Rook will treat <code>preserveFilesystemOnDelete</code> as being set to 'true'.</li> </ul>","title":"Pools"},{"location":"CRDs/ceph-filesystem-crd/#metadata-server-settings","text":"<p>The metadata server settings correspond to the MDS daemon settings.</p> <ul> <li><code>activeCount</code>: The number of active MDS instances. As load increases, CephFS will automatically partition the filesystem across the MDS instances. Rook will create double the number of MDS instances as requested by the active count. The extra instances will be in standby mode for failover.</li> <li><code>activeStandby</code>: If true, the extra MDS instances will be in active standby mode and will keep a warm cache of the filesystem metadata for faster failover. The instances will be assigned by CephFS in failover pairs. If false, the extra MDS instances will all be on passive standby mode and will not maintain a warm cache of the metadata.</li> <li><code>mirroring</code>: Sets up mirroring of the filesystem</li> <li><code>enabled</code>: whether mirroring is enabled on that filesystem (default: false)</li> <li><code>peers</code>: to configure mirroring peers<ul> <li><code>secretNames</code>:  a list of peers to connect to. Currently (Ceph Pacific release) only a single peer is supported where a peer represents a Ceph cluster.</li> </ul> </li> <li><code>snapshotSchedules</code>: schedule(s) snapshot.One or more schedules are supported.<ul> <li><code>path</code>: filesystem source path to take the snapshot on</li> <li><code>interval</code>: frequency of the snapshots. The interval can be specified in days, hours, or minutes using d, h, m suffix respectively.</li> <li><code>startTime</code>: optional, determines at what time the snapshot process starts, specified using the ISO 8601 time format.</li> </ul> </li> <li><code>snapshotRetention</code>: allow to manage retention policies:<ul> <li><code>path</code>: filesystem source path to apply the retention on</li> <li><code>duration</code>:</li> </ul> </li> <li><code>annotations</code>: Key value pair list of annotations to add.</li> <li><code>labels</code>: Key value pair list of labels to add.</li> <li><code>placement</code>: The mds pods can be given standard Kubernetes placement restrictions with <code>nodeAffinity</code>, <code>tolerations</code>, <code>podAffinity</code>, and <code>podAntiAffinity</code> similar to placement defined for daemons configured by the cluster CRD.</li> <li><code>resources</code>: Set resource requests/limits for the Filesystem MDS Pod(s), see MDS Resources Configuration Settings</li> <li><code>priorityClassName</code>: Set priority class name for the Filesystem MDS Pod(s)</li> <li><code>startupProbe</code> : Disable, or override timing and threshold values of the Filesystem MDS startup probe</li> <li><code>livenessProbe</code> : Disable, or override timing and threshold values of the Filesystem MDS livenessProbe.</li> </ul>","title":"Metadata Server Settings"},{"location":"CRDs/ceph-filesystem-crd/#mds-resources-configuration-settings","text":"<p>The format of the resource requests/limits structure is the same as described in the Ceph Cluster CRD documentation.</p> <p>If the memory resource limit is declared Rook will automatically set the MDS configuration <code>mds_cache_memory_limit</code>. The configuration value is calculated with the aim that the actual MDS memory consumption remains consistent with the MDS pods' resource declaration.</p> <p>In order to provide the best possible experience running Ceph in containers, Rook internally recommends the memory for MDS daemons to be at least 4096MB. If a user configures a limit or request value that is too low, Rook will still run the pod(s) and print a warning to the operator log.</p>","title":"MDS Resources Configuration Settings"},{"location":"CRDs/ceph-fs-mirror-crd/","text":"<p>This guide assumes you have created a Rook cluster as explained in the main Quickstart guide</p>","title":"Filesystem Mirror CRD"},{"location":"CRDs/ceph-fs-mirror-crd/#ceph-filesystemmirror-crd","text":"<p>Rook allows creation and updating the fs-mirror daemon through the custom resource definitions (CRDs). CephFS will support asynchronous replication of snapshots to a remote (different Ceph cluster) CephFS file system via cephfs-mirror tool. Snapshots are synchronized by mirroring snapshot data followed by creating a snapshot with the same name (for a given directory on the remote file system) as the snapshot being synchronized. For more information about user management and capabilities see the Ceph docs.</p>","title":"Ceph FilesystemMirror CRD"},{"location":"CRDs/ceph-fs-mirror-crd/#creating-daemon","text":"<p>To get you started, here is a simple example of a CRD to deploy an cephfs-mirror daemon.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephFilesystemMirror\nmetadata:\n  name: my-fs-mirror\n  namespace: rook-ceph\n</code></pre>","title":"Creating daemon"},{"location":"CRDs/ceph-fs-mirror-crd/#settings","text":"<p>If any setting is unspecified, a suitable default will be used automatically.</p>","title":"Settings"},{"location":"CRDs/ceph-fs-mirror-crd/#filesystemmirror-metadata","text":"<ul> <li><code>name</code>: The name that will be used for the Ceph cephfs-mirror daemon.</li> <li><code>namespace</code>: The Kubernetes namespace that will be created for the Rook cluster. The services, pods, and other resources created by the operator will be added to this namespace.</li> </ul>","title":"FilesystemMirror metadata"},{"location":"CRDs/ceph-fs-mirror-crd/#filesystemmirror-settings","text":"<ul> <li><code>placement</code>: The cephfs-mirror pods can be given standard Kubernetes placement restrictions with <code>nodeAffinity</code>, <code>tolerations</code>, <code>podAffinity</code>, and <code>podAntiAffinity</code> similar to placement defined for daemons configured by the cluster CRD.</li> <li><code>annotations</code>: Key value pair list of annotations to add.</li> <li><code>labels</code>: Key value pair list of labels to add.</li> <li><code>resources</code>: The resource requirements for the cephfs-mirror pods.</li> <li><code>priorityClassName</code>: The priority class to set on the cephfs-mirror pods.</li> </ul>","title":"FilesystemMirror Settings"},{"location":"CRDs/ceph-fs-mirror-crd/#configuring-mirroring-peers","text":"<p>In order to configure mirroring peers, please refer to the CephFilesystem documentation.</p>","title":"Configuring mirroring peers"},{"location":"CRDs/ceph-fs-subvolumegroup/","text":"<p>This guide assumes you have created a Rook cluster as explained in the main Quickstart guide</p>","title":"SubVolume Group CRD"},{"location":"CRDs/ceph-fs-subvolumegroup/#cephfilesystemsubvolumegroup-crd","text":"<p>Rook allows creation of Ceph Filesystem SubVolumeGroups through the custom resource definitions (CRDs). Filesystem subvolume groups are an abstraction for a directory level higher than Filesystem subvolumes to effect policies (e.g., File layouts) across a set of subvolumes. For more information about CephFS volume, subvolumegroup and subvolume refer to the Ceph docs.</p>","title":"CephFilesystemSubVolumeGroup CRD"},{"location":"CRDs/ceph-fs-subvolumegroup/#creating-daemon","text":"<p>To get you started, here is a simple example of a CRD to create a subvolumegroup on the CephFilesystem \"myfs\".</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephFilesystemSubVolumeGroup\nmetadata:\n  name: group-a\n  namespace: rook-ceph # namespace:cluster\nspec:\n  # filesystemName is the metadata name of the CephFilesystem CR where the subvolume group will be created\n  filesystemName: myfs\n</code></pre>","title":"Creating daemon"},{"location":"CRDs/ceph-fs-subvolumegroup/#settings","text":"<p>If any setting is unspecified, a suitable default will be used automatically.</p>","title":"Settings"},{"location":"CRDs/ceph-fs-subvolumegroup/#cephfilesystemsubvolumegroup-metadata","text":"<ul> <li><code>name</code>: The name that will be used for the Ceph Filesystem subvolume group.</li> </ul>","title":"CephFilesystemSubVolumeGroup metadata"},{"location":"CRDs/ceph-fs-subvolumegroup/#cephfilesystemsubvolumegroup-spec","text":"<ul> <li><code>filesystemName</code>: The metadata name of the CephFilesystem CR where the subvolume group will be created.</li> </ul>","title":"CephFilesystemSubVolumeGroup spec"},{"location":"CRDs/ceph-nfs-crd/","text":"","title":"Ceph NFS Server CRD"},{"location":"CRDs/ceph-nfs-crd/#overview","text":"<p>Rook allows exporting NFS shares of a CephFilesystem or CephObjectStore through the CephNFS custom resource definition. This will spin up a cluster of NFS Ganesha servers that coordinate with one another via shared RADOS objects. The servers will be configured for NFSv4.1+ access only, as serving earlier protocols can inhibit responsiveness after a server restart.</p>  <p>WARNING: Due to a number of Ceph issues and changes, Rook officially only supports Ceph v16.2.7 or higher for CephNFS. If you are using an earlier version, upgrade your Ceph version following the advice given in Rook's v1.8 NFS docs.</p>","title":"Overview"},{"location":"CRDs/ceph-nfs-crd/#samples","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephNFS\nmetadata:\n  name: my-nfs\n  namespace: rook-ceph\nspec:\n  # Settings for the NFS server\n  server:\n    active: 1\n\n    placement:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: role\n              operator: In\n              values:\n              - nfs-node\n      topologySpreadConstraints:\n      tolerations:\n      - key: nfs-node\n        operator: Exists\n      podAffinity:\n      podAntiAffinity:\n\n    annotations:\n      my-annotation: something\n\n    labels:\n      my-label: something\n\n    resources:\n      limits:\n        cpu: \"500m\"\n        memory: \"1024Mi\"\n      requests:\n        cpu: \"500m\"\n        memory: \"1024Mi\"\n\n    priorityClassName:\n\n    logLevel: NIV_INFO\n</code></pre>","title":"Samples"},{"location":"CRDs/ceph-nfs-crd/#nfs-settings","text":"","title":"NFS Settings"},{"location":"CRDs/ceph-nfs-crd/#server","text":"<p>The <code>server</code> spec sets configuration for Rook-created NFS-Ganesha servers.</p> <ul> <li><code>active</code>: The number of active NFS servers. Rook supports creating more than one active NFS   server, but cannot guarantee high availability. For values greater than 1, see the known issue below.</li> <li><code>placement</code>: Kubernetes placement restrictions to apply to NFS server Pod(s). This is similar to   placement defined for daemons configured by the CephCluster CRD.</li> <li><code>annotations</code>: Kubernetes annotations to apply to NFS server Pod(s)</li> <li><code>labels</code>: Kubernetes labels to apply to NFS server Pod(s)</li> <li><code>resources</code>: Kubernetes resource requests and limits to set on NFS server Pod(s)</li> <li><code>priorityClassName</code>: Set priority class name for the NFS server Pod(s)</li> <li><code>logLevel</code>: The log level that NFS-Ganesha servers should output.   Default value: NIV_INFO   Supported values: NIV_NULL | NIV_FATAL | NIV_MAJ | NIV_CRIT | NIV_WARN | NIV_EVENT | NIV_INFO | NIV_DEBUG | NIV_MID_DEBUG | NIV_FULL_DEBUG | NB_LOG_LEVEL</li> </ul>","title":"Server"},{"location":"CRDs/ceph-nfs-crd/#creating-exports","text":"<p>When a CephNFS is first created, all NFS daemons within the CephNFS cluster will share a configuration with no exports defined.</p>","title":"Creating Exports"},{"location":"CRDs/ceph-nfs-crd/#using-the-ceph-dashboard","text":"<p>Exports can be created via the Ceph dashboard for Ceph v16 as well. To enable and use the Ceph dashboard in Rook, see here.</p>","title":"Using the Ceph Dashboard"},{"location":"CRDs/ceph-nfs-crd/#using-the-ceph-cli","text":"<p>The Ceph CLI can be used from the Rook toolbox pod to create and manage NFS exports. To do so, first ensure the necessary Ceph mgr modules are enabled, if necessary, and that the Ceph orchestrator backend is set to Rook.</p>","title":"Using the Ceph CLI"},{"location":"CRDs/ceph-nfs-crd/#enable-the-ceph-orchestrator-if-necessary","text":"<ul> <li>Required for Ceph v16.2.7 and below</li> <li>Optional for Ceph v16.2.8 and above</li> <li>Must be disabled for Ceph v17.2.0 due to a Ceph regression <pre><code>ceph mgr module enable rook\nceph mgr module enable nfs\nceph orch set backend rook\n</code></pre> </li> </ul>  <p>Ceph's NFS CLI can create NFS exports that are backed by CephFS (a CephFilesystem) or Ceph Object Gateway (a CephObjectStore). <code>cluster_id</code> or <code>cluster-name</code> in the Ceph NFS docs normally refers to the name of the NFS cluster, which is the CephNFS name in the Rook context.</p> <p>For creating an NFS export for the CephNFS and CephFilesystem example manifests, the below command can be used. This creates an export for the <code>/test</code> pseudo path. <pre><code>ceph nfs export create cephfs my-nfs /test myfs\n</code></pre> </p> <p>The below command will list the current NFS exports for the example CephNFS cluster, which will give the output shown for the current example. <pre><code>ceph nfs export ls my-nfs\n</code></pre>  <pre><code>[\n  \"/test\"\n]\n</code></pre> </p> <p>The simple <code>/test</code> export's info can be listed as well. Notice from the example that only NFS protocol v4 via TCP is supported. <pre><code>ceph nfs export info my-nfs /test\n</code></pre>  <pre><code>{\n  \"export_id\": 1,\n  \"path\": \"/\",\n  \"cluster_id\": \"my-nfs\",\n  \"pseudo\": \"/test\",\n  \"access_type\": \"RW\",\n  \"squash\": \"none\",\n  \"security_label\": true,\n  \"protocols\": [\n    4\n  ],\n  \"transports\": [\n    \"TCP\"\n  ],\n  \"fsal\": {\n    \"name\": \"CEPH\",\n    \"user_id\": \"nfs.my-nfs.1\",\n    \"fs_name\": \"myfs\"\n  },\n  \"clients\": []\n}\n</code></pre> </p> <p>If you are done managing NFS exports and don't need the Ceph orchestrator module enabled for anything else, it may be preferable to disable the Rook and NFS mgr modules to free up a small amount of RAM in the Ceph mgr Pod. <pre><code>ceph orch set backend \"\"\nceph mgr module disable rook\n</code></pre> </p>","title":"Enable the Ceph orchestrator if necessary"},{"location":"CRDs/ceph-nfs-crd/#mounting-exports","text":"<p>Each CephNFS server has a unique Kubernetes Service. This is because NFS clients can't readily handle NFS failover. CephNFS services are named with the pattern <code>rook-ceph-nfs-&lt;cephnfs-name&gt;-&lt;id&gt;</code> <code>&lt;id&gt;</code> is a unique letter ID (e.g., a, b, c, etc.) for a given NFS server. For example, <code>rook-ceph-nfs-my-nfs-a</code>.</p> <p>For each NFS client, choose an NFS service to use for the connection. With NFS v4, you can mount an export by its path using a mount command like below. You can mount all exports at once by omitting the export path and leaving the directory as just <code>/</code>. <pre><code>mount -t nfs4 -o proto=tcp &lt;nfs-service-address&gt;:/&lt;export-path&gt; &lt;mount-location&gt;\n</code></pre> </p>","title":"Mounting exports"},{"location":"CRDs/ceph-nfs-crd/#exposing-the-nfs-server-outside-of-the-kubernetes-cluster","text":"<p>Use a LoadBalancer Service to expose an NFS server (and its exports) outside of the Kubernetes cluster. The Service's endpoint can be used as the NFS service address when mounting the export manually. We provide an example Service here: <code>deploy/examples/nfs-load-balancer.yaml</code>.</p>","title":"Exposing the NFS server outside of the Kubernetes cluster"},{"location":"CRDs/ceph-nfs-crd/#scaling-the-active-server-count","text":"<p>It is possible to scale the size of the cluster up or down by modifying the <code>spec.server.active</code> field. Scaling the cluster size up can be done at will. Once the new server comes up, clients can be assigned to it immediately.</p> <p>The CRD always eliminates the highest index servers first, in reverse order from how they were started. Scaling down the cluster requires that clients be migrated from servers that will be eliminated to others. That process is currently a manual one and should be performed before reducing the size of the cluster.</p>  <p>WARNING: see the known issue below about setting this value greater than one.</p>","title":"Scaling the active server count"},{"location":"CRDs/ceph-nfs-crd/#known-issues","text":"","title":"Known issues"},{"location":"CRDs/ceph-nfs-crd/#serveractive-count-greater-than-1","text":"<ul> <li>Active-active scale out does not work well with the NFS protocol. If one NFS server in a cluster   is offline, other servers may block client requests until the offline server returns, which may   not always happen due to the Kubernetes scheduler.</li> <li>Workaround: It is safest to run only a single NFS server, but we do not limit this if it     benefits your use case.</li> </ul>","title":"server.active count greater than 1"},{"location":"CRDs/ceph-nfs-crd/#ceph-v1720","text":"<ul> <li>Ceph NFS management with the Rook mgr module enabled has a breaking regression with the Ceph   Quincy v17.2.0 release.</li> <li>Workaround: Leave Ceph's Rook orchestrator mgr module disabled. If you have enabled it, you must     disable it using the snippet below from the toolbox. <pre><code>ceph orch set backend \"\"\nceph mgr module disable rook\n</code></pre> </li> </ul>","title":"Ceph v17.2.0"},{"location":"CRDs/ceph-nfs-crd/#advanced-configuration","text":"<p>All CephNFS daemons are configured using shared RADOS objects stored in a Ceph pool named <code>.nfs</code>. Users can modify the configuration object for each CephNFS cluster if they wish to customize the configuration.</p>","title":"Advanced configuration"},{"location":"CRDs/ceph-nfs-crd/#changing-configuration-of-the-nfs-pool","text":"<p>By default, Rook creates the <code>.nfs</code> pool with Ceph's default configuration. If you wish to change the configuration of this pool (for example to change its failure domain or replication factor), you can create a CephBlockPool with the <code>spec.name</code> field set to <code>.nfs</code>. This pool must be replicated and cannot be erasure coded. <code>deploy/examples/nfs.yaml</code> contains a sample for reference.</p>","title":"Changing configuration of the .nfs pool"},{"location":"CRDs/ceph-nfs-crd/#adding-custom-nfs-ganesha-config-file-changes","text":"<p>The NFS-Ganesha config file format for these objects is documented in the NFS-Ganesha project.</p> <p>Use Ceph's <code>rados</code> tool from the toolbox to interact with the configuration object. The below command will get you started by dumping the contents of the config object to stdout. The output will look something like the example shown if you have already created two exports as documented above. It is best not to modify any of the export objects created by Ceph so as not to cause errors with Ceph's export management.</p> <p><pre><code>rados --pool &lt;pool&gt; --namespace &lt;namespace&gt; get conf-nfs.&lt;cephnfs-name&gt; -\n</code></pre>  <pre><code>%url \"rados://&lt;pool&gt;/&lt;namespace&gt;/export-1\"\n%url \"rados://&lt;pool&gt;/&lt;namespace&gt;/export-2\"\n</code></pre> </p> <p><code>rados ls</code> and <code>rados put</code> are other commands you will want to work with the other shared configuration objects.</p> <p>Of note, it is possible to pre-populate the NFS configuration and export objects prior to creating CephNFS server clusters.</p>","title":"Adding custom NFS-Ganesha config file changes"},{"location":"CRDs/ceph-nfs-crd/#ceph-csi-nfs-provisioner-and-nfs-csi-driver","text":"<p>EXPERIMENTAL: this feature is experimental, and we do not guarantee it is bug-free, nor will we support upgrades to future versions</p>  <p>In version 1.9.1, Rook is able to deploy the experimental NFS Ceph CSI driver. This requires Ceph CSI version 3.6.0 or above. We recommend Ceph v16.2.7 or above.</p> <p>For this section, we will refer to Rook's deployment examples in the deploy/examples directory.</p> <p>The Ceph CSI NFS provisioner and driver require additional RBAC to operate. Apply the <code>deploy/examples/csi/nfs/rbac.yaml</code> manifest to deploy the additional resources.</p> <p>Rook will only deploy the Ceph CSI NFS provisioner and driver components when the <code>ROOK_CSI_ENABLE_NFS</code> config is set to <code>\"true\"</code> in the <code>rook-ceph-operator-config</code> configmap. Change the value in your manifest, or patch the resource as below. <pre><code>kubectl --namespace rook-ceph patch configmap rook-ceph-operator-config --type merge --patch '{\"data\":{\"ROOK_CSI_ENABLE_NFS\": \"true\"}}'\n</code></pre> </p>  <p>NOTE: The rook-ceph operator Helm chart will deploy the required RBAC and enable the driver components if <code>csi.nfs.enabled</code> is set to <code>true</code>.</p>  <p>In order to create NFS exports via the CSI driver, you must first create a CephFilesystem to serve as the underlying storage for the exports, and you must create a CephNFS to run an NFS server that will expose the exports.</p> <p>From the examples, <code>filesystem.yaml</code> creates a CephFilesystem called <code>myfs</code>, and <code>nfs.yaml</code> creates an NFS server called <code>my-nfs</code>.</p> <p>You may need to enable or disable the Ceph orchestrator. Follow the same steps documented above based on your Ceph version and desires.</p> <p>You must also create a storage class. Ceph CSI is designed to support any arbitrary Ceph cluster, but we are focused here only on Ceph clusters deployed by Rook. Let's take a look at a portion of the example storage class found at <code>deploy/examples/csi/nfs/storageclass.yaml</code> and break down how the values are determined.</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-nfs\nprovisioner: rook-ceph.nfs.csi.ceph.com # [1]\nparameters:\n  nfsCluster: my-nfs # [2]\n  server: rook-ceph-nfs-my-nfs-a # [3]\n  clusterID: rook-ceph # [4]\n  fsName: myfs # [5]\n  pool: myfs-replicated # [6]\n\n  # [7] (entire csi.storage.k8s.io/* section immediately below)\n  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner\n  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner\n  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph\n  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node\n  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\n\n# ... some fields omitted ...\n</code></pre>  <ol> <li><code>provisioner</code>: rook-ceph.nfs.csi.ceph.com because rook-ceph is the namespace where the    CephCluster is installed</li> <li><code>nfsCluster</code>: my-nfs because this is the name of the CephNFS</li> <li><code>server</code>: rook-ceph-nfs-my-nfs-a because Rook creates this Kubernetes Service for the CephNFS    named my-nfs</li> <li><code>clusterID</code>: rook-ceph because this is the namespace where the CephCluster is installed</li> <li><code>fsName</code>: myfs because this is the name of the CephFilesystem used to back the NFS exports</li> <li><code>pool</code>: myfs-replicated because myfs is the name of the CephFilesystem defined in <code>fsName</code> and because replicated is the name of a data pool defined in the CephFilesystem</li> <li><code>csi.storage.k8s.io/*</code>: note that these values are shared with the Ceph CSI CephFS provisioner</li> </ol> <p>See <code>deploy/examples/csi/nfs/pvc.yaml</code> for an example of how to create a PVC that will create an NFS export. The export will be created and a PV created for the PVC immediately, even without a Pod to mount the PVC. The <code>share</code> parameter set on the resulting PV contains the share path (<code>share</code>) which can be used as the export path when mounting the export manually.</p> <p>See <code>deploy/examples/csi/nfs/pod.yaml</code> for an example of how a PVC can be connected to an application pod.</p>","title":"Ceph CSI NFS provisioner and NFS CSI driver"},{"location":"CRDs/ceph-object-multisite-crd/","text":"<p>The following CRDs enable Ceph object stores to isolate or replicate data via multisite. For more information on multisite, visit the ceph-object-multisite documentation.</p>","title":"Ceph Object Multisite CRDs"},{"location":"CRDs/ceph-object-multisite-crd/#ceph-object-realm-crd","text":"<p>Rook allows creation of a realm in a ceph cluster for object stores through the custom resource definitions (CRDs). The following settings are available for Ceph object store realms.</p>","title":"Ceph Object Realm CRD"},{"location":"CRDs/ceph-object-multisite-crd/#sample","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectRealm\nmetadata:\n  name: realm-a\n  namespace: rook-ceph\n# This endpoint in this section needs is an endpoint from the master zone  in the master zone group of realm-a. See object-multisite.md for more details.\nspec:\n  pull:\n    endpoint: http://10.2.105.133:80\n</code></pre>","title":"Sample"},{"location":"CRDs/ceph-object-multisite-crd/#object-realm-settings","text":"","title":"Object Realm Settings"},{"location":"CRDs/ceph-object-multisite-crd/#metadata","text":"<ul> <li><code>name</code>: The name of the object realm to create</li> <li><code>namespace</code>: The namespace of the Rook cluster where the object realm is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-object-multisite-crd/#spec","text":"<ul> <li><code>pull</code>: This optional section is for the pulling the realm for another ceph cluster.</li> <li><code>endpoint</code>: The endpoint in the realm from another ceph cluster you want to pull from. This endpoint must be in the master zone of the master zone group of the realm.</li> </ul>","title":"Spec"},{"location":"CRDs/ceph-object-multisite-crd/#ceph-object-zone-group-crd","text":"<p>Rook allows creation of zone groups in a ceph cluster for object stores through the custom resource definitions (CRDs). The following settings are available for Ceph object store zone groups.</p>","title":"Ceph Object Zone Group CRD"},{"location":"CRDs/ceph-object-multisite-crd/#sample_1","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectZoneGroup\nmetadata:\n  name: zonegroup-a\n  namespace: rook-ceph\nspec:\n  realm: realm-a\n</code></pre>","title":"Sample"},{"location":"CRDs/ceph-object-multisite-crd/#object-zone-group-settings","text":"","title":"Object Zone Group Settings"},{"location":"CRDs/ceph-object-multisite-crd/#metadata_1","text":"<ul> <li><code>name</code>: The name of the object zone group to create</li> <li><code>namespace</code>: The namespace of the Rook cluster where the object zone group is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-object-multisite-crd/#spec_1","text":"<ul> <li><code>realm</code>: The object realm in which the zone group will be created. This matches the name of the object realm CRD.</li> </ul>","title":"Spec"},{"location":"CRDs/ceph-object-multisite-crd/#ceph-object-zone-crd","text":"<p>Rook allows creation of zones in a ceph cluster for object stores through the custom resource definitions (CRDs). The following settings are available for Ceph object store zone.</p>","title":"Ceph Object Zone CRD"},{"location":"CRDs/ceph-object-multisite-crd/#sample_2","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectZone\nmetadata:\n  name: zone-a\n  namespace: rook-ceph\nspec:\n  zoneGroup: zonegroup-a\n  metadataPool:\n    failureDomain: host\n    replicated:\n      size: 3\n  dataPool:\n    failureDomain: osd\n    erasureCoded:\n      dataChunks: 2\n      codingChunks: 1\n</code></pre>","title":"Sample"},{"location":"CRDs/ceph-object-multisite-crd/#object-zone-settings","text":"","title":"Object Zone Settings"},{"location":"CRDs/ceph-object-multisite-crd/#metadata_2","text":"<ul> <li><code>name</code>: The name of the object zone to create</li> <li><code>namespace</code>: The namespace of the Rook cluster where the object zone is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-object-multisite-crd/#pools","text":"<p>The pools allow all of the settings defined in the Pool CRD spec. For more details, see the Pool CRD settings. In the example above, there must be at least three hosts (size 3) and at least three devices (2 data + 1 coding chunks) in the cluster.</p>","title":"Pools"},{"location":"CRDs/ceph-object-multisite-crd/#spec_2","text":"<ul> <li><code>zonegroup</code>: The object zonegroup in which the zone will be created. This matches the name of the object zone group CRD.</li> <li><code>metadataPool</code>: The settings used to create all of the object store metadata pools. Must use replication.</li> <li><code>dataPool</code>: The settings to create the object store data pool. Can use replication or erasure coding.</li> </ul>","title":"Spec"},{"location":"CRDs/ceph-object-store-crd/","text":"<p>Rook allows creation and customization of object stores through the custom resource definitions (CRDs). The following settings are available for Ceph object stores.</p>","title":"Ceph Object Store CRD"},{"location":"CRDs/ceph-object-store-crd/#sample","text":"","title":"Sample"},{"location":"CRDs/ceph-object-store-crd/#erasure-coded","text":"<p>Erasure coded pools can only be used with <code>dataPools</code>. The <code>metadataPool</code> must use a replicated pool.</p>  <p>NOTE: This sample requires at least 3 bluestore OSDs, with each OSD located on a different node.</p>  <p>The OSDs must be located on different nodes, because the <code>failureDomain</code> is set to <code>host</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStore\nmetadata:\n  name: my-store\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    failureDomain: host\n    replicated:\n      size: 3\n  dataPool:\n    failureDomain: host\n    erasureCoded:\n      dataChunks: 2\n      codingChunks: 1\n  preservePoolsOnDelete: true\n  gateway:\n    # sslCertificateRef:\n    # caBundleRef:\n    port: 80\n    # securePort: 443\n    instances: 1\n    # A key/value list of annotations\n    annotations:\n    #  key: value\n    placement:\n    #  nodeAffinity:\n    #    requiredDuringSchedulingIgnoredDuringExecution:\n    #      nodeSelectorTerms:\n    #      - matchExpressions:\n    #        - key: role\n    #          operator: In\n    #          values:\n    #          - rgw-node\n    #  tolerations:\n    #  - key: rgw-node\n    #    operator: Exists\n    #  podAffinity:\n    #  podAntiAffinity:\n    #  topologySpreadConstraints:\n    resources:\n    #  limits:\n    #    cpu: \"500m\"\n    #    memory: \"1024Mi\"\n    #  requests:\n    #    cpu: \"500m\"\n    #    memory: \"1024Mi\"\n  #zone:\n    #name: zone-a\n</code></pre>","title":"Erasure Coded"},{"location":"CRDs/ceph-object-store-crd/#object-store-settings","text":"","title":"Object Store Settings"},{"location":"CRDs/ceph-object-store-crd/#metadata","text":"<ul> <li><code>name</code>: The name of the object store to create, which will be reflected in the pool and other resource names.</li> <li><code>namespace</code>: The namespace of the Rook cluster where the object store is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-object-store-crd/#pools","text":"<p>The pools allow all of the settings defined in the Pool CRD spec. For more details, see the Pool CRD settings. In the example above, there must be at least three hosts (size 3) and at least three devices (2 data + 1 coding chunks) in the cluster.</p> <p>When the <code>zone</code> section is set pools with the object stores name will not be created since the object-store will the using the pools created by the ceph-object-zone.</p> <ul> <li><code>metadataPool</code>: The settings used to create all of the object store metadata pools. Must use replication.</li> <li><code>dataPool</code>: The settings to create the object store data pool. Can use replication or erasure coding.</li> <li><code>preservePoolsOnDelete</code>: If it is set to 'true' the pools used to support the object store will remain when the object store will be deleted. This is a security measure to avoid accidental loss of data. It is set to 'false' by default. If not specified is also deemed as 'false'.</li> </ul>","title":"Pools"},{"location":"CRDs/ceph-object-store-crd/#gateway-settings","text":"<p>The gateway settings correspond to the RGW daemon settings.</p> <ul> <li><code>type</code>: <code>S3</code> is supported</li> <li><code>sslCertificateRef</code>: If specified, this is the name of the Kubernetes secret(<code>opaque</code> or <code>tls</code>   type) that contains the TLS certificate to be used for secure connections to the object store.   If it is an opaque Kubernetes Secret, Rook will look in the secret provided at the <code>cert</code> key name. The value of the <code>cert</code> key must be   in the format expected by the RGW   service:   \"The server key, server certificate, and any other CA or intermediate certificates be supplied in   one file. Each of these items must be in PEM form.\" They are scenarios where the certificate DNS is set for a particular domain   that does not include the local Kubernetes DNS, namely the object store DNS service endpoint. If   adding the service DNS name to the certificate is not empty another key can be specified in the   secret's data: <code>insecureSkipVerify: true</code> to skip the certificate verification. It is not   recommended to enable this option since TLS is susceptible to machine-in-the-middle attacks unless   custom verification is used.</li> <li><code>caBundleRef</code>: If specified, this is the name of the Kubernetes secret (type <code>opaque</code>) that   contains additional custom ca-bundle to use. The secret must be in the same namespace as the Rook   cluster. Rook will look in the secret provided at the <code>cabundle</code> key name.</li> <li><code>port</code>: The port on which the Object service will be reachable. If host networking is enabled, the RGW daemons will also listen on that port. If running on SDN, the RGW daemon listening port will be 8080 internally.</li> <li><code>securePort</code>: The secure port on which RGW pods will be listening. A TLS certificate must be specified either via <code>sslCerticateRef</code> or <code>service.annotations</code></li> <li><code>instances</code>: The number of pods that will be started to load balance this object store.</li> <li><code>externalRgwEndpoints</code>: A list of IP addresses to connect to external existing Rados Gateways (works with external mode). This setting will be ignored if the <code>CephCluster</code> does not have <code>external</code> spec enabled. Refer to the external cluster section for more details.</li> <li><code>annotations</code>: Key value pair list of annotations to add.</li> <li><code>labels</code>: Key value pair list of labels to add.</li> <li><code>placement</code>: The Kubernetes placement settings to determine where the RGW pods should be started in the cluster.</li> <li><code>resources</code>: Set resource requests/limits for the Gateway Pod(s), see Resource Requirements/Limits.</li> <li><code>priorityClassName</code>: Set priority class name for the Gateway Pod(s)</li> <li><code>service</code>: The annotations to set on to the Kubernetes Service of RGW. The service serving cert feature supported in Openshift is enabled by the following example: <pre><code>gateway:\n  service:\n    annotations:\n      service.beta.openshift.io/serving-cert-secret-name: &lt;name of TLS secret for automatic generation&gt;\n</code></pre> </li> </ul> <p>Example of external rgw endpoints to connect to:</p> <pre><code>gateway:\n  port: 80\n  externalRgwEndpoints:\n    - ip: 192.168.39.182\n</code></pre>  <p>This will create a service with the endpoint <code>192.168.39.182</code> on port <code>80</code>, pointing to the Ceph object external gateway. All the other settings from the gateway section will be ignored, except for <code>securePort</code>.</p>","title":"Gateway Settings"},{"location":"CRDs/ceph-object-store-crd/#zone-settings","text":"<p>The zone settings allow the object store to join custom created ceph-object-zone.</p> <ul> <li><code>name</code>: the name of the ceph-object-zone the object store will be in.</li> </ul>","title":"Zone Settings"},{"location":"CRDs/ceph-object-store-crd/#runtime-settings","text":"","title":"Runtime settings"},{"location":"CRDs/ceph-object-store-crd/#mime-types","text":"<p>Rook provides a default <code>mime.types</code> file for each Ceph object store. This file is stored in a Kubernetes ConfigMap with the name <code>rook-ceph-rgw-&lt;STORE-NAME&gt;-mime-types</code>. For most users, the default file should suffice, however, the option is available to users to edit the <code>mime.types</code> file in the ConfigMap as they desire. Users may have their own special file types, and particularly security conscious users may wish to pare down the file to reduce the possibility of a file type execution attack.</p> <p>Rook will not overwrite an existing <code>mime.types</code> ConfigMap so that user modifications will not be destroyed. If the object store is destroyed and recreated, the ConfigMap will also be destroyed and created anew.</p>","title":"MIME types"},{"location":"CRDs/ceph-object-store-crd/#health-settings","text":"<p>Rook-Ceph will be default monitor the state of the object store endpoints. The following CRD settings are available:</p> <ul> <li><code>healthCheck</code>: main object store health monitoring section</li> <li><code>bucket</code>: Rook checks that the object store is usable regularly. This is explained in more     detail below. Use this config to disable or change the interval at which Rook verifies the     object store connectivity.</li> <li><code>startupProbe</code>: Disable, or override timing and threshold values of the object gateway startup probe.</li> <li><code>livenessProbe</code>: Disable, or override timing and threshold values of the object gateway liveness probe.</li> <li><code>readinessProbe</code>: Disable, or override timing and threshold values of the object gateway readiness probe.</li> </ul> <p>Here is a complete example:</p> <pre><code>healthCheck:\n  bucket:\n    disabled: false\n    interval: 60s\n  startupProbe:\n    disabled: false\n  livenessProbe:\n    disabled: false\n    periodSeconds: 5\n    failureThreshold: 4\n  readinessProbe:\n    disabled: false\n    periodSeconds: 5\n    failureThreshold: 2\n</code></pre>  <p>The endpoint health check procedure is the following:</p> <ol> <li>Create an S3 user</li> <li>Create a bucket with that user</li> <li>PUT the file in the object store</li> <li>GET the file from the object store</li> <li>Verify object consistency</li> <li>Update CR health status check</li> </ol> <p>Rook-Ceph always keeps the bucket and the user for the health check, it just does a PUT and GET of an s3 object since creating a bucket is an expensive operation.</p>","title":"Health settings"},{"location":"CRDs/ceph-object-store-crd/#security-settings","text":"<p>Ceph RGW supports encryption via Key Management System (KMS) using HashiCorp Vault. Refer to the vault kms section for detailed explanation. If these settings are defined, then RGW establish a connection between Vault and whenever S3 client sends a request with Server Side Encryption, it encrypts that using the key specified by the client. For more details w.r.t RGW, please refer Ceph Vault documentation</p> <p>The <code>security</code> section contains settings related to KMS encryption of the RGW.</p> <pre><code>security:\n  kms:\n    connectionDetails:\n      KMS_PROVIDER: vault\n      VAULT_ADDR: http://vault.default.svc.cluster.local:8200\n      VAULT_BACKEND_PATH: rgw\n      VAULT_SECRET_ENGINE: kv\n      VAULT_BACKEND: v2\n    # name of the k8s secret containing the kms authentication token\n    tokenSecretName: rgw-vault-token\n</code></pre>  <p>For RGW, please note the following:</p> <ul> <li><code>VAULT_SECRET_ENGINE</code> option is specifically for RGW to mention about the secret engine which can be used, currently supports two: kv and transit. And for kv engine only version 2 is supported.</li> <li> <p>The Storage administrator needs to create a secret in the Vault server so that S3 clients use that key for encryption $ vault kv put rook/ key=$(openssl rand -base64 32) # kv engine $ vault write -f transit/keys/ exportable=true # transit engine  <li> <p>TLS authentication with custom certificates between Vault and CephObjectStore RGWs are supported from ceph v16.2.6 onwards</p> </li>","title":"Security settings"},{"location":"CRDs/ceph-object-store-crd/#deleting-a-cephobjectstore","text":"<p>During deletion of a CephObjectStore resource, Rook protects against accidental or premature destruction of user data by blocking deletion if there are any object buckets in the object store being deleted. Buckets may have been created by users or by ObjectBucketClaims.</p> <p>For deletion to be successful, all buckets in the object store must be removed. This may require manual deletion or removal of all ObjectBucketClaims. Alternately, the <code>cephobjectstore.ceph.rook.io</code> finalizer on the CephObjectStore can be removed to remove the Kubernetes Custom Resource, but the Ceph pools which store the data will not be removed in this case.</p> <p>Rook will warn about which buckets are blocking deletion in three ways: 1. An event will be registered on the CephObjectStore resource 1. A status condition will be added to the CephObjectStore resource 1. An error will be added to the Rook-Ceph Operator log</p>","title":"Deleting a CephObjectStore"},{"location":"CRDs/ceph-object-store-user-crd/","text":"<p>Rook allows creation and customization of object store users through the custom resource definitions (CRDs). The following settings are available for Ceph object store users.</p>","title":"Ceph Object Store User CRD"},{"location":"CRDs/ceph-object-store-user-crd/#sample","text":"<pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStoreUser\nmetadata:\n  name: my-user\n  namespace: rook-ceph\nspec:\n  store: my-store\n  displayName: my-display-name\n  quotas:\n    maxBuckets: 100\n    maxSize: 10G\n    maxObjects: 10000\n  capabilities:\n    user: \"*\"\n    bucket: \"*\"\n</code></pre>","title":"Sample"},{"location":"CRDs/ceph-object-store-user-crd/#object-store-user-settings","text":"","title":"Object Store User Settings"},{"location":"CRDs/ceph-object-store-user-crd/#metadata","text":"<ul> <li><code>name</code>: The name of the object store user to create, which will be reflected in the secret and other resource names.</li> <li><code>namespace</code>: The namespace of the Rook cluster where the object store user is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-object-store-user-crd/#spec","text":"<ul> <li><code>store</code>: The object store in which the user will be created. This matches the name of the objectstore CRD.</li> <li><code>displayName</code>: The display name which will be passed to the <code>radosgw-admin user create</code> command.</li> <li><code>quotas</code>: This represents quota limitation can be set on the user (support added in Rook v1.7.3 and up).    Please refer here for details.<ul> <li><code>maxBuckets</code>: The maximum bucket limit for the user.</li> <li><code>maxSize</code>: Maximum size limit of all objects across all the user's buckets.</li> <li><code>maxObjects</code>: Maximum number of objects across all the user's buckets.</li> </ul> </li> <li><code>capabilities</code>: Ceph allows users to be given additional permissions (support added in Rook v1.7.3 and up). Due to missing APIs in go-ceph for updating the user capabilities, this setting can currently only be used during the creation of the object store user. If a user's capabilities need modified, the user must be deleted and re-created.   See the Ceph docs for more info.   Rook supports adding <code>read</code>, <code>write</code>, <code>read, write</code>, or <code>*</code> permissions for the following resources:<ul> <li><code>users</code></li> <li><code>buckets</code></li> <li><code>usage</code></li> <li><code>metadata</code></li> <li><code>zone</code></li> </ul> </li> </ul>","title":"Spec"},{"location":"CRDs/ceph-pool-crd/","text":"<p>Rook allows creation and customization of storage pools through the custom resource definitions (CRDs). The following settings are available for pools.</p>","title":"Ceph Block Pool CRD"},{"location":"CRDs/ceph-pool-crd/#samples","text":"","title":"Samples"},{"location":"CRDs/ceph-pool-crd/#replicated","text":"<p>For optimal performance, while also adding redundancy, this sample will configure Ceph to make three full copies of the data on multiple nodes.</p>  <p>NOTE: This sample requires at least 1 OSD per node, with each OSD located on 3 different nodes.</p>  <p>Each OSD must be located on a different node, because the <code>failureDomain</code> is set to <code>host</code> and the <code>replicated.size</code> is set to <code>3</code>.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  failureDomain: host\n  replicated:\n    size: 3\n  deviceClass: hdd\n</code></pre>","title":"Replicated"},{"location":"CRDs/ceph-pool-crd/#hybrid-storage-pools","text":"<p>Hybrid storage is a combination of two different storage tiers. For example, SSD and HDD. This helps to improve the read performance of cluster by placing, say, 1st copy of data on the higher performance tier (SSD or NVME) and remaining replicated copies on lower cost tier (HDDs).</p> <p>WARNING Hybrid storage pools are likely to suffer from lower availability if a node goes down. The data across the two tiers may actually end up on the same node, instead of being spread across unique nodes (or failure domains) as expected. Instead of using hybrid pools, consider configuring primary affinity from the toolbox.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  failureDomain: host\n  replicated:\n    size: 3\n    hybridStorage:\n      primaryDeviceClass: ssd\n      secondaryDeviceClass: hdd\n</code></pre>   <p>IMPORTANT: The device classes <code>primaryDeviceClass</code> and <code>secondaryDeviceClass</code> must have at least one OSD associated with them or else the pool creation will fail.</p>","title":"Hybrid Storage Pools"},{"location":"CRDs/ceph-pool-crd/#erasure-coded","text":"<p>This sample will lower the overall storage capacity requirement, while also adding redundancy by using erasure coding.</p>  <p>NOTE: This sample requires at least 3 bluestore OSDs.</p>  <p>The OSDs can be located on a single Ceph node or spread across multiple nodes, because the <code>failureDomain</code> is set to <code>osd</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: ecpool\n  namespace: rook-ceph\nspec:\n  failureDomain: osd\n  erasureCoded:\n    dataChunks: 2\n    codingChunks: 1\n  deviceClass: hdd\n</code></pre>  <p>High performance applications typically will not use erasure coding due to the performance overhead of creating and distributing the chunks in the cluster.</p> <p>When creating an erasure-coded pool, it is highly recommended to create the pool when you have bluestore OSDs in your cluster (see the OSD configuration settings. Filestore OSDs have limitations that are unsafe and lower performance.</p>","title":"Erasure Coded"},{"location":"CRDs/ceph-pool-crd/#mirroring","text":"<p>RADOS Block Device (RBD) mirroring is a process of asynchronous replication of Ceph block device images between two or more Ceph clusters. Mirroring ensures point-in-time consistent replicas of all changes to an image, including reads and writes, block device resizing, snapshots, clones and flattening. It is generally useful when planning for Disaster Recovery. Mirroring is for clusters that are geographically distributed and stretching a single cluster is not possible due to high latencies.</p> <p>The following will enable mirroring of the pool at the image level:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  replicated:\n    size: 3\n  mirroring:\n    enabled: true\n    mode: image\n    # schedule(s) of snapshot\n    snapshotSchedules:\n      - interval: 24h # daily snapshots\n        startTime: 14:00:00-05:00\n</code></pre>  <p>Once mirroring is enabled, Rook will by default create its own bootstrap peer token so that it can be used by another cluster. The bootstrap peer token can be found in a Kubernetes Secret. The name of the Secret is present in the Status field of the CephBlockPool CR:</p> <pre><code>status:\n  info:\n    rbdMirrorBootstrapPeerSecretName: pool-peer-token-replicapool\n</code></pre>  <p>This secret can then be fetched like so:</p> <pre><code>kubectl get secret -n rook-ceph pool-peer-token-replicapool -o jsonpath='{.data.token}'|base64 -d\n</code></pre>   <pre><code>eyJmc2lkIjoiOTFlYWUwZGQtMDZiMS00ZDJjLTkxZjMtMTMxMWM5ZGYzODJiIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFEN1psOWZ3V1VGRHhBQWdmY0gyZi8xeUhYeGZDUTU5L1N0NEE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjEwMS4xOC4yMjM6MzMwMCx2MToxMC4xMDEuMTguMjIzOjY3ODldIn0=\n</code></pre>   <p>The secret must be decoded. The result will be another base64 encoded blob that you will import in the destination cluster:</p> <pre><code>external-cluster-console # rbd mirror pool peer bootstrap import &lt;token file path&gt;\n</code></pre>  <p>See the official rbd mirror documentation on how to add a bootstrap peer.</p>","title":"Mirroring"},{"location":"CRDs/ceph-pool-crd/#data-spread-across-subdomains","text":"<p>Imagine the following topology with datacenters containing racks and then hosts:</p> <pre><code>.\n\u251c\u2500\u2500 datacenter-1\n\u2502   \u251c\u2500\u2500 rack-1\n\u2502   \u2502   \u251c\u2500\u2500 host-1\n\u2502   \u2502   \u251c\u2500\u2500 host-2\n\u2502   \u2514\u2500\u2500 rack-2\n\u2502       \u251c\u2500\u2500 host-3\n\u2502       \u251c\u2500\u2500 host-4\n\u2514\u2500\u2500 datacenter-2\n    \u251c\u2500\u2500 rack-3\n    \u2502   \u251c\u2500\u2500 host-5\n    \u2502   \u251c\u2500\u2500 host-6\n    \u2514\u2500\u2500 rack-4\n        \u251c\u2500\u2500 host-7\n        \u2514\u2500\u2500 host-8\n</code></pre>  <p>As an administrator I would like to place 4 copies across both datacenter where each copy inside a datacenter is across a rack. This can be achieved by the following:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  replicated:\n    size: 4\n    replicasPerFailureDomain: 2\n    subFailureDomain: rack\n</code></pre>","title":"Data spread across subdomains"},{"location":"CRDs/ceph-pool-crd/#pool-settings","text":"","title":"Pool Settings"},{"location":"CRDs/ceph-pool-crd/#metadata","text":"<ul> <li><code>name</code>: The name of the pool to create.</li> <li><code>namespace</code>: The namespace of the Rook cluster where the pool is created.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-pool-crd/#spec","text":"<ul> <li><code>replicated</code>: Settings for a replicated pool. If specified, <code>erasureCoded</code> settings must not be specified.</li> <li><code>size</code>: The desired number of copies to make of the data in the pool.</li> <li><code>requireSafeReplicaSize</code>: set to false if you want to create a pool with size 1, setting pool size 1 could lead to data loss without recovery. Make sure you are ABSOLUTELY CERTAIN that is what you want.</li> <li><code>replicasPerFailureDomain</code>: Sets up the number of replicas to place in a given failure domain. For instance, if the failure domain is a datacenter (cluster is stretched) then you will have 2 replicas per datacenter where each replica ends up on a different host. This gives you a total of 4 replicas and for this, the <code>size</code> must be set to 4. The default is 1.</li> <li><code>subFailureDomain</code>: Name of the CRUSH bucket representing a sub-failure domain. In a stretched configuration this option represent the \"last\" bucket where replicas will end up being written. Imagine the cluster is stretched across two datacenters, you can then have 2 copies per datacenter and each copy on a different CRUSH bucket. The default is \"host\".</li> <li><code>erasureCoded</code>: Settings for an erasure-coded pool. If specified, <code>replicated</code> settings must not be specified. See below for more details on erasure coding.</li> <li><code>dataChunks</code>: Number of chunks to divide the original object into</li> <li><code>codingChunks</code>: Number of coding chunks to generate</li> <li> <p><code>failureDomain</code>: The failure domain across which the data will be spread. This can be set to a value of either <code>osd</code> or <code>host</code>, with <code>host</code> being the default setting. A failure domain can also be set to a different type (e.g. <code>rack</code>), if the OSDs are created on nodes with the supported topology labels. If the <code>failureDomain</code> is changed on the pool, the operator will create a new CRUSH rule and update the pool.     If a <code>replicated</code> pool of size <code>3</code> is configured and the <code>failureDomain</code> is set to <code>host</code>, all three copies of the replicated data will be placed on OSDs located on <code>3</code> different Ceph hosts. This case is guaranteed to tolerate a failure of two hosts without a loss of data. Similarly, a failure domain set to <code>osd</code>, can tolerate a loss of two OSD devices.</p> <p>If erasure coding is used, the data and coding chunks are spread across the configured failure domain.</p>  <p>NOTE: Neither Rook, nor Ceph, prevent the creation of a cluster where the replicated data (or Erasure Coded chunks) can be written safely. By design, Ceph will delay checking for suitable OSDs until a write request is made and this write can hang if there are not sufficient OSDs to satisfy the request. * <code>deviceClass</code>: Sets up the CRUSH rule for the pool to distribute data only on the specified device class. If left empty or unspecified, the pool will use the cluster's default CRUSH root, which usually distributes data over all OSDs, regardless of their class. * <code>crushRoot</code>: The root in the crush map to be used by the pool. If left empty or unspecified, the default root will be used. Creating a crush hierarchy for the OSDs currently requires the Rook toolbox to run the Ceph tools described here. * <code>enableRBDStats</code>: Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false. For more info see the ceph documentation. * <code>name</code>: The name of Ceph pools is based on the <code>metadata.name</code> of the CephBlockPool CR. Some built-in Ceph pools   require names that are incompatible with K8s resource names. These special pools can be configured   by setting this <code>name</code> to override the name of the Ceph pool that is created instead of using the <code>metadata.name</code> for the pool.   Only the following pool names are supported: <code>device_health_metrics</code>, <code>.nfs</code>, and <code>.mgr</code>. See the example builtin mgr pool.</p>  </li> <li> <p><code>parameters</code>: Sets any parameters listed to the given pool</p> </li> <li><code>target_size_ratio:</code> gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool, for more info see the ceph documentation</li> <li> <p><code>compression_mode</code>: Sets up the pool for inline compression when using a Bluestore OSD. If left unspecified does not setup any compression mode for the pool. Values supported are the same as Bluestore inline compression modes, such as <code>none</code>, <code>passive</code>, <code>aggressive</code>, and <code>force</code>.</p> </li> <li> <p><code>mirroring</code>: Sets up mirroring of the pool</p> </li> <li><code>enabled</code>: whether mirroring is enabled on that pool (default: false)</li> <li><code>mode</code>: mirroring mode to run, possible values are \"pool\" or \"image\" (required). Refer to the mirroring modes Ceph documentation for more details.</li> <li><code>snapshotSchedules</code>: schedule(s) snapshot at the pool level. Only supported as of Ceph Octopus (v15) release. One or more schedules are supported.<ul> <li><code>interval</code>: frequency of the snapshots. The interval can be specified in days, hours, or minutes using d, h, m suffix respectively.</li> <li><code>startTime</code>: optional, determines at what time the snapshot process starts, specified using the ISO 8601 time format.</li> </ul> </li> <li> <p><code>peers</code>: to configure mirroring peers. See the prerequisite RBD Mirror documentation first.</p> <ul> <li><code>secretNames</code>:  a list of peers to connect to. Currently only a single peer is supported where a peer represents a Ceph cluster.</li> </ul> </li> <li> <p><code>statusCheck</code>: Sets up pool mirroring status</p> </li> <li> <p><code>mirror</code>: displays the mirroring status</p> <ul> <li><code>disabled</code>: whether to enable or disable pool mirroring status</li> <li><code>interval</code>: time interval to refresh the mirroring status (default 60s)</li> </ul> </li> <li> <p><code>quotas</code>: Set byte and object quotas. See the ceph documentation for more info.</p> </li> <li><code>maxSize</code>: quota in bytes as a string with quantity suffixes (e.g. \"10Gi\")</li> <li><code>maxObjects</code>: quota in objects as an integer     &gt; NOTE: A value of 0 disables the quota.</li> </ul>","title":"Spec"},{"location":"CRDs/ceph-pool-crd/#add-specific-pool-properties","text":"<p>With <code>poolProperties</code> you can set any pool property:</p> <pre><code>spec:\n  parameters:\n    &lt;name of the parameter&gt;: &lt;parameter value&gt;\n</code></pre>  <p>For instance:</p> <pre><code>spec:\n  parameters:\n    min_size: 1\n</code></pre>","title":"Add specific pool properties"},{"location":"CRDs/ceph-pool-crd/#erasure-coding","text":"<p>Erasure coding allows you to keep your data safe while reducing the storage overhead. Instead of creating multiple replicas of the data, erasure coding divides the original data into chunks of equal size, then generates extra chunks of that same size for redundancy.</p> <p>For example, if you have an object of size 2MB, the simplest erasure coding with two data chunks would divide the object into two chunks of size 1MB each (data chunks). One more chunk (coding chunk) of size 1MB will be generated. In total, 3MB will be stored in the cluster. The object will be able to suffer the loss of any one of the chunks and still be able to reconstruct the original object.</p> <p>The number of data and coding chunks you choose will depend on your resiliency to loss and how much storage overhead is acceptable in your storage cluster. Here are some examples to illustrate how the number of chunks affects the storage and loss toleration.</p>    Data chunks (k) Coding chunks (m) Total storage Losses Tolerated OSDs required     2 1 1.5x 1 3   2 2 2x 2 4   4 2 1.5x 2 6   16 4 1.25x 4 20    <p>The <code>failureDomain</code> must be also be taken into account when determining the number of chunks. The failure domain determines the level in the Ceph CRUSH hierarchy where the chunks must be uniquely distributed. This decision will impact whether node losses or disk losses are tolerated. There could also be performance differences of placing the data across nodes or osds.</p> <ul> <li><code>host</code>: All chunks will be placed on unique hosts</li> <li><code>osd</code>: All chunks will be placed on unique OSDs</li> </ul> <p>If you do not have a sufficient number of hosts or OSDs for unique placement the pool can be created, writing to the pool will hang.</p> <p>Rook currently only configures two levels in the CRUSH map. It is also possible to configure other levels such as <code>rack</code> with by adding topology labels to the nodes.</p>","title":"Erasure Coding"},{"location":"CRDs/ceph-pool-radosnamespace-crd/","text":"<p>This guide assumes you have created a Rook cluster as explained in the main Quickstart guide</p>","title":"RADOS Namespace CRD"},{"location":"CRDs/ceph-pool-radosnamespace-crd/#cephblockpoolradosnamespace-crd","text":"<p>RADOS currently uses pools both for data distribution (pools are shared into PGs, which map to OSDs) and as the granularity for security (capabilities can restrict access by pool).  Overloading pools for both purposes makes it hard to do multi-tenancy because it not a good idea to have a very large number of pools.</p> <p>A namespace would be a division of a pool into separate logical namespaces. For more information about BlockPool and namespace refer to the Ceph docs</p> <p>Having multiple namespaces in a pool would allow multiple Kubernetes clusters to share one unique ceph cluster without creating a pool per kubernetes cluster and it will also allow to have tenant isolation between multiple tenenats in a single Kubernetes cluster without creating multiple pools for tenants.</p> <p>Rook allows creation of Ceph BlockPool RadosNamespaces through the custom resource definitions (CRDs).</p>","title":"CephBlockPoolRadosNamespace CRD"},{"location":"CRDs/ceph-pool-radosnamespace-crd/#example","text":"<p>To get you started, here is a simple example of a CR to create a CephBlockPoolRadosNamespace on the CephBlockPool \"replicapool\".</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPoolRadosNamespace\nmetadata:\n  name: namespace-a\n  namespace: rook-ceph # namespace:cluster\nspec:\n  # The name of the CephBlockPool CR where the namespace is created.\n  blockPoolName: replicapool\n</code></pre>","title":"Example"},{"location":"CRDs/ceph-pool-radosnamespace-crd/#settings","text":"<p>If any setting is unspecified, a suitable default will be used automatically.</p>","title":"Settings"},{"location":"CRDs/ceph-pool-radosnamespace-crd/#metadata","text":"<ul> <li><code>name</code>: The name that will be used for the Ceph BlockPool rados namespace.</li> </ul>","title":"Metadata"},{"location":"CRDs/ceph-pool-radosnamespace-crd/#spec","text":"<ul> <li><code>blockPoolName</code>: The metadata name of the CephBlockPool CR where the rados namespace will be created.</li> </ul>","title":"Spec"},{"location":"CRDs/ceph-rbd-mirror-crd/","text":"<p>Rook allows creation and updating rbd-mirror daemon(s) through the custom resource definitions (CRDs). RBD images can be asynchronously mirrored between two Ceph clusters. For more information about user management and capabilities see the Ceph docs.</p>","title":"Ceph RBDMirror CRD"},{"location":"CRDs/ceph-rbd-mirror-crd/#creating-daemons","text":"<p>To get you started, here is a simple example of a CRD to deploy an rbd-mirror daemon.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephRBDMirror\nmetadata:\n  name: my-rbd-mirror\n  namespace: rook-ceph\nspec:\n  count: 1\n</code></pre>","title":"Creating daemons"},{"location":"CRDs/ceph-rbd-mirror-crd/#prerequisites","text":"<p>This guide assumes you have created a Rook cluster as explained in the main Quickstart guide</p>","title":"Prerequisites"},{"location":"CRDs/ceph-rbd-mirror-crd/#settings","text":"<p>If any setting is unspecified, a suitable default will be used automatically.</p>","title":"Settings"},{"location":"CRDs/ceph-rbd-mirror-crd/#rbdmirror-metadata","text":"<ul> <li><code>name</code>: The name that will be used for the Ceph RBD Mirror daemon.</li> <li><code>namespace</code>: The Kubernetes namespace that will be created for the Rook cluster. The services, pods, and other resources created by the operator will be added to this namespace.</li> </ul>","title":"RBDMirror metadata"},{"location":"CRDs/ceph-rbd-mirror-crd/#rbdmirror-settings","text":"<ul> <li><code>count</code>: The number of rbd mirror instance to run.</li> <li><code>placement</code>: The rbd mirror pods can be given standard Kubernetes placement restrictions with <code>nodeAffinity</code>, <code>tolerations</code>, <code>podAffinity</code>, and <code>podAntiAffinity</code> similar to placement defined for daemons configured by the cluster CRD..</li> <li><code>annotations</code>: Key value pair list of annotations to add.</li> <li><code>labels</code>: Key value pair list of labels to add.</li> <li><code>resources</code>: The resource requirements for the rbd mirror pods.</li> <li><code>priorityClassName</code>: The priority class to set on the rbd mirror pods.</li> </ul>","title":"RBDMirror Settings"},{"location":"CRDs/ceph-rbd-mirror-crd/#configuring-mirroring-peers","text":"<p>Configure mirroring peers individually for each CephBlockPool. Refer to the CephBlockPool documentation for more detail.</p>","title":"Configuring mirroring peers"},{"location":"Common%20Issues/ceph-common-issues/","text":"<p>Many of these problem cases are hard to summarize down to a short phrase that adequately describes the problem. Each problem will start with a bulleted list of symptoms. Keep in mind that all symptoms may not apply depending on the configuration of Rook. If the majority of the symptoms are seen there is a fair chance you are experiencing that problem.</p> <p>If after trying the suggestions found on this page and the problem is not resolved, the Rook team is very happy to help you troubleshoot the issues in their Slack channel. Once you have registered for the Rook Slack, proceed to the <code>#ceph</code> channel to ask for assistance.</p>","title":"Ceph Common Issues"},{"location":"Common%20Issues/ceph-common-issues/#table-of-contents","text":"<ul> <li>Troubleshooting Techniques</li> <li>Cluster failing to service requests</li> <li>Monitors are the only pods running</li> <li>PVCs stay in pending state</li> <li>OSD pods are failing to start</li> <li>OSD pods are not created on my devices</li> <li>Node hangs after reboot</li> <li>Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7</li> <li>Set debug log level for all Ceph daemons</li> <li>Activate log to file for a particular Ceph daemon</li> <li>A worker node using RBD devices hangs up</li> <li>Too few PGs per OSD warning is shown</li> <li>LVM metadata can be corrupted with OSD on LV-backed PVC</li> <li>OSD prepare job fails due to low aio-max-nr setting</li> <li>Unexpected partitions created</li> <li>Operator environment variables are ignored</li> </ul> <p>See also the CSI Troubleshooting Guide.</p>","title":"Table of Contents"},{"location":"Common%20Issues/ceph-common-issues/#troubleshooting-techniques","text":"<p>There are two main categories of information you will need to investigate issues in the cluster:</p> <ol> <li>Kubernetes status and logs documented here</li> <li>Ceph cluster status (see upcoming Ceph tools section)</li> </ol>","title":"Troubleshooting Techniques"},{"location":"Common%20Issues/ceph-common-issues/#ceph-tools","text":"<p>After you verify the basic health of the running pods, next you will want to run Ceph tools for status of the storage components. There are two ways to run the Ceph tools, either in the Rook toolbox or inside other Rook pods that are already running.</p> <ul> <li>Logs on a specific node to find why a PVC is failing to mount</li> <li>See the log collection topic for a script that will help you gather the logs</li> <li>Other artifacts:</li> <li>The monitors that are expected to be in quorum: <code>kubectl -n &lt;cluster-namespace&gt; get configmap rook-ceph-mon-endpoints -o yaml | grep data</code></li> </ul>","title":"Ceph Tools"},{"location":"Common%20Issues/ceph-common-issues/#tools-in-the-rook-toolbox","text":"<p>The rook-ceph-tools pod provides a simple environment to run Ceph tools. Once the pod is up and running, connect to the pod to execute Ceph commands to evaluate that current state of the cluster.</p> <pre><code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[*].metadata.name}') bash\n</code></pre>","title":"Tools in the Rook Toolbox"},{"location":"Common%20Issues/ceph-common-issues/#ceph-commands","text":"<p>Here are some common commands to troubleshoot a Ceph cluster:</p> <ul> <li><code>ceph status</code></li> <li><code>ceph osd status</code></li> <li><code>ceph osd df</code></li> <li><code>ceph osd utilization</code></li> <li><code>ceph osd pool stats</code></li> <li><code>ceph osd tree</code></li> <li><code>ceph pg stat</code></li> </ul> <p>The first two status commands provide the overall cluster health. The normal state for cluster operations is HEALTH_OK, but will still function when the state is in a HEALTH_WARN state. If you are in a WARN state, then the cluster is in a condition that it may enter the HEALTH_ERROR state at which point all disk I/O operations are halted. If a HEALTH_WARN state is observed, then one should take action to prevent the cluster from halting when it enters the HEALTH_ERROR state.</p> <p>There are many Ceph sub-commands to look at and manipulate Ceph objects, well beyond the scope this document. See the Ceph documentation for more details of gathering information about the health of the cluster. In addition, there are other helpful hints and some best practices located in the Advanced Configuration section. Of particular note, there are scripts for collecting logs and gathering OSD information there.</p>","title":"Ceph Commands"},{"location":"Common%20Issues/ceph-common-issues/#cluster-failing-to-service-requests","text":"","title":"Cluster failing to service requests"},{"location":"Common%20Issues/ceph-common-issues/#symptoms","text":"<ul> <li>Execution of the <code>ceph</code> command hangs</li> <li>PersistentVolumes are not being created</li> <li>Large amount of slow requests are blocking</li> <li>Large amount of stuck requests are blocking</li> <li>One or more MONs are restarting periodically</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation","text":"<p>Create a rook-ceph-tools pod to investigate the current state of Ceph. Here is an example of what one might see. In this case the <code>ceph status</code> command would just hang so a CTRL-C needed to be sent.</p> <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\n\nceph status\n^CCluster connection interrupted or timed out\n</code></pre>  <p>Another indication is when one or more of the MON pods restart frequently. Note the 'mon107' that has only been up for 16 minutes in the following output.</p> <pre><code>kubectl -n rook-ceph get all -o wide --show-all\n</code></pre>   <pre><code>NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE\npo/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402\npo/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402\nrook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404\nrook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403\nrook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404\nrook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403\n</code></pre>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#solution","text":"<p>What is happening here is that the MON pods are restarting and one or more of the Ceph daemons are not getting configured with the proper cluster information. This is commonly the result of not specifying a value for <code>dataDirHostPath</code> in your Cluster CRD.</p> <p>The <code>dataDirHostPath</code> setting specifies a path on the local host for the Ceph daemons to store configuration and data. Setting this to a path like <code>/var/lib/rook</code>, reapplying your Cluster CRD and restarting all the Ceph daemons (MON, MGR, OSD, RGW) should solve this problem. After the Ceph daemons have been restarted, it is advisable to restart the rook-tool pod.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#monitors-are-the-only-pods-running","text":"","title":"Monitors are the only pods running"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_1","text":"<ul> <li>Rook operator is running</li> <li>Either a single mon starts or the mons start very slowly (at least several minutes apart)</li> <li>The crash-collector pods are crashing</li> <li>No mgr, osd, or other daemons are created except the CSI driver</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_1","text":"<p>When the operator is starting a cluster, the operator will start one mon at a time and check that they are healthy before continuing to bring up all three mons. If the first mon is not detected healthy, the operator will continue to check until it is healthy. If the first mon fails to start, a second and then a third mon may attempt to start. However, they will never form quorum and the orchestration will be blocked from proceeding.</p> <p>The crash-collector pods will be blocked from starting until the mons have formed quorum the first time.</p> <p>There are several common causes for the mons failing to form quorum:</p> <ul> <li>The operator pod does not have network connectivity to the mon pod(s). The network may be configured incorrectly.</li> <li>One or more mon pods are in running state, but the operator log shows they are not able to form quorum</li> <li>A mon is using configuration from a previous installation. See the cleanup guide   for cleaning the previous cluster.</li> <li>A firewall may be blocking the ports required for the Ceph mons to form quorum. Ensure ports 6789 and 3300 are enabled.   See the Ceph networking guide for more details.</li> <li>There may be MTU mismatch between different networking components. Some networks may be more   susceptible to mismatch than others. If Kubernetes CNI or hosts enable jumbo frames (MTU 9000),   Ceph will use large packets to maximize network bandwidth. If other parts of the networking chain   don't support jumbo frames, this could result in lost or rejected packets unexpectedly.</li> </ul>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#operator-fails-to-connect-to-the-mon","text":"<p>First look at the logs of the operator to confirm if it is able to connect to the mons.</p> <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-operator\n</code></pre>  <p>Likely you will see an error similar to the following that the operator is timing out when connecting to the mon. The last command is <code>ceph mon_status</code>, followed by a timeout message five minutes later.</p>  <pre><code>2018-01-21 21:47:32.375833 I | exec: Running command: ceph mon_status --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442263890\n2018-01-21 21:52:35.370533 I | exec: 2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300\n2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300\n2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out\n2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n</code></pre>   <p>The error would appear to be an authentication error, but it is misleading. The real issue is a timeout.</p>","title":"Operator fails to connect to the mon"},{"location":"Common%20Issues/ceph-common-issues/#solution_1","text":"<p>If you see the timeout in the operator log, verify if the mon pod is running (see the next section). If the mon pod is running, check the network connectivity between the operator pod and the mon pod. A common issue is that the CNI is not configured correctly.</p> <p>To verify the network connectivity: - Get the endpoint for a mon - Curl the mon from the operator pod</p> <p>For example, this command will curl the first mon from the operator:</p> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-operator -- curl $(kubectl -n rook-ceph get svc -l app=rook-ceph-mon -o jsonpath='{.items[0].spec.clusterIP}'):3300 2&gt;/dev/null\n</code></pre>   <pre><code>ceph v2\n</code></pre>   <p>If \"ceph v2\" is printed to the console, the connection was successful. If the command does not respond or otherwise fails, the network connection cannot be established.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#failing-mon-pod","text":"<p>Second we need to verify if the mon pod started successfully.</p> <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-mon\n</code></pre>   <pre><code>NAME                                READY     STATUS               RESTARTS   AGE\nrook-ceph-mon-a-69fb9c78cd-58szd    1/1       CrashLoopBackOff     2          47s\n</code></pre>   <p>If the mon pod is failing as in this example, you will need to look at the mon pod status or logs to determine the cause. If the pod is in a crash loop backoff state, you should see the reason by describing the pod.</p> <pre><code># The pod shows a termination status that the keyring does not match the existing keyring\nkubectl -n rook-ceph describe pod -l mon=rook-ceph-mon0\n</code></pre>   <pre><code>...\n    Last State:    Terminated\n      Reason:    Error\n      Message:    The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring.\n                    You may need to delete the contents of dataDirHostPath on the host from a previous deployment.\n...\n</code></pre>   <p>See the solution in the next section regarding cleaning up the <code>dataDirHostPath</code> on the nodes.</p>","title":"Failing mon pod"},{"location":"Common%20Issues/ceph-common-issues/#solution_2","text":"<p>This is a common problem reinitializing the Rook cluster when the local directory used for persistence has not been purged. This directory is the <code>dataDirHostPath</code> setting in the cluster CRD and is typically set to <code>/var/lib/rook</code>. To fix the issue you will need to delete all components of Rook and then delete the contents of <code>/var/lib/rook</code> (or the directory specified by <code>dataDirHostPath</code>) on each of the hosts in the cluster. Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p>  <p>IMPORTANT: Deleting the <code>dataDirHostPath</code> folder is destructive to the storage. Only delete the folder if you are trying to permanently purge the Rook cluster.</p>  <p>See the Cleanup Guide for more details.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#pvcs-stay-in-pending-state","text":"","title":"PVCs stay in pending state"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_2","text":"<ul> <li>When you create a PVC based on a rook storage class, it stays pending indefinitely</li> </ul> <p>For the Wordpress example, you might see two PVCs in pending state.</p> <pre><code>kubectl get pvc\n</code></pre>   <pre><code>NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nmysql-pv-claim   Pending                                      rook-ceph-block   8s\nwp-pv-claim      Pending                                      rook-ceph-block   16s\n</code></pre>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_2","text":"<p>There are two common causes for the PVCs staying in pending state:</p> <ol> <li>There are no OSDs in the cluster</li> <li>The CSI provisioner pod is not running or is not responding to the request to provision the storage</li> </ol>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#confirm-if-there-are-osds","text":"<p>To confirm if you have OSDs in your cluster, connect to the Rook Toolbox and run the <code>ceph status</code> command. You should see that you have at least one OSD <code>up</code> and <code>in</code>. The minimum number of OSDs required depends on the <code>replicated.size</code> setting in the pool created for the storage class. In a \"test\" cluster, only one OSD is required (see <code>storageclass-test.yaml</code>). In the production storage class example (<code>storageclass.yaml</code>), three OSDs would be required.</p> <pre><code>ceph status\n</code></pre>   <pre><code>  cluster:\n    id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 11m)\n    mgr: a(active, since 10m)\n    osd: 1 osds: 1 up (since 46s), 1 in (since 109m)\n</code></pre>","title":"Confirm if there are OSDs"},{"location":"Common%20Issues/ceph-common-issues/#osd-prepare-logs","text":"<p>If you don't see the expected number of OSDs, let's investigate why they weren't created. On each node where Rook looks for OSDs to configure, you will see an \"osd prepare\" pod.</p> <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare\n</code></pre>   <pre><code>NAME                                 ...  READY   STATUS      RESTARTS   AGE\nrook-ceph-osd-prepare-minikube-9twvk   0/2     Completed   0          30m\n</code></pre>   <p>See the section on why OSDs are not getting created to investigate the logs.</p>","title":"OSD Prepare Logs"},{"location":"Common%20Issues/ceph-common-issues/#csi-driver","text":"<p>The CSI driver may not be responding to the requests. Look in the logs of the CSI provisioner pod to see if there are any errors during the provisioning.</p> <p>There are two provisioner pods:</p> <pre><code>kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner\n</code></pre>  <p>Get the logs of each of the pods. One of them should be the \"leader\" and be responding to requests.</p> <pre><code>kubectl -n rook-ceph logs csi-cephfsplugin-provisioner-d77bb49c6-q9hwq csi-provisioner\n</code></pre>  <p>See also the CSI Troubleshooting Guide.</p>","title":"CSI Driver"},{"location":"Common%20Issues/ceph-common-issues/#operator-unresponsiveness","text":"<p>Lastly, if you have OSDs <code>up</code> and <code>in</code>, the next step is to confirm the operator is responding to the requests. Look in the Operator pod logs around the time when the PVC was created to confirm if the request is being raised. If the operator does not show requests to provision the block image, the operator may be stuck on some other operation. In this case, restart the operator pod to get things going again.</p>","title":"Operator unresponsiveness"},{"location":"Common%20Issues/ceph-common-issues/#solution_3","text":"<p>If the \"osd prepare\" logs didn't give you enough clues about why the OSDs were not being created, please review your cluster.yaml configuration. The common misconfigurations include:</p> <ul> <li>If <code>useAllDevices: true</code>, Rook expects to find local devices attached to the nodes. If no devices are found, no OSDs will be created.</li> <li>If <code>useAllDevices: false</code>, OSDs will only be created if <code>deviceFilter</code> is specified.</li> <li>Only local devices attached to the nodes will be configurable by Rook. In other words, the devices must show up under <code>/dev</code>.</li> <li>The devices must not have any partitions or filesystems on them. Rook will only configure raw devices. Partitions are not yet supported.</li> </ul>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#osd-pods-are-failing-to-start","text":"","title":"OSD pods are failing to start"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_3","text":"<ul> <li>OSD pods are failing to start</li> <li>You have started a cluster after tearing down another cluster</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_3","text":"<p>When an OSD starts, the device or directory will be configured for consumption. If there is an error with the configuration, the pod will crash and you will see the CrashLoopBackoff status for the pod. Look in the osd pod logs for an indication of the failure.</p> <pre><code>$ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs\n...\n</code></pre>  <p>One common case for failure is that you have re-deployed a test cluster and some state may remain from a previous deployment. If your cluster is larger than a few nodes, you may get lucky enough that the monitors were able to start and form quorum. However, now the OSDs pods may fail to start due to the old state. Looking at the OSD pod logs you will see an error about the file already existing.</p> <pre><code>$ kubectl -n rook-ceph logs rook-ceph-osd-fl8fs\n</code></pre>   <pre><code>...\n2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid\n2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists\n2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists\n2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists\n2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists\n</code></pre>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#solution_4","text":"<p>If the error is from the file that already exists, this is a common problem reinitializing the Rook cluster when the local directory used for persistence has not been purged. This directory is the <code>dataDirHostPath</code> setting in the cluster CRD and is typically set to <code>/var/lib/rook</code>. To fix the issue you will need to delete all components of Rook and then delete the contents of <code>/var/lib/rook</code> (or the directory specified by <code>dataDirHostPath</code>) on each of the hosts in the cluster. Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#osd-pods-are-not-created-on-my-devices","text":"","title":"OSD pods are not created on my devices"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_4","text":"<ul> <li>No OSD pods are started in the cluster</li> <li>Devices are not configured with OSDs even though specified in the Cluster CRD</li> <li>One OSD pod is started on each node instead of multiple pods for each device</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_4","text":"<p>First, ensure that you have specified the devices correctly in the CRD. The Cluster CRD has several ways to specify the devices that are to be consumed by the Rook storage:</p> <ul> <li><code>useAllDevices: true</code>: Rook will consume all devices it determines to be available</li> <li><code>deviceFilter</code>: Consume all devices that match this regular expression</li> <li><code>devices</code>: Explicit list of device names on each node to consume</li> </ul> <p>Second, if Rook determines that a device is not available (has existing partitions or a formatted filesystem), Rook will skip consuming the devices. If Rook is not starting OSDs on the devices you expect, Rook may have skipped it for this reason. To see if a device was skipped, view the OSD preparation log on the node where the device was skipped. Note that it is completely normal and expected for OSD prepare pod to be in the <code>completed</code> state. After the job is complete, Rook leaves the pod around in case the logs need to be investigated.</p> <pre><code># Get the prepare pods in the cluster\nkubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare\n</code></pre>   <pre><code>NAME                                   READY     STATUS      RESTARTS   AGE\nrook-ceph-osd-prepare-node1-fvmrp      0/1       Completed   0          18m\nrook-ceph-osd-prepare-node2-w9xv9      0/1       Completed   0          22m\nrook-ceph-osd-prepare-node3-7rgnv      0/1       Completed   0          22m\n</code></pre>   <pre><code># view the logs for the node of interest in the \"provision\" container\nkubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision\n[...]\n</code></pre>  <p>Here are some key lines to look for in the log:</p>  <pre><code># A device will be skipped if Rook sees it has partitions or a filesystem\n2019-05-30 19:02:57.353171 W | cephosd: skipping device sda that is in use\n2019-05-30 19:02:57.452168 W | skipping device \"sdb5\": [\"Used by ceph-disk\"]\n\n# Other messages about a disk being unusable by ceph include:\nInsufficient space (&lt;5GB) on vgs\nInsufficient space (&lt;5GB)\nLVM detected\nHas BlueStore device label\nlocked\nread-only\n\n# A device is going to be configured\n2019-05-30 19:02:57.535598 I | cephosd: device sdc to be configured by ceph-volume\n\n# For each device configured you will see a report printed to the log\n2019-05-30 19:02:59.844642 I |   Type            Path                                                    LV Size         % of device\n2019-05-30 19:02:59.844651 I | ----------------------------------------------------------------------------------------------------\n2019-05-30 19:02:59.844677 I |   [data]          /dev/sdc                                                7.00 GB         100%\n</code></pre>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#solution_5","text":"<p>Either update the CR with the correct settings, or clean the partitions or filesystem from your devices. To clean devices from a previous install see the cleanup guide.</p> <p>After the settings are updated or the devices are cleaned, trigger the operator to analyze the devices again by restarting the operator. Each time the operator starts, it will ensure all the desired devices are configured. The operator does automatically deploy OSDs in most scenarios, but an operator restart will cover any scenarios that the operator doesn't detect automatically.</p> <pre><code># Restart the operator to ensure devices are configured. A new pod will automatically be started when the current operator pod is deleted.\nkubectl -n rook-ceph delete pod -l app=rook-ceph-operator\n[...]\n</code></pre>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#node-hangs-after-reboot","text":"<p>This issue is fixed in Rook v1.3 or later.</p>","title":"Node hangs after reboot"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_5","text":"<ul> <li>After issuing a <code>reboot</code> command, node never returned online</li> <li>Only a power cycle helps</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_5","text":"<p>On a node running a pod with a Ceph persistent volume</p> <pre><code>mount | grep rbd\n</code></pre>   <pre><code># _netdev mount option is absent, also occurs for cephfs\n# OS is not aware PV is mounted over network\n/dev/rbdx on ... (rw,relatime, ..., noquota)\n</code></pre>   <p>When the reboot command is issued, network interfaces are terminated before disks are unmounted. This results in the node hanging as repeated attempts to unmount Ceph persistent volumes fail with the following error:</p>  <pre><code>libceph: connect [monitor-ip]:6789 error -101\n</code></pre>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#solution_6","text":"<p>The node needs to be drained before reboot. After the successful drain, the node can be rebooted as usual.</p> <p>Because <code>kubectl drain</code> command automatically marks the node as unschedulable (<code>kubectl cordon</code> effect), the node needs to be uncordoned once it's back online.</p> <p>Drain the node:</p> <pre><code>$ kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data\n</code></pre>  <p>Uncordon the node:</p> <pre><code>$ kubectl uncordon &lt;node-name&gt;\n</code></pre>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47","text":"","title":"Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_6","text":"<ul> <li>More than one shared filesystem (CephFS) has been created in the cluster</li> <li>A pod attempts to mount any other shared filesystem besides the first one that was created</li> <li>The pod incorrectly gets the first filesystem mounted instead of the intended filesystem</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#solution_7","text":"<p>The only solution to this problem is to upgrade your kernel to <code>4.7</code> or higher. This is due to a mount flag added in the kernel version <code>4.7</code> which allows to chose the filesystem by name.</p> <p>For additional info on the kernel version requirement for multiple shared filesystems (CephFS), see Filesystem - Kernel version requirement.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#set-debug-log-level-for-all-ceph-daemons","text":"<p>You can set a given log level and apply it to all the Ceph daemons at the same time. For this, make sure the toolbox pod is running, then determine the level you want (between 0 and 20). You can find the list of all subsystems and their default values in Ceph logging and debug official guide. Be careful when increasing the level as it will produce very verbose logs.</p> <p>Assuming you want a log level of 1, you will run:</p> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level 1\n</code></pre>  <p>Output:</p>  <pre><code> ceph config set global debug_context 1\n ceph config set global debug_lockdep 1\n...\n...\n</code></pre>   <p>Once you are done debugging, you can revert all the debug flag to their default value by running the following:</p> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level default\n</code></pre>","title":"Set debug log level for all Ceph daemons"},{"location":"Common%20Issues/ceph-common-issues/#activate-log-to-file-for-a-particular-ceph-daemon","text":"<p>They are cases where looking at Kubernetes logs is not enough for diverse reasons, but just to name a few:</p> <ul> <li>not everyone is familiar for Kubernetes logging and expects to find logs in traditional directories</li> <li>logs get eaten (buffer limit from the log engine) and thus not requestable from Kubernetes</li> </ul> <p>So for each daemon, <code>dataDirHostPath</code> is used to store logs, if logging is activated. Rook will bindmount <code>dataDirHostPath</code> for every pod. Let's say you want to enable logging for <code>mon.a</code>, but only for this daemon. Using the toolbox or from inside the operator run:</p> <pre><code>ceph config set mon.a log_to_file true\n</code></pre>  <p>This will activate logging on the filesystem, you will be able to find logs in <code>dataDirHostPath/$NAMESPACE/log</code>, so typically this would mean <code>/var/lib/rook/rook-ceph/log</code>. You don't need to restart the pod, the effect will be immediate.</p> <p>To disable the logging on file, simply set <code>log_to_file</code> to <code>false</code>.</p>","title":"Activate log to file for a particular Ceph daemon"},{"location":"Common%20Issues/ceph-common-issues/#a-worker-node-using-rbd-devices-hangs-up","text":"","title":"A worker node using RBD devices hangs up"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_7","text":"<ul> <li>There is no progress on I/O from/to one of RBD devices (<code>/dev/rbd*</code> or <code>/dev/nbd*</code>).</li> <li>After that, the whole worker node hangs up.</li> </ul>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_6","text":"<p>This happens when the following conditions are satisfied.</p> <ul> <li>The problematic RBD device and the corresponding OSDs are co-located.</li> <li>There is an XFS filesystem on top of this device.</li> </ul> <p>In addition, when this problem happens, you can see the following messages in <code>dmesg</code>.</p> <pre><code>dmesg\n</code></pre>   <pre><code>...\n[51717.039319] INFO: task kworker/2:1:5938 blocked for more than 120 seconds.\n[51717.039361]       Not tainted 4.15.0-72-generic #81-Ubuntu\n[51717.039388] \"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n...\n</code></pre>   <p>It's so-called <code>hung_task</code> problem and means that there is a deadlock in the kernel. For more detail, please refer to the corresponding issue comment.</p>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#solution_8","text":"<p>This problem will be solve by the following two fixes.</p> <ul> <li>Linux kernel: A minor feature that is introduced by this commit. It will be included in Linux v5.6.</li> <li>Ceph: A fix that uses the above-mentioned kernel's feature. The Ceph community will probably discuss this fix after releasing Linux v5.6.</li> </ul> <p>You can bypass this problem by using ext4 or any other filesystems rather than XFS. Filesystem type can be specified with <code>csi.storage.k8s.io/fstype</code> in StorageClass resource.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#too-few-pgs-per-osd-warning-is-shown","text":"","title":"Too few PGs per OSD warning is shown"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_8","text":"<ul> <li><code>ceph status</code> shows \"too few PGs per OSD\" warning as follows.</li> </ul> <pre><code>ceph status\n</code></pre>   <pre><code>  cluster:\n    id:     fd06d7c3-5c5c-45ca-bdea-1cf26b783065\n    health: HEALTH_WARN\n            too few PGs per OSD (16 &lt; min 30)\n</code></pre>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#solution_9","text":"<p>The meaning of this warning is written in the document. However, in many cases it is benign. For more information, please see the blog entry. Please refer to Configuring Pools if you want to know the proper <code>pg_num</code> of pools and change these values.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc","text":"","title":"LVM metadata can be corrupted with OSD on LV-backed PVC"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_9","text":"<p>There is a critical flaw in OSD on LV-backed PVC. LVM metadata can be corrupted if both the host and OSD container modify it simultaneously. For example, the administrator might modify it on the host, while the OSD initialization process in a container could modify it too. In addition, if <code>lvmetad</code> is running, the possibility of occurrence gets higher. In this case, the change of LVM metadata in OSD container is not reflected to LVM metadata cache in host for a while.</p> <p>If you still decide to configure an OSD on LVM, please keep the following in mind to reduce the probability of this issue.</p>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#solution_10","text":"<ul> <li>Disable <code>lvmetad.</code></li> <li>Avoid configuration of LVs from the host. In addition, don't touch the VGs and physical volumes that back these LVs.</li> <li>Avoid incrementing the <code>count</code> field of <code>storageClassDeviceSets</code> and create a new LV that backs an OSD simultaneously.</li> </ul> <p>You can know whether the above-mentioned tag exists with the command: <code>sudo lvs -o lv_name,lv_tags</code>. If the <code>lv_tag</code> field is empty in an LV corresponding to the OSD lv_tags, this OSD encountered the problem. In this case, please retire this OSD or replace with other new OSD before restarting.</p> <p>This problem doesn't happen in newly created LV-backed PVCs because OSD container doesn't modify LVM metadata anymore. The existing lvm mode OSDs work continuously even thought upgrade your Rook. However, using the raw mode OSDs is recommended because of the above-mentioned problem. You can replace the existing OSDs with raw mode OSDs by retiring them and adding new OSDs one by one. See the documents Remove an OSD and Add an OSD on a PVC.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#osd-prepare-job-fails-due-to-low-aio-max-nr-setting","text":"<p>If the Kernel is configured with a low aio-max-nr setting, the OSD prepare job might fail with the following error:</p> <pre><code>exec: stderr: 2020-09-17T00:30:12.145+0000 7f0c17632f40 -1 bdev(0x56212de88700 /var/lib/ceph/osd/ceph-0//block) _aio_start io_setup(2) failed with EAGAIN; try increasing /proc/sys/fs/aio-max-nr\n</code></pre>  <p>To overcome this, you need to increase the value of <code>fs.aio-max-nr</code> of your sysctl configuration (typically <code>/etc/sysctl.conf</code>). You can do this with your favorite configuration management system.</p> <p>Alternatively, you can have a DaemonSet to apply the configuration for you on all your nodes.</p>","title":"OSD prepare job fails due to low aio-max-nr setting"},{"location":"Common%20Issues/ceph-common-issues/#unexpected-partitions-created","text":"","title":"Unexpected partitions created"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_10","text":"<p>Users running Rook versions v1.6.0-v1.6.7 may observe unwanted OSDs on partitions that appear unexpectedly and seemingly randomly, which can corrupt existing OSDs.</p> <p>Unexpected partitions are created on host disks that are used by Ceph OSDs. This happens more often on SSDs than HDDs and usually only on disks that are 875GB or larger. Many tools like <code>lsblk</code>, <code>blkid</code>, <code>udevadm</code>, and <code>parted</code> will not show a partition table type for the partition. Newer versions of <code>blkid</code> are generally able to recognize the type as \"atari\".</p> <p>The underlying issue causing this is Atari partition (sometimes identified as AHDI) support in the Linux kernel. Atari partitions have very relaxed specifications compared to other partition types, and it is relatively easy for random data written to a disk to appear as an Atari partition to the Linux kernel. Ceph's Bluestore OSDs have an anecdotally high probability of writing data on to disks that can appear to the kernel as an Atari partition.</p> <p>Below is an example of <code>lsblk</code> output from a node where phantom Atari partitions are present. Note that <code>sdX1</code> is never present for the phantom partitions, and <code>sdX2</code> is 48G on all disks. <code>sdX3</code> is a variable size and may not always be present. It is possible for <code>sdX4</code> to appear, though it is an anecdotally rare event. <pre><code># lsblk\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsdb      8:16   0     3T  0 disk\n\u251c\u2500sdb2   8:18   0    48G  0 part\n\u2514\u2500sdb3   8:19   0   6.1M  0 part\nsdc      8:32   0     3T  0 disk\n\u251c\u2500sdc2   8:34   0    48G  0 part\n\u2514\u2500sdc3   8:35   0   6.2M  0 part\nsdd      8:48   0     3T  0 disk\n\u251c\u2500sdd2   8:50   0    48G  0 part\n\u2514\u2500sdd3   8:51   0   6.3M  0 part\n</code></pre> </p> <p>You can see https://github.com/rook/rook/issues/7940 for more detailed information and discussion.</p>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#solution_11","text":"","title":"Solution"},{"location":"Common%20Issues/ceph-common-issues/#recover-from-corruption-v160-v167","text":"<p>If you are using Rook v1.6, you must first update to v1.6.8 or higher to avoid further incidents of OSD corruption caused by these Atari partitions.</p> <p>An old workaround suggested using <code>deviceFilter: ^sd[a-z]+$</code>, but this still results in unexpected partitions. Rook will merely stop creating new OSDs on the partitions. It does not fix a related issue that <code>ceph-volume</code> that is unaware of the Atari partition problem. Users who used this workaround are still at risk for OSD failures in the future.</p> <p>To resolve the issue, immediately update to v1.6.8 or higher. After the update, no corruption should occur on OSDs created in the future. Next, to get back to a healthy Ceph cluster state, focus on one corruped disk at a time and remove all OSDs on each corrupted disk one disk at a time.</p> <p>As an example, you may have <code>/dev/sdb</code> with two unexpected partitions (<code>/dev/sdb2</code> and <code>/dev/sdb3</code>) as well as a second corrupted disk <code>/dev/sde</code> with one unexpected partition (<code>/dev/sde2</code>). 1. First, remove the OSDs associated with <code>/dev/sdb</code>, <code>/dev/sdb2</code>, and <code>/dev/sdb3</code>. There might be    only one, or up to 3 OSDs depending on how your system was affected. Again see the OSD management doc. 2. Use <code>dd</code> to wipe the first sectors of the partitions followed by the disk itself. E.g.,    - <code>dd if=/dev/zero of=/dev/sdb2 bs=1M</code>    - <code>dd if=/dev/zero of=/dev/sdb3 bs=1M</code>    - <code>dd if=/dev/zero of=/dev/sdb bs=1M</code> 3. Then wipe clean <code>/dev/sdb</code> to prepare it for a new OSD.    See the teardown document for details. 4. After this, scale up the Rook operator to deploy a new OSD to <code>/dev/sdb</code>. This will allow Ceph to    use <code>/dev/sdb</code> for data recovery and replication while the next OSDs are removed. 5. Now Repeat steps 1-4 for <code>/dev/sde</code> and <code>/dev/sde2</code>, and continue for any other corruped disks.</p> <p>If your Rook-Ceph cluster does not have any critical data stored in it, it may be simpler to uninstall Rook completely and redeploy with v1.6.8 or higher.</p>","title":"Recover from corruption (v1.6.0-v1.6.7)"},{"location":"Common%20Issues/ceph-common-issues/#operator-environment-variables-are-ignored","text":"","title":"Operator environment variables are ignored"},{"location":"Common%20Issues/ceph-common-issues/#symptoms_11","text":"<p>Configuration settings passed as environment variables do not take effect as expected. For example, the discover daemonset is not created, even though <code>ROOK_ENABLE_DISCOVERY_DAEMON=\"true\"</code> is set.</p>","title":"Symptoms"},{"location":"Common%20Issues/ceph-common-issues/#investigation_7","text":"<p>Inspect the <code>rook-ceph-operator-config</code> ConfigMap for conflicting settings. The ConfigMap takes precedence over the environment. The ConfigMap must exist, even if all actual configuration is supplied through the environment.</p> <p>Look for lines with the <code>op-k8sutil</code> prefix in the operator logs. These lines detail the final values, and source, of the different configuration variables.</p> <p>Verify that both of the following messages are present in the operator logs:</p> <pre><code>rook-ceph-operator-config-controller successfully started\nrook-ceph-operator-config-controller done reconciling\n</code></pre>","title":"Investigation"},{"location":"Common%20Issues/ceph-common-issues/#solution_12","text":"<p>If it does not exist, create an empty ConfigMap:</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: rook-ceph-operator-config\n  namespace: rook-ceph # namespace:operator\ndata: {}\n</code></pre>  <p>If the ConfigMap exists, remove any keys that you wish to configure through the environment.</p>","title":"Solution"},{"location":"Common%20Issues/ceph-disaster-recovery/","text":"<p>Under extenuating circumstances, steps may be necessary to recover the cluster health. There are several types of recovery addressed in this document: * Restoring Mon Quorum * Restoring CRDs After Deletion * Adopt an existing Rook Ceph cluster into a new Kubernetes cluster * Backing up and restoring a cluster based on PVCs into a new Kubernetes cluster</p>","title":"Disaster Recovery"},{"location":"Common%20Issues/ceph-disaster-recovery/#restoring-mon-quorum","text":"<p>Under extenuating circumstances, the mons may lose quorum. If the mons cannot form quorum again, there is a manual procedure to get the quorum going again. The only requirement is that at least one mon is still healthy. The following steps will remove the unhealthy mons from quorum and allow you to form a quorum again with a single mon, then grow the quorum back to the original size.</p> <p>For example, if you have three mons and lose quorum, you will need to remove the two bad mons from quorum, notify the good mon that it is the only mon in quorum, and then restart the good mon.</p>","title":"Restoring Mon Quorum"},{"location":"Common%20Issues/ceph-disaster-recovery/#stop-the-operator","text":"<p>First, stop the operator so it will not try to failover the mons while we are modifying the monmap</p> <pre><code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0\n</code></pre>","title":"Stop the operator"},{"location":"Common%20Issues/ceph-disaster-recovery/#inject-a-new-monmap","text":"<p>WARNING: Injecting a monmap must be done very carefully. If run incorrectly, your cluster could be permanently destroyed.</p>  <p>The Ceph monmap keeps track of the mon quorum. We will update the monmap to only contain the healthy mon. In this example, the healthy mon is <code>rook-ceph-mon-b</code>, while the unhealthy mons are <code>rook-ceph-mon-a</code> and <code>rook-ceph-mon-c</code>.</p> <p>Take a backup of the current <code>rook-ceph-mon-b</code> Deployment:</p> <pre><code>kubectl -n rook-ceph get deployment rook-ceph-mon-b -o yaml &gt; rook-ceph-mon-b-deployment.yaml\n</code></pre>  <p>Open the file and copy the <code>command</code> and <code>args</code> from the <code>mon</code> container (see <code>containers</code> list). This is needed for the monmap changes. Cleanup the copied <code>command</code> and <code>args</code> fields to form a pastable command. Example:</p> <p>The following parts of the <code>mon</code> container:</p> <pre><code>[...]\n  containers:\n  - args:\n    - --fsid=41a537f2-f282-428e-989f-a9e07be32e47\n    - --keyring=/etc/ceph/keyring-store/keyring\n    - --log-to-stderr=true\n    - --err-to-stderr=true\n    - --mon-cluster-log-to-stderr=true\n    - '--log-stderr-prefix=debug '\n    - --default-log-to-file=false\n    - --default-mon-cluster-log-to-file=false\n    - --mon-host=$(ROOK_CEPH_MON_HOST)\n    - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)\n    - --id=b\n    - --setuser=ceph\n    - --setgroup=ceph\n    - --foreground\n    - --public-addr=10.100.13.242\n    - --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db\n    - --public-bind-addr=$(ROOK_POD_IP)\n    command:\n    - ceph-mon\n[...]\n</code></pre>  <p>Should be made into a command like this: (do not copy the example command!)</p> <pre><code>ceph-mon \\\n    --fsid=41a537f2-f282-428e-989f-a9e07be32e47 \\\n    --keyring=/etc/ceph/keyring-store/keyring \\\n    --log-to-stderr=true \\\n    --err-to-stderr=true \\\n    --mon-cluster-log-to-stderr=true \\\n    --log-stderr-prefix=debug \\\n    --default-log-to-file=false \\\n    --default-mon-cluster-log-to-file=false \\\n    --mon-host=$ROOK_CEPH_MON_HOST \\\n    --mon-initial-members=$ROOK_CEPH_MON_INITIAL_MEMBERS \\\n    --id=b \\\n    --setuser=ceph \\\n    --setgroup=ceph \\\n    --foreground \\\n    --public-addr=10.100.13.242 \\\n    --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db \\\n    --public-bind-addr=$ROOK_POD_IP\n</code></pre>  <p>(be sure to remove the single quotes around the <code>--log-stderr-prefix</code> flag and the parenthesis around the variables being passed ROOK_CEPH_MON_HOST, ROOK_CEPH_MON_INITIAL_MEMBERS and ROOK_POD_IP )</p> <p>Patch the <code>rook-ceph-mon-b</code> Deployment to stop this mon working without deleting the mon pod:</p> <pre><code>kubectl -n rook-ceph patch deployment rook-ceph-mon-b  --type='json' -p '[{\"op\":\"remove\", \"path\":\"/spec/template/spec/containers/0/livenessProbe\"}]'\n\nkubectl -n rook-ceph patch deployment rook-ceph-mon-b -p '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"mon\", \"command\": [\"sleep\", \"infinity\"], \"args\": []}]}}}}'\n</code></pre>  <p>Connect to the pod of a healthy mon and run the following commands.</p> <pre><code>kubectl -n rook-ceph exec -it &lt;mon-pod&gt; bash\n</code></pre>   <pre><code># set a few simple variables\ncluster_namespace=rook-ceph\ngood_mon_id=b\nmonmap_path=/tmp/monmap\n\n# extract the monmap to a file, by pasting the ceph mon command\n# from the good mon deployment and adding the\n# `--extract-monmap=${monmap_path}` flag\nceph-mon \\\n    --fsid=41a537f2-f282-428e-989f-a9e07be32e47 \\\n    --keyring=/etc/ceph/keyring-store/keyring \\\n    --log-to-stderr=true \\\n    --err-to-stderr=true \\\n    --mon-cluster-log-to-stderr=true \\\n    --log-stderr-prefix=debug \\\n    --default-log-to-file=false \\\n    --default-mon-cluster-log-to-file=false \\\n    --mon-host=$ROOK_CEPH_MON_HOST \\\n    --mon-initial-members=$ROOK_CEPH_MON_INITIAL_MEMBERS \\\n    --id=b \\\n    --setuser=ceph \\\n    --setgroup=ceph \\\n    --foreground \\\n    --public-addr=10.100.13.242 \\\n    --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db \\\n    --public-bind-addr=$ROOK_POD_IP \\\n    --extract-monmap=${monmap_path}\n\n# review the contents of the monmap\nmonmaptool --print /tmp/monmap\n\n# remove the bad mon(s) from the monmap\nmonmaptool ${monmap_path} --rm &lt;bad_mon&gt;\n\n# in this example we remove mon0 and mon2:\nmonmaptool ${monmap_path} --rm a\nmonmaptool ${monmap_path} --rm c\n\n# inject the modified monmap into the good mon, by pasting\n# the ceph mon command and adding the\n# `--inject-monmap=${monmap_path}` flag, like this\nceph-mon \\\n    --fsid=41a537f2-f282-428e-989f-a9e07be32e47 \\\n    --keyring=/etc/ceph/keyring-store/keyring \\\n    --log-to-stderr=true \\\n    --err-to-stderr=true \\\n    --mon-cluster-log-to-stderr=true \\\n    --log-stderr-prefix=debug \\\n    --default-log-to-file=false \\\n    --default-mon-cluster-log-to-file=false \\\n    --mon-host=$ROOK_CEPH_MON_HOST \\\n    --mon-initial-members=$ROOK_CEPH_MON_INITIAL_MEMBERS \\\n    --id=b \\\n    --setuser=ceph \\\n    --setgroup=ceph \\\n    --foreground \\\n    --public-addr=10.100.13.242 \\\n    --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db \\\n    --public-bind-addr=$ROOK_POD_IP \\\n    --inject-monmap=${monmap_path}\n</code></pre>   <p>Exit the shell to continue.</p>","title":"Inject a new monmap"},{"location":"Common%20Issues/ceph-disaster-recovery/#edit-the-rook-configmaps","text":"<p>Edit the configmap that the operator uses to track the mons.</p> <pre><code>kubectl -n rook-ceph edit configmap rook-ceph-mon-endpoints\n</code></pre>  <p>In the <code>data</code> element you will see three mons such as the following (or more depending on your <code>moncount</code>):</p> <pre><code>data: a=10.100.35.200:6789;b=10.100.13.242:6789;c=10.100.35.12:6789\n</code></pre>  <p>Delete the bad mons from the list, for example to end up with a single good mon:</p> <pre><code>data: b=10.100.13.242:6789\n</code></pre>  <p>Save the file and exit.</p> <p>Now we need to adapt a Secret which is used for the mons and other components. The following <code>kubectl patch</code> command is an easy way to do that. In the end it patches the <code>rook-ceph-config</code> secret and updates the two key/value pairs <code>mon_host</code> and <code>mon_initial_members</code>.</p> <pre><code>mon_host=$(kubectl -n rook-ceph get svc rook-ceph-mon-b -o jsonpath='{.spec.clusterIP}')\nkubectl -n rook-ceph patch secret rook-ceph-config -p '{\"stringData\": {\"mon_host\": \"[v2:'\"${mon_host}\"':3300,v1:'\"${mon_host}\"':6789]\", \"mon_initial_members\": \"'\"${good_mon_id}\"'\"}}'\n</code></pre>   <p>NOTE: If you are using <code>hostNetwork: true</code>, you need to replace the <code>mon_host</code> var with the node IP the mon is pinned to (<code>nodeSelector</code>). This is because there is no <code>rook-ceph-mon-*</code> service created in that \"mode\".</p>","title":"Edit the Rook configmaps"},{"location":"Common%20Issues/ceph-disaster-recovery/#restart-the-mon","text":"<p>You will need to \"restart\" the good mon pod with the original <code>ceph-mon</code> command to pick up the changes. For this run <code>kubectl replace</code> on the backup of the mon deployment yaml:</p> <pre><code>kubectl replace --force -f rook-ceph-mon-b-deployment.yaml\n</code></pre>   <p>NOTE: Option <code>--force</code> will delete the deployment and create a new one</p>  <p>Start the rook toolbox and verify the status of the cluster.</p> <pre><code>ceph -s\n</code></pre>  <p>The status should show one mon in quorum. If the status looks good, your cluster should be healthy again.</p>","title":"Restart the mon"},{"location":"Common%20Issues/ceph-disaster-recovery/#restart-the-operator","text":"<p>Start the rook operator again to resume monitoring the health of the cluster. <pre><code># create the operator. it is safe to ignore the errors that a number of resources already exist.\nkubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1\n</code></pre> </p> <p>The operator will automatically add more mons to increase the quorum size again, depending on the <code>mon.count</code>.</p>","title":"Restart the operator"},{"location":"Common%20Issues/ceph-disaster-recovery/#restoring-crds-after-deletion","text":"<p>When the Rook CRDs are deleted, the Rook operator will respond to the deletion event to attempt to clean up the cluster resources. If any data appears present in the cluster, Rook will refuse to allow the resources to be deleted since the operator will refuse to remove the finalizer on the CRs until the underlying data is deleted. For more details, see the dependency design doc.</p> <p>While it is good that the CRs will not be deleted and the underlying Ceph data and daemons continue to be available, the CRs will be stuck indefinitely in a <code>Deleting</code> state in which the operator will not continue to ensure cluster health. Upgrades will be blocked, further updates to the CRs are prevented, and so on. Since Kubernetes does not allow undeleting resources, the following procedure will allow you to restore the CRs to their prior state without even necessarily suffering cluster downtime.</p> <ol> <li>Scale down the operator</li> </ol> <pre><code>kubectl -n rook-ceph scale --replicas=0 deploy/rook-ceph-operator\n</code></pre>  <ol> <li>Backup all Rook CRs and critical metadata</li> </ol> <pre><code># Store the CephCluster CR settings. Also, save other Rook CRs that are in terminating state.\nkubectl -n rook-ceph get cephcluster rook-ceph -o yaml &gt; cluster.yaml\n\n# Backup critical secrets and configmaps in case something goes wrong later in the procedure\nkubectl -n rook-ceph get secret -o yaml &gt; secrets.yaml\nkubectl -n rook-ceph get configmap -o yaml &gt; configmaps.yaml\n</code></pre>  <ol> <li>Remove the owner references from all critical Rook resources that were referencing the CephCluster CR.    The critical resources include:</li> <li>Secrets: <code>rook-ceph-admin-keyring</code>, <code>rook-ceph-config</code>, <code>rook-ceph-mon</code>, <code>rook-ceph-mons-keyring</code></li> <li>ConfigMap: <code>rook-ceph-mon-endpoints</code></li> <li>Services: <code>rook-ceph-mon-*</code>, <code>rook-ceph-mgr-*</code></li> <li>Deployments: <code>rook-ceph-mon-*</code>, <code>rook-ceph-osd-*</code>, <code>rook-ceph-mgr-*</code></li> <li>PVCs (if applicable): <code>rook-ceph-mon-*</code> and the OSD PVCs (named <code>&lt;deviceset&gt;-*</code>, for example <code>set1-data-*</code>)</li> </ol> <p>For example, remove this entire block from each resource:</p> <pre><code>ownerReferences:\n- apiVersion: ceph.rook.io/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: CephCluster\n    name: rook-ceph\n    uid: &lt;uid&gt;\n</code></pre>  <ol> <li>After confirming all critical resources have had the owner reference to the CephCluster CR removed, now    we allow the cluster CR to be deleted. Remove the finalizer by editing the CephCluster CR.</li> </ol> <pre><code>kubectl -n rook-ceph edit cephcluster\n</code></pre>  <p>For example, remove the following from the CR metadata:</p> <pre><code>    finalizers:\n    - cephcluster.ceph.rook.io\n</code></pre>  <p>After the finalizer is removed, the CR will be immediately deleted. If all owner references were properly removed, all ceph daemons will continue running and there will be no downtime.</p> <ol> <li>Create the CephCluster CR with the same settings as previously</li> </ol> <pre><code># Use the same cluster settings as exported above in step 2.\nkubectl create -f cluster.yaml\n</code></pre>  <ol> <li>If there are other CRs in terminating state such as CephBlockPools, CephObjectStores, or CephFilesystems,    follow the above steps as well for those CRs:</li> <li>Backup the CR</li> <li>Remove the finalizer and confirm the CR is deleted (the underlying Ceph resources will be preserved)</li> <li> <p>Create the CR again</p> </li> <li> <p>Scale up the operator</p> </li> </ol> <pre><code>kubectl -n rook-ceph scale --replicas=1 deploy/rook-ceph-operator\n</code></pre>  <p>Watch the operator log to confirm that the reconcile completes successfully.</p>","title":"Restoring CRDs After Deletion"},{"location":"Common%20Issues/ceph-disaster-recovery/#adopt-an-existing-rook-ceph-cluster-into-a-new-kubernetes-cluster","text":"<p>Situations this section can help resolve:</p> <ol> <li>The Kubernetes environment underlying a running Rook Ceph cluster failed catastrophically, requiring a new Kubernetes environment in which the user wishes to recover the previous Rook Ceph cluster.</li> <li>The user wishes to migrate their existing Rook Ceph cluster to a new Kubernetes environment, and downtime can be tolerated.</li> </ol>","title":"Adopt an existing Rook Ceph cluster into a new Kubernetes cluster"},{"location":"Common%20Issues/ceph-disaster-recovery/#prerequisites","text":"<ol> <li>A working Kubernetes cluster to which we will migrate the previous Rook Ceph cluster.</li> <li>At least one Ceph mon db is in quorum, and sufficient number of Ceph OSD is <code>up</code> and <code>in</code> before disaster.</li> <li>The previous Rook Ceph cluster is not running.</li> </ol>","title":"Prerequisites"},{"location":"Common%20Issues/ceph-disaster-recovery/#overview-for-steps-below","text":"<ol> <li>Start a new and clean Rook Ceph cluster, with old <code>CephCluster</code> <code>CephBlockPool</code> <code>CephFilesystem</code> <code>CephNFS</code> <code>CephObjectStore</code>.</li> <li>Shut the new cluster down when it has been created successfully.</li> <li>Replace ceph-mon data with that of the old cluster.</li> <li>Replace <code>fsid</code> in <code>secrets/rook-ceph-mon</code> with that of the old one.</li> <li>Fix monmap in ceph-mon db.</li> <li>Fix ceph mon auth key.</li> <li>Disable auth.</li> <li>Start the new cluster, watch it resurrect.</li> <li>Fix admin auth key, and enable auth.</li> <li>Restart cluster for the final time.</li> </ol>","title":"Overview for Steps below"},{"location":"Common%20Issues/ceph-disaster-recovery/#steps","text":"<p>Assuming <code>dataHostPathData</code> is <code>/var/lib/rook</code>, and the <code>CephCluster</code> trying to adopt is named <code>rook-ceph</code>.</p> <ol> <li>Make sure the old Kubernetes cluster is completely torn down and the new Kubernetes cluster is up and running without Rook Ceph.</li> <li>Backup <code>/var/lib/rook</code> in all the Rook Ceph nodes to a different directory. Backups will be used later.</li> <li>Pick a <code>/var/lib/rook/rook-ceph/rook-ceph.config</code> from any previous Rook Ceph node and save the old cluster <code>fsid</code> from its content.</li> <li>Remove <code>/var/lib/rook</code> from all the Rook Ceph nodes.</li> <li>Add identical <code>CephCluster</code> descriptor to the new Kubernetes cluster, especially identical <code>spec.storage.config</code> and <code>spec.storage.nodes</code>, except <code>mon.count</code>, which should be set to <code>1</code>.</li> <li>Add identical <code>CephFilesystem</code> <code>CephBlockPool</code> <code>CephNFS</code> <code>CephObjectStore</code> descriptors (if any) to the new Kubernetes cluster.</li> <li>Install Rook Ceph in the new Kubernetes cluster.</li> <li>Watch the operator logs with <code>kubectl -n rook-ceph logs -f rook-ceph-operator-xxxxxxx</code>, and wait until the orchestration has settled.</li> <li>STATE: Now the cluster will have <code>rook-ceph-mon-a</code>, <code>rook-ceph-mgr-a</code>, and all the auxiliary pods up and running, and zero (hopefully) <code>rook-ceph-osd-ID-xxxxxx</code> running. <code>ceph -s</code> output should report 1 mon, 1 mgr running, and all of the OSDs down, all PGs are in <code>unknown</code> state. Rook should not start any OSD daemon since all devices belongs to the old cluster (which have a different <code>fsid</code>).</li> <li> <p>Run <code>kubectl -n rook-ceph exec -it rook-ceph-mon-a-xxxxxxxx bash</code> to enter the <code>rook-ceph-mon-a</code> pod,</p> <pre><code>mon-a# cat /etc/ceph/keyring-store/keyring  # save this keyring content for later use\nmon-a# exit\n</code></pre>  </li> <li> <p>Stop the Rook operator by running <code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code> and set <code>replicas</code> to <code>0</code>.</p> </li> <li>Stop cluster daemons by running <code>kubectl -n rook-ceph delete deploy/X</code> where X is every deployment in namespace <code>rook-ceph</code>, except <code>rook-ceph-operator</code> and <code>rook-ceph-tools</code>.</li> <li> <p>Save the <code>rook-ceph-mon-a</code> address with <code>kubectl -n rook-ceph get cm/rook-ceph-mon-endpoints -o yaml</code> in the new Kubernetes cluster for later use.</p> </li> <li> <p>SSH to the host where <code>rook-ceph-mon-a</code> in the new Kubernetes cluster resides.</p> <ol> <li>Remove <code>/var/lib/rook/mon-a</code></li> <li>Pick a healthy <code>rook-ceph-mon-ID</code> directory (<code>/var/lib/rook/mon-ID</code>) in the previous backup, copy to <code>/var/lib/rook/mon-a</code>. <code>ID</code> is any healthy mon node ID of the old cluster.</li> <li>Replace <code>/var/lib/rook/mon-a/keyring</code> with the saved keyring, preserving only the <code>[mon.]</code> section, remove <code>[client.admin]</code> section.</li> <li> <p>Run <code>docker run -it --rm -v /var/lib/rook:/var/lib/rook ceph/ceph:v14.2.1-20190430 bash</code>. The Docker image tag should match the Ceph version used in the Rook cluster. The <code>/etc/ceph/ceph.conf</code> file needs to exist for <code>ceph-mon</code> to work.</p> <pre><code>touch /etc/ceph/ceph.conf\ncd /var/lib/rook\nceph-mon --extract-monmap monmap --mon-data ./mon-a/data  # Extract monmap from old ceph-mon db and save as monmap\nmonmaptool --print monmap  # Print the monmap content, which reflects the old cluster ceph-mon configuration.\nmonmaptool --rm a monmap  # Delete `a` from monmap.\nmonmaptool --rm b monmap  # Repeat, and delete `b` from monmap.\nmonmaptool --rm c monmap  # Repeat this pattern until all the old ceph-mons are removed\nmonmaptool --rm d monmap\nmonmaptool --rm e monmap\nmonmaptool --addv a [v2:10.77.2.216:3300,v1:10.77.2.216:6789] monmap   # Replace it with the rook-ceph-mon-a address you got from previous command.\nceph-mon --inject-monmap monmap --mon-data ./mon-a/data  # Replace monmap in ceph-mon db with our modified version.\nrm monmap\nexit\n</code></pre>  </li> </ol> </li> <li> <p>Tell Rook to run as old cluster by running <code>kubectl -n rook-ceph edit secret/rook-ceph-mon</code> and changing <code>fsid</code> to the original <code>fsid</code>. Note that the <code>fsid</code> is base64 encoded and must not contain a trailing carriage return. For example:</p> <pre><code>$ echo -n a811f99a-d865-46b7-8f2c-f94c064e4356 | base64  # Replace with the fsid from your old cluster.\n</code></pre>  </li> <li> <p>Disable authentication by running <code>kubectl -n rook-ceph edit cm/rook-config-override</code> and adding content below:</p> <pre><code>data:\nconfig: |\n    [global]\n    auth cluster required = none\n    auth service required = none\n    auth client required = none\n    auth supported = none\n</code></pre>  </li> <li> <p>Bring the Rook Ceph operator back online by running <code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code> and set <code>replicas</code> to <code>1</code>.</p> </li> <li>Watch the operator logs with <code>kubectl -n rook-ceph logs -f rook-ceph-operator-xxxxxxx</code>, and wait until the orchestration has settled.</li> <li>STATE: Now the new cluster should be up and running with authentication disabled. <code>ceph -s</code> should report 1 mon &amp; 1 mgr &amp; all of the OSDs up and running, and all PGs in either <code>active</code> or <code>degraded</code> state.</li> <li> <p>Run <code>kubectl -n rook-ceph exec -it rook-ceph-tools-XXXXXXX bash</code> to enter tools pod:</p> <pre><code>vi key\n# [paste keyring content saved before, preserving only `[client admin]` section]\nceph auth import -i key\nrm key\n</code></pre>  </li> <li> <p>Re-enable authentication by running <code>kubectl -n rook-ceph edit cm/rook-config-override</code> and removing auth configuration added in previous steps.</p> </li> <li>Stop the Rook operator by running <code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code> and set <code>replicas</code> to <code>0</code>.</li> <li>Shut down entire new cluster by running <code>kubectl -n rook-ceph delete deploy/X</code> where X is every deployment in namespace <code>rook-ceph</code>, except <code>rook-ceph-operator</code> and <code>rook-ceph-tools</code>, again. This time OSD daemons are present and should be removed too.</li> <li>Bring the Rook Ceph operator back online by running <code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code> and set <code>replicas</code> to <code>1</code>.</li> <li>Watch the operator logs with <code>kubectl -n rook-ceph logs -f rook-ceph-operator-xxxxxxx</code>, and wait until the orchestration has settled.</li> <li>STATE: Now the new cluster should be up and running with authentication enabled. <code>ceph -s</code> output should not change much comparing to previous steps.</li> </ol>","title":"Steps"},{"location":"Common%20Issues/ceph-disaster-recovery/#backing-up-and-restoring-a-cluster-based-on-pvcs-into-a-new-kubernetes-cluster","text":"<p>It is possible to migrate/restore an rook/ceph cluster from an existing Kubernetes cluster to a new one without resorting to SSH access or ceph tooling. This allows doing the migration using standard kubernetes resources only. This guide assumes the following 1. You have a CephCluster that uses PVCs to persist mon and osd data. Let's call it the \"old cluster\" 1. You can restore the PVCs as-is in the new cluster. Usually this is done by taking regular snapshots of the PVC volumes and using a tool that can re-create PVCs from these snapshots in the underlying cloud provider. Velero is one such tool. (https://github.com/vmware-tanzu/velero) 1. You have regular backups of the secrets and configmaps in the rook-ceph namespace. Velero provides this functionality too.</p> <p>Do the following in the new cluster: 1. Stop the rook operator by scaling the deployment <code>rook-ceph-operator</code> down to zero: <code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas 0</code> and deleting the other deployments. An example command to do this is <code>k -n rook-ceph delete deployment -l operator!=rook</code> 1. Restore the rook PVCs to the new cluster. 1. Copy the keyring and fsid secrets from the old cluster: <code>rook-ceph-mgr-a-keyring</code>, <code>rook-ceph-mon</code>, <code>rook-ceph-mons-keyring</code>, <code>rook-ceph-osd-0-keyring</code>, ... 1. Delete mon services and copy them from the old cluster: <code>rook-ceph-mon-a</code>, <code>rook-ceph-mon-b</code>, ... Note that simply re-applying won't work because the goal here is to restore the <code>clusterIP</code> in each service and this field is immutable in <code>Service</code> resources. 1. Copy the endpoints configmap from the old cluster: <code>rook-ceph-mon-endpoints</code> 1. Scale the rook operator up again : <code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas 1</code> 1. Wait until the reconciliation is over.</p>","title":"Backing up and restoring a cluster based on PVCs into a new Kubernetes cluster"},{"location":"Common%20Issues/ceph-mon-health/","text":"<p>Failure in a distributed system is to be expected. Ceph was designed from the ground up to deal with the failures of a distributed system. At the next layer, Rook was designed from the ground up to automate recovery of Ceph components that traditionally required admin intervention. Monitor health is the most critical piece of the equation that Rook actively monitors. If they are not in a good state, the operator will take action to restore their health and keep your cluster protected from disaster.</p> <p>The Ceph monitors (mons) are the brains of the distributed cluster. They control all of the metadata that is necessary to store and retrieve your data as well as keep it safe. If the monitors are not in a healthy state you will risk losing all the data in your system.</p>","title":"Monitor Health"},{"location":"Common%20Issues/ceph-mon-health/#monitor-identity","text":"<p>Each monitor in a Ceph cluster has a static identity. Every component in the cluster is aware of the identity, and that identity must be immutable. The identity of a mon is its IP address.</p> <p>To have an immutable IP address in Kubernetes, Rook creates a K8s service for each monitor. The clusterIP of the service will act as the stable identity.</p> <p>When a monitor pod starts, it will bind to its podIP and it will expect communication to be via its service IP address.</p>","title":"Monitor Identity"},{"location":"Common%20Issues/ceph-mon-health/#monitor-quorum","text":"<p>Multiple mons work together to provide redundancy by each keeping a copy of the metadata. A variation of the distributed algorithm Paxos is used to establish consensus about the state of the cluster. Paxos requires a super-majority of mons to be running in order to establish quorum and perform operations in the cluster. If the majority of mons are not running, quorum is lost and nothing can be done in the cluster.</p>","title":"Monitor Quorum"},{"location":"Common%20Issues/ceph-mon-health/#how-many-mons","text":"<p>Most commonly a cluster will have three mons. This would mean that one mon could go down and allow the cluster to remain healthy. You would still have 2/3 mons running to give you consensus in the cluster for any operation.</p> <p>For highest availability, an odd number of mons is required. Fifty percent of mons will not be sufficient to maintain quorum. If you had two mons and one of them went down, you would have 1/2 of quorum. Since that is not a super-majority, the cluster would have to wait until the second mon is up again. Rook allows an even number of mons for higher durability. See the disaster recovery guide if quorum is lost and to recover mon quorum from a single mon.</p> <p>The number of mons to create in a cluster depends on your tolerance for losing a node. If you have 1 mon zero nodes can be lost to maintain quorum. With 3 mons one node can be lost, and with 5 mons two nodes can be lost. Because the Rook operator will automatically start a new new monitor if one dies, you typically only need three mons. The more mons you have, the more overhead there will be to make a change to the cluster, which could become a performance issue in a large cluster.</p>","title":"How many mons?"},{"location":"Common%20Issues/ceph-mon-health/#mitigating-monitor-failure","text":"<p>Whatever the reason that a mon may fail (power failure, software crash, software hang, etc), there are several layers of mitigation in place to help recover the mon. It is always better to bring an existing mon back up than to failover to bring up a new mon.</p> <p>The Rook operator creates a mon with a Deployment to ensure that the mon pod will always be restarted if it fails. If a mon pod stops for any reason, Kubernetes will automatically start the pod up again.</p> <p>In order for a mon to support a pod/node restart, the mon metadata is persisted to disk, either under the <code>dataDirHostPath</code> specified in the CephCluster CR, or in the volume defined by the <code>volumeClaimTemplate</code> in the CephCluster CR. This will allow the mon to start back up with its existing metadata and continue where it left off even if the pod had to be re-created. Without this persistence, the mon cannot restart.</p>","title":"Mitigating Monitor Failure"},{"location":"Common%20Issues/ceph-mon-health/#failing-over-a-monitor","text":"<p>If a mon is unhealthy and the K8s pod restart or liveness probe are not sufficient to bring a mon back up, the operator will make the decision to terminate the unhealthy monitor deployment and bring up a new monitor with a new identity. This is an operation that must be done while mon quorum is maintained by other mons in the cluster.</p> <p>The operator checks for mon health every 45 seconds. If a monitor is down, the operator will wait 10 minutes before failing over the unhealthy mon. These two intervals can be configured as parameters to the CephCluster CR (see below). If the intervals are too short, it could be unhealthy if the mons are failed over too aggressively. If the intervals are too long, the cluster could be at risk of losing quorum if a new monitor is not brought up before another mon fails.</p> <pre><code>healthCheck:\n  daemonHealth:\n    mon:\n      disabled: false\n      interval: 45s\n      timeout: 10m\n</code></pre>  <p>If you want to force a mon to failover for testing or other purposes, you can scale down the mon deployment to 0, then wait for the timeout. Note that the operator may scale up the mon again automatically if the operator is restarted or if a full reconcile is triggered, such as when the CephCluster CR is updated.</p> <p>If the mon pod is in pending state and couldn't be assigned to a node (say, due to node drain), then the operator will wait for the timeout again before the mon failover. So the timeout waiting for the mon failover will be doubled in this case.</p> <p>To disable monitor automatic failover, the <code>timeout</code> can be set to <code>0</code>, if the monitor goes out of quorum Rook will never fail it over onto another node. This is especially useful for planned maintenance.</p>","title":"Failing over a Monitor"},{"location":"Common%20Issues/ceph-mon-health/#example-failover","text":"<p>Rook will create mons with pod names such as mon-a, mon-b, and mon-c. Let's say mon-b had an issue and the pod failed. <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-mon\n</code></pre> </p>  <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\nrook-ceph-mon-a-74dc96545-ch5ns    1/1     Running   0          9m\nrook-ceph-mon-b-6b9d895c4c-bcl2h   1/1     Error     2          9m\nrook-ceph-mon-c-7d6df6d65c-5cjwl   1/1     Running   0          8m\n</code></pre>   <p>After a failover, you will see the unhealthy mon removed and a new mon added such as mon-d. A fully healthy mon quorum is now running again. <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-mon\n</code></pre> </p>  <pre><code>NAME                             READY     STATUS    RESTARTS   AGE\nrook-ceph-mon-a-74dc96545-ch5ns    1/1     Running   0          19m\nrook-ceph-mon-c-7d6df6d65c-5cjwl   1/1     Running   0          18m\nrook-ceph-mon-d-9e7ea7e76d-4bhxm   1/1     Running   0          20s\n</code></pre>   <p>From the toolbox we can verify the status of the health mon quorum: <pre><code>ceph -s\n</code></pre> </p>  <pre><code>  cluster:\n    id:     35179270-8a39-4e08-a352-a10c52bb04ff\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,d (age 2m)\n    mgr: a(active, since 12m)\n    osd: 3 osds: 3 up (since 10m), 3 in (since 10m)\n  ...\n</code></pre>","title":"Example Failover"},{"location":"Common%20Issues/ceph-openshift-issues/","text":"","title":"OpenShift Common Issues"},{"location":"Common%20Issues/ceph-openshift-issues/#enable-monitoring-in-the-storage-dashboard","text":"<p>OpenShift Console uses OpenShift Prometheus for monitoring and populating data in Storage Dashboard. Additional configuration is required to monitor the Ceph Cluster from the storage dashboard.</p> <ol> <li>Change the monitoring namespace to <code>openshift-monitoring</code></li> </ol> <p>Change the namespace of the RoleBinding <code>rook-ceph-metrics</code> from <code>rook-ceph</code> to <code>openshift-monitoring</code> for the <code>prometheus-k8s</code> ServiceAccount in rbac.yaml.</p> <pre><code>subjects:\n- kind: ServiceAccount\n  name: prometheus-k8s\n  namespace: openshift-monitoring\n</code></pre>  <ol> <li> <p>Enable Ceph Cluster monitoring</p> <p>Follow ceph-monitoring/prometheus-alerts.</p> </li> <li> <p>Set the required label on the namespace</p> <p><code>oc label namespace rook-ceph \"openshift.io/cluster-monitoring=true\"</code></p> </li> </ol>","title":"Enable Monitoring in the Storage Dashboard"},{"location":"Common%20Issues/ceph-openshift-issues/#troubleshoot-monitoring-issues","text":"<p>Pre-req: Switch to <code>rook-ceph</code> namespace with <code>oc project rook-ceph</code></p>  <ol> <li> <p>Ensure ceph-mgr pod is Running</p> <pre><code>oc get pods -l app=rook-ceph-mgr\n</code></pre>   <pre><code>NAME            READY   STATUS    RESTARTS   AGE\nrook-ceph-mgr   1/1     Running   0          14h\n</code></pre>   </li> <li> <p>Ensure service monitor is present</p> <pre><code>oc get servicemonitor rook-ceph-mgr\n</code></pre>   <pre><code>NAME                          AGE\nrook-ceph-mgr                 14h\n</code></pre>   </li> <li> <p>Ensure prometheus rules are present</p> <pre><code>oc get prometheusrules -l prometheus=rook-prometheus\n</code></pre>   <pre><code>NAME                    AGE\nprometheus-ceph-rules   14h\n</code></pre>   </li> </ol>","title":"Troubleshoot Monitoring Issues"},{"location":"Common%20Issues/ceph-osd-mgmt/","text":"<p>Ceph Object Storage Daemons (OSDs) are the heart and soul of the Ceph storage platform. Each OSD manages a local device and together they provide the distributed storage. Rook will automate creation and management of OSDs to hide the complexity based on the desired state in the CephCluster CR as much as possible. This guide will walk through some of the scenarios to configure OSDs where more configuration may be required.</p>","title":"Ceph OSD Management"},{"location":"Common%20Issues/ceph-osd-mgmt/#osd-health","text":"<p>The rook-ceph-tools pod provides a simple environment to run Ceph tools. The <code>ceph</code> commands mentioned in this document should be run from the toolbox.</p> <p>Once the is created, connect to the pod to execute the <code>ceph</code> commands to analyze the health of the cluster, in particular the OSDs and placement groups (PGs). Some common commands to analyze OSDs include: <pre><code>ceph status\nceph osd tree\nceph osd status\nceph osd df\nceph osd utilization\n</code></pre> </p> <pre><code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') bash\n</code></pre>","title":"OSD Health"},{"location":"Common%20Issues/ceph-osd-mgmt/#add-an-osd","text":"<p>The QuickStart Guide will provide the basic steps to create a cluster and start some OSDs. For more details on the OSD settings also see the Cluster CRD documentation. If you are not seeing OSDs created, see the Ceph Troubleshooting Guide.</p> <p>To add more OSDs, Rook will automatically watch for new nodes and devices being added to your cluster. If they match the filters or other settings in the <code>storage</code> section of the cluster CR, the operator will create new OSDs.</p>","title":"Add an OSD"},{"location":"Common%20Issues/ceph-osd-mgmt/#add-an-osd-on-a-pvc","text":"<p>In more dynamic environments where storage can be dynamically provisioned with a raw block storage provider, the OSDs can be backed by PVCs. See the <code>storageClassDeviceSets</code> documentation in the Cluster CRD topic.</p> <p>To add more OSDs, you can either increase the <code>count</code> of the OSDs in an existing device set or you can add more device sets to the cluster CR. The operator will then automatically create new OSDs according to the updated cluster CR.</p>","title":"Add an OSD on a PVC"},{"location":"Common%20Issues/ceph-osd-mgmt/#remove-an-osd","text":"<p>To remove an OSD due to a failed disk or other re-configuration, consider the following to ensure the health of the data through the removal process:</p> <ul> <li>Confirm you will have enough space on your cluster after removing your OSDs to properly handle the deletion</li> <li>Confirm the remaining OSDs and their placement groups (PGs) are healthy in order to handle the rebalancing of the data</li> <li>Do not remove too many OSDs at once</li> <li>Wait for rebalancing between removing multiple OSDs</li> </ul> <p>If all the PGs are <code>active+clean</code> and there are no warnings about being low on space, this means the data is fully replicated and it is safe to proceed. If an OSD is failing, the PGs will not be perfectly clean and you will need to proceed anyway.</p>","title":"Remove an OSD"},{"location":"Common%20Issues/ceph-osd-mgmt/#host-based-cluster","text":"<p>Update your CephCluster CR. Depending on your CR settings, you may need to remove the device from the list or update the device filter. If you are using <code>useAllDevices: true</code>, no change to the CR is necessary.</p> <p>IMPORTANT: On host-based clusters, you may need to stop the Rook Operator while performing OSD removal steps in order to prevent Rook from detecting the old OSD and trying to re-create it before the disk is wiped or removed.</p> <p>To stop the Rook Operator, run <code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0</code>.</p> <p>You must perform steps below to (1) purge the OSD and either (2.a) delete the underlying data or (2.b)replace the disk before starting the Rook Operator again.</p> <p>Once you have done that, you can start the Rook operator again with <code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1</code>.</p>","title":"Host-based cluster"},{"location":"Common%20Issues/ceph-osd-mgmt/#pvc-based-cluster","text":"<p>To reduce the storage in your cluster or remove a failed OSD on a PVC:</p> <ol> <li>Shrink the number of OSDs in the <code>storageClassDeviceSets</code> in the CephCluster CR. If you have multiple device sets,    you may need to change the index of <code>0</code> in this example path.</li> <li><code>kubectl -n rook-ceph patch CephCluster rook-ceph --type=json -p '[{\"op\": \"replace\", \"path\": \"/spec/storage/storageClassDeviceSets/0/count\", \"value\":&lt;desired number&gt;}]'</code></li> <li>Reduce the <code>count</code> of the OSDs to the desired number. Rook will not take any action to automatically remove the extra OSD(s).</li> <li>Identify the PVC that belongs to the OSD that is failed or otherwise being removed.</li> <li><code>kubectl -n rook-ceph get pvc -l ceph.rook.io/DeviceSet=&lt;deviceSet&gt;</code></li> <li>Identify the OSD you desire to remove.</li> <li>The OSD assigned to the PVC can be found in the labels on the PVC</li> <li><code>kubectl -n rook-ceph get pod -l ceph.rook.io/pvc=&lt;orphaned-pvc&gt; -o yaml | grep ceph-osd-id</code></li> <li>For example, this might return: <code>ceph-osd-id: \"0\"</code></li> <li>Remember the OSD ID for purging the OSD below</li> </ol> <p>If you later increase the count in the device set, note that the operator will create PVCs with the highest index that is not currently in use by existing OSD PVCs.</p>","title":"PVC-based cluster"},{"location":"Common%20Issues/ceph-osd-mgmt/#confirm-the-osd-is-down","text":"<p>If you want to remove an unhealthy OSD, the osd pod may be in an error state such as <code>CrashLoopBackoff</code> or the <code>ceph</code> commands in the toolbox may show which OSD is <code>down</code>. If you want to remove a healthy OSD, you should run <code>kubectl -n rook-ceph scale deployment rook-ceph-osd-&lt;ID&gt; --replicas=0</code> and <code>ceph osd down osd.&lt;ID&gt;</code> from the toolbox.</p>","title":"Confirm the OSD is down"},{"location":"Common%20Issues/ceph-osd-mgmt/#purge-the-osd-from-the-ceph-cluster","text":"<p>OSD removal can be automated with the example found in the rook-ceph-purge-osd job. In the osd-purge.yaml, change the <code>&lt;OSD-IDs&gt;</code> to the ID(s) of the OSDs you want to remove.</p> <ol> <li>Run the job: <code>kubectl create -f osd-purge.yaml</code></li> <li>When the job is completed, review the logs to ensure success: <code>kubectl -n rook-ceph logs -l app=rook-ceph-purge-osd</code></li> <li>When finished, you can delete the job: <code>kubectl delete -f osd-purge.yaml</code></li> </ol> <p>If you want to remove OSDs by hand, continue with the following sections. However, we recommend you to use the above-mentioned job to avoid operation errors.</p>","title":"Purge the OSD from the Ceph cluster"},{"location":"Common%20Issues/ceph-osd-mgmt/#purge-the-osd-manually","text":"<p>If the OSD purge job fails or you need fine-grained control of the removal, here are the individual commands that can be run from the toolbox.</p> <ol> <li>Detach the OSD PVC from Rook</li> <li><code>kubectl -n rook-ceph label pvc &lt;orphaned-pvc&gt; ceph.rook.io/DeviceSetPVCId-</code></li> <li>Mark the OSD as <code>out</code> if not already marked as such by Ceph. This signals Ceph to start moving (backfilling) the data that was on that OSD to another OSD.</li> <li><code>ceph osd out osd.&lt;ID&gt;</code> (for example if the OSD ID is 23 this would be <code>ceph osd out osd.23</code>)</li> <li>Wait for the data to finish backfilling to other OSDs.</li> <li><code>ceph status</code> will indicate the backfilling is done when all of the PGs are <code>active+clean</code>. If desired, it's safe to remove the disk after that.</li> <li>Remove the OSD from the Ceph cluster</li> <li><code>ceph osd purge &lt;ID&gt; --yes-i-really-mean-it</code></li> <li>Verify the OSD is removed from the node in the CRUSH map</li> <li><code>ceph osd tree</code></li> </ol> <p>The operator can automatically remove OSD deployments that are considered \"safe-to-destroy\" by Ceph. After the steps above, the OSD will be considered safe to remove since the data has all been moved to other OSDs. But this will only be done automatically by the operator if you have this setting in the cluster CR: <pre><code>removeOSDsIfOutAndSafeToRemove: true\n</code></pre> </p> <p>Otherwise, you will need to delete the deployment directly:    - <code>kubectl delete deployment -n rook-ceph rook-ceph-osd-&lt;ID&gt;</code></p> <p>In PVC-based cluster, remove the orphaned PVC, if necessary.</p>","title":"Purge the OSD manually"},{"location":"Common%20Issues/ceph-osd-mgmt/#delete-the-underlying-data","text":"<p>If you want to clean the device where the OSD was running, see in the instructions to wipe a disk on the Cleaning up a Cluster topic.</p>","title":"Delete the underlying data"},{"location":"Common%20Issues/ceph-osd-mgmt/#replace-an-osd","text":"<p>To replace a disk that has failed:</p> <ol> <li>Run the steps in the previous section to Remove an OSD.</li> <li>Replace the physical device and verify the new device is attached.</li> <li>Check if your cluster CR will find the new device. If you are using <code>useAllDevices: true</code> you can skip this step. If your cluster CR lists individual devices or uses a device filter you may need to update the CR.</li> <li>The operator ideally will automatically create the new OSD within a few minutes of adding the new device or updating the CR. If you don't see a new OSD automatically created, restart the operator (by deleting the operator pod) to trigger the OSD creation.</li> <li>Verify if the OSD is created on the node by running <code>ceph osd tree</code> from the toolbox.</li> </ol> <p>Note that the OSD might have a different ID than the previous OSD that was replaced.</p>","title":"Replace an OSD"},{"location":"Common%20Issues/common-issues/","text":"<p>To help troubleshoot your Rook clusters, here are some tips on what information will help solve the issues you might be seeing. If after trying the suggestions found on this page and the problem is not resolved, the Rook team is very happy to help you troubleshoot the issues in their Slack channel. Once you have registered for the Rook Slack, proceed to the General channel to ask for assistance.</p>","title":"Common Issues"},{"location":"Common%20Issues/common-issues/#ceph","text":"<p>For common issues specific to Ceph, see the Ceph Common Issues page.</p>","title":"Ceph"},{"location":"Common%20Issues/common-issues/#troubleshooting-techniques","text":"<p>Kubernetes status and logs are the the main resources needed to investigate issues in any Rook cluster.</p>","title":"Troubleshooting Techniques"},{"location":"Common%20Issues/common-issues/#kubernetes-tools","text":"<p>Kubernetes status is the first line of investigating when something goes wrong with the cluster. Here are a few artifacts that are helpful to gather:</p> <ul> <li>Rook pod status:</li> <li><code>kubectl get pod -n &lt;cluster-namespace&gt; -o wide</code><ul> <li>e.g., <code>kubectl get pod -n rook-ceph -o wide</code></li> </ul> </li> <li>Logs for Rook pods</li> <li>Logs for the operator: <code>kubectl logs -n &lt;cluster-namespace&gt; -l app=&lt;storage-backend-operator&gt;</code><ul> <li>e.g., <code>kubectl logs -n rook-ceph -l app=rook-ceph-operator</code></li> </ul> </li> <li>Logs for a specific pod: <code>kubectl logs -n &lt;cluster-namespace&gt; &lt;pod-name&gt;</code>, or a pod using a label such as mon1: <code>kubectl logs -n &lt;cluster-namespace&gt; -l &lt;label-matcher&gt;</code><ul> <li>e.g., <code>kubectl logs -n rook-ceph -l mon=a</code></li> </ul> </li> <li>Logs on a specific node to find why a PVC is failing to mount:<ul> <li>Connect to the node, then get kubelet logs (if your distro is using systemd): <code>journalctl -u kubelet</code></li> </ul> </li> <li>Pods with multiple containers<ul> <li>For all containers, in order: <code>kubectl -n &lt;cluster-namespace&gt; logs &lt;pod-name&gt; --all-containers</code></li> <li>For a single container: <code>kubectl -n &lt;cluster-namespace&gt; logs &lt;pod-name&gt; -c &lt;container-name&gt;</code></li> </ul> </li> <li>Logs for pods which are no longer running: <code>kubectl -n &lt;cluster-namespace&gt; logs --previous &lt;pod-name&gt;</code></li> </ul> <p>Some pods have specialized init containers, so you may need to look at logs for different containers within the pod.</p> <ul> <li><code>kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container-name&gt;</code></li> <li>Other Rook artifacts: <code>kubectl -n &lt;cluster-namespace&gt; get all</code></li> </ul>","title":"Kubernetes Tools"},{"location":"Contributing/development-environment/","text":"<p>You can choose any Kubernetes install of your choice. The test framework only depends on <code>kubectl</code> being configured. To install <code>kubectl</code>, please see the official guide.</p>","title":"Install Kubernetes"},{"location":"Contributing/development-environment/#minikube","text":"<p>The developers of Rook are working on Minikube and thus it is the recommended way to quickly get Rook up and running. Minikube should not be used for production but the Rook authors consider it great for development. While other tools such as k3d/kind are great, users have faced issues deploying Rook.</p> <p>Always use a virtual machine when testing Rook. Never use your host system where local devices may mistakenly be consumed.</p> <p>To install Minikube follow the official guide. It is recommended to use the kvm2 driver when running on a Linux machine and the hyperkit driver when running on a MacOS. Both allow to create and attach additional disks to the virtual machine. This is required for the Ceph OSD to consume one drive.  We don't recommend any other drivers for Rook. You will need a Minikube version 1.23 or higher.</p> <p>Starting the cluster on Minikube is as simple as running:</p> <pre><code># On Linux\nminikube start --disk-size=40g --extra-disks=1 --driver kvm2\n\n# On MacOS\nminikube start --disk-size=40g --extra-disks=1 --driver hyperkit\n</code></pre>  <p>It is recommended to install a Docker client on your host system too. Depending on your operating system follow the official guide.</p> <p>Stopping the cluster and destroying the Minikube virtual machine can be done with:</p> <pre><code>minikube delete\n</code></pre>","title":"Minikube"},{"location":"Contributing/development-environment/#install-helm","text":"<p>Use helm.sh to install Helm and set up Rook charts defined under <code>_output/charts</code> (generated by build):</p> <ul> <li>To install and set up Helm charts for Rook run <code>tests/scripts/helm.sh up</code>.</li> <li>To clean up <code>tests/scripts/helm.sh clean</code>.</li> </ul> <p>NOTE: These helper scripts depend on some artifacts under the <code>_output/</code> directory generated during build time. These scripts should be run from the project root.</p> <p>NOTE: If Helm is not available in your <code>PATH</code>, Helm will be downloaded to a temporary directory (<code>/tmp/rook-tests-scripts-helm</code>) and used from that directory.</p>","title":"Install Helm"},{"location":"Contributing/development-flow/","text":"<p>Thank you for your time and effort to help us improve Rook! Here are a few steps to get started. If you have any questions, don't hesitate to reach out to us on our Slack dev channel.</p>","title":"Contributing"},{"location":"Contributing/development-flow/#prerequisites","text":"<ol> <li>GO 1.17 or greater installed</li> <li>Git client installed</li> <li>GitHub account</li> </ol>","title":"Prerequisites"},{"location":"Contributing/development-flow/#initial-setup","text":"","title":"Initial Setup"},{"location":"Contributing/development-flow/#create-a-fork","text":"<p>From your browser navigate to http://github.com/rook/rook and click the \"Fork\" button.</p>","title":"Create a Fork"},{"location":"Contributing/development-flow/#clone-your-fork","text":"<p>Open a console window and do the following;</p> <pre><code># Create the rook repo path\nmkdir -p $GOPATH/src/github.com/rook\n\n# Navigate to the local repo path and clone your fork\ncd $GOPATH/src/github.com/rook\n\n# Clone your fork, where &lt;user&gt; is your GitHub account name\n$ git clone https://github.com/&lt;user&gt;/rook.git\ncd rook\n</code></pre>","title":"Clone Your Fork"},{"location":"Contributing/development-flow/#build","text":"<p>Building Rook-Ceph is simple.</p> <pre><code>make\n</code></pre>  <p>If you want to use <code>podman</code> instead of <code>docker</code> then uninstall <code>docker</code> packages from your machine, make will automatically pick up <code>podman</code>.</p>","title":"Build"},{"location":"Contributing/development-flow/#development-settings","text":"<p>To provide consistent whitespace and other formatting in your <code>go</code> and other source files (e.g., Markdown), it is recommended you apply the following settings in your IDE:</p> <ul> <li>Format with the <code>goreturns</code> tool</li> <li>Trim trailing whitespace</li> <li>Markdown Table of Contents is correctly updated automatically</li> </ul> <p>For example, in VS Code this translates to the following settings:</p> <pre><code>{\n    \"editor.formatOnSave\": true,\n    \"go.buildOnSave\": \"package\",\n    \"go.formatTool\": \"goreturns\",\n    \"files.trimTrailingWhitespace\": true,\n    \"files.insertFinalNewline\": true,\n    \"files.trimFinalNewlines\": true,\n    \"markdown.extension.toc.unorderedList.marker\": \"*\",\n    \"markdown.extension.toc.githubCompatibility\": true,\n    \"markdown.extension.toc.levels\": \"2..2\"\n}\n</code></pre>  <p>In addition to that it is recommended to install the following extensions:</p> <ul> <li>Markdown All in One by Yu Zhang - Visual Studio Marketplace</li> </ul>","title":"Development Settings"},{"location":"Contributing/development-flow/#add-upstream-remote","text":"<p>First you will need to add the upstream remote to your local git:</p> <pre><code># Add 'upstream' to the list of remotes\ngit remote add upstream https://github.com/rook/rook.git\n\n# Verify the remote was added\ngit remote -v\n</code></pre>  <p>Now you should have at least <code>origin</code> and <code>upstream</code> remotes. You can also add other remotes to collaborate with other contributors.</p>","title":"Add Upstream Remote"},{"location":"Contributing/development-flow/#layout","text":"<p>A source code layout is shown below, annotated with comments about the use of each important directory:</p> <pre><code>rook\n\u251c\u2500\u2500 build                         # build makefiles and logic to build, publish and release all Rook artifacts\n\u251c\u2500\u2500 cluster\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 charts                    # Helm charts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rook-ceph\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rook-ceph-cluster\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 examples                  # Sample yaml files for Rook cluster\n\u2502\n\u251c\u2500\u2500 cmd                           # Binaries with main entrypoint\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rook                      # Main command entry points for operators and daemons\n\u2502\n\u251c\u2500\u2500 design                        # Design documents for the various components of the Rook project\n\u251c\u2500\u2500 Documentation                 # Rook project Documentation\n\u251c\u2500\u2500 images                        # Dockerfiles to build images for all supported storage providers\n\u2502\n\u251c\u2500\u2500 pkg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 apis\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ceph.rook.io          # ceph specific specs for cluster, file, object\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 v1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 client                    # auto-generated strongly typed client code to access Rook APIs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 clusterd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 daemon                    # daemons for each storage provider\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ceph\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 discover\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 operator                  # all orchestration logic and custom controllers for each storage provider\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ceph\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 discover\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 k8sutil\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 util\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 version\n\u2514\u2500\u2500 tests                         # integration tests\n \u00a0\u00a0 \u251c\u2500\u2500 framework                 # the Rook testing framework\n\u00a0 \u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 clients               # test clients used to consume Rook resources during integration tests\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 installer             # installs Rook and its supported storage providers into integration tests environments\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 utils\n \u00a0\u00a0 \u251c\u2500\u2500 integration               # all test cases that will be invoked during integration testing\n \u00a0\u00a0 \u2514\u2500\u2500 scripts                   # scripts for setting up integration and manual testing environments\n</code></pre>","title":"Layout"},{"location":"Contributing/development-flow/#development","text":"<p>To add a feature or to make a bug fix, you will need to create a branch in your fork and then submit a pull request (PR) from the branch.</p>","title":"Development"},{"location":"Contributing/development-flow/#design-document","text":"<p>For new features of significant scope and complexity, a design document is recommended before work begins on the implementation. So create a design document if:</p> <ul> <li>Adding a new CRD</li> <li>Adding a significant feature to an existing storage provider. If the design is simple enough to describe in a github issue, you likely don't need a full design doc.</li> </ul> <p>For smaller, straightforward features and bug fixes, there is no need for a design document. Authoring a design document for big features has many advantages:</p> <ul> <li>Helps flesh out the approach by forcing the author to think critically about the feature and can identify potential issues early on</li> <li>Gets agreement amongst the community before code is written that could be wasted effort in the wrong direction</li> <li>Serves as an artifact of the architecture that is easier to read for visitors to the project than just the code by itself</li> </ul> <p>Note that writing code to prototype the feature while working on the design may be very useful to help flesh out the approach.</p> <p>A design document should be written as a markdown file in the design folder. You can follow the process outlined in the design template. You will see many examples of previous design documents in that folder. Submit a pull request for the design to be discussed and approved by the community before being merged into master, just like any other change to the repository.</p> <p>An issue should be opened to track the work of authoring and completing the design document. This issue is in addition to the issue that is tracking the implementation of the feature. The design label should be assigned to the issue to denote it as such.</p>","title":"Design Document"},{"location":"Contributing/development-flow/#create-a-branch","text":"<p>From a console, create a new branch based on your fork and start working on it:</p> <pre><code># Ensure all your remotes are up to date with the latest\ngit fetch --all\n\n# Create a new branch that is based off upstream master.  Give it a simple, but descriptive name.\n# Generally it will be two to three words separated by dashes and without numbers.\ngit checkout -b feature-name upstream/master\n</code></pre>  <p>Now you are ready to make the changes and commit to your branch.</p>","title":"Create a Branch"},{"location":"Contributing/development-flow/#updating-your-fork","text":"<p>During the development lifecycle, you will need to keep up-to-date with the latest upstream master. As others on the team push changes, you will need to <code>rebase</code> your commits on top of the latest. This avoids unnecessary merge commits and keeps the commit history clean.</p> <p>Whenever you need to update your local repository, you never want to merge. You always will rebase. Otherwise you will end up with merge commits in the git history. If you have any modified files, you will first have to stash them (<code>git stash save -u \"&lt;some description&gt;\"</code>).</p> <pre><code>git fetch --all\ngit rebase upstream/master\n</code></pre>  <p>Rebasing is a very powerful feature of Git. You need to understand how it works or else you will risk losing your work. Read about it in the Git documentation, it will be well worth it. In a nutshell, rebasing does the following:</p> <ul> <li>\"Unwinds\" your local commits. Your local commits are removed temporarily from the history.</li> <li>The latest changes from upstream are added to the history</li> <li>Your local commits are re-applied one by one</li> <li>If there are merge conflicts, you will be prompted to fix them before continuing. Read the output closely. It will tell you how to complete the rebase.</li> <li>When done rebasing, you will see all of your commits in the history.</li> </ul>","title":"Updating Your Fork"},{"location":"Contributing/development-flow/#submitting-a-pull-request","text":"<p>Once you have implemented the feature or bug fix in your branch, you will open a Pull Request (PR) to the upstream Rook repository. Before opening the PR ensure you have added unit tests and all unit tests are passing. Please clean your commit history and rebase on the latest upstream changes.</p> <p>See Unit Tests below for instructions on how to run unit tests.</p> <p>In order to open a pull request (PR) it is required to be up to date with the latest changes upstream. If other commits are pushed upstream before your PR is merged, you will also need to rebase again before it will be merged.</p>","title":"Submitting a Pull Request"},{"location":"Contributing/development-flow/#regression-testing","text":"<p>All pull requests must pass the unit and integration tests before they can be merged. These tests automatically run against every pull request as a part of Rook's continuous integration (CI) process. The results of these tests along with code reviews and other criteria determine whether your request will be accepted into the <code>rook/rook</code> repo.</p>","title":"Regression Testing"},{"location":"Contributing/development-flow/#unit-tests","text":"<p>From the root of your local Rook repo execute the following to run all of the unit tests:</p> <pre><code>make test\n</code></pre>  <p>Unit tests for individual packages can be run with the standard <code>go test</code> command. Before you open a PR, confirm that you have sufficient code coverage on the packages that you changed. View the <code>coverage.html</code> in a browser to inspect your new code.</p> <pre><code>go test -coverprofile=coverage.out\ngo tool cover -html=coverage.out -o coverage.html\n</code></pre>","title":"Unit Tests"},{"location":"Contributing/development-flow/#writing-unit-tests","text":"<p>There is no one-size-fits-all approach to unit testing, but we attempt to provide good tips for writing unit tests for Rook below.</p> <p>Unit tests should help people reading and reviewing the code understand the intended behavior of the code.</p> <p>Good unit tests start with easily testable code. Small chunks (\"units\") of code can be easily tested for every possible input. Higher-level code units that are built from smaller, already-tested units can more easily verify that the units are combined together correctly.</p> <p>Common cases that may need tests: * the feature is enabled * the feature is disabled * the feature is only partially enabled, for every possible way it can be partially enabled * every error that can be encountered during execution of the feature * the feature can be disabled (including partially) after it was enabled * the feature can be modified (including partially) after it was enabled * if there is a slice/array involved, test length = 0, length = 1, length = 3, length == max, length &gt; max * an input is not specified, for each input * an input is specified incorrectly, for each input * a resource the code relies on doesn't exist, for each dependency</p>","title":"Writing unit tests"},{"location":"Contributing/development-flow/#running-the-integration-tests","text":"<p>Rook's upstream continuous integration (CI) tests will run integration tests against your changes automatically.</p> <p>You do not need to run these tests locally, but you may if you like. For instructions on how to do so, follow the test instructions.</p>","title":"Running the Integration Tests"},{"location":"Contributing/development-flow/#commit-structure","text":"<p>Rook maintainers value clear, lengthy and explanatory commit messages. So by default each of your commits must:</p> <ul> <li>be prefixed by the component it's affecting, if Ceph, then the title of the commit message should be <code>ceph: my commit title</code>. If not the commit-lint bot will complain.</li> <li>contain a commit message which explains the original issue and how it was fixed if a bug. If a feature it is a full description of the new functionality.</li> <li>refer to the issue it's closing, this is mandatory when fixing a bug</li> <li>have a sign-off, this is achieved by adding <code>-s</code> when committing so in practice run <code>git commit -s</code>. If not the DCO bot will complain. If you forgot to add the sign-off you can also amend a previous commit with the sign-off by running <code>git commit --amend -s</code>. If you've pushed your changes to GitHub already you'll need to force push your branch with <code>git push -f</code>.</li> </ul> <p>Here is an example of an acceptable commit message:</p> <pre><code>component: commit title\n\nThis is the commit message, here I'm explaining, what the bug was along with its root cause.\nThen I'm explaining how I fixed it.\n\nCloses: https://github.com/rook/rook/issues/&lt;NUMBER&gt;\nSigned-off-by: First Name Last Name &lt;email address&gt;\n</code></pre>  <p>The <code>component</code> MUST be in the list checked by the CI.</p> <p>Note: sometimes you will feel like there is not so much to say, for instance if you are fixing a typo in a text. In that case, it is acceptable to shorten the commit message. Also, you don't always need to close an issue, again for a very small fix.</p> <p>You can read more about conventional commits.</p>","title":"Commit structure"},{"location":"Contributing/development-flow/#commit-history","text":"<p>To prepare your branch to open a PR, you will need to have the minimal number of logical commits so we can maintain a clean commit history. Most commonly a PR will include a single commit where all changes are squashed, although sometimes there will be multiple logical commits.</p> <pre><code># Inspect your commit history to determine if you need to squash commits\ngit log\n\n# Rebase the commits and edit, squash, or even reorder them as you determine will keep the history clean.\n# In this example, the last 5 commits will be opened in the git rebase tool.\ngit rebase -i HEAD~5\n</code></pre>  <p>Once your commit history is clean, ensure you have based on the latest upstream before you open the PR.</p>","title":"Commit History"},{"location":"Contributing/development-flow/#submitting","text":"<p>Go to the Rook github to open the PR. If you have pushed recently, you should see an obvious link to open the PR. If you have not pushed recently, go to the Pull Request tab and select your fork and branch for the PR.</p> <p>After the PR is open, you can make changes simply by pushing new commits. Your PR will track the changes in your fork and update automatically.</p> <p>Never open a pull request against a released branch (e.g. release-1.2) unless the content you are editing is gone from master and only exists in the released branch. By default, you should always open a pull request against master.</p>","title":"Submitting"},{"location":"Contributing/development-flow/#backport-a-fix-to-a-release-branch","text":"<p>The flow for getting a fix into a release branch is:</p> <ol> <li>Open a PR to merge the changes to master following the process outlined above.</li> <li>Add the backport label to that PR such as backport-release-1.7</li> <li>After your PR is merged to master, the mergify bot will automatically open a PR with your commits backported to the release branch</li> <li>If there are any conflicts you will need to resolve them by pulling the branch, resolving the conflicts and force push back the branch</li> <li>After the CI is green, the bot will automatically merge the backport PR.</li> </ol>","title":"Backport a Fix to a Release Branch"},{"location":"Contributing/storage-providers/","text":"<p>Rook is the home for operators for multiple storage providers. Each of these storage providers has specific requirements and each of them is very independent. There is no runtime dependency between the storage providers. Development is where the storage providers benefit from one another.</p> <p>Rook provides a development framework with a goal of enabling storage providers to create operators for Kubernetes to manage their storage layer. As the storage provider community grows, we expect this framework to grow as common storage constructs are identified that will benefit the community. Rook does not aim to replace other frameworks or communities, but to fill gaps not provided by other core projects.</p> <p>Storage providers in Rook are currently built on the Controller Runtime, but may also be built on other frameworks such as the Operator SDK or Kubebuilder. The choice of the underlying framework is up to the storage provider.</p> <p>Rook does not aim to be a general framework for storage, but to provide a very specific set of helpers to meet the storage provider needs in the Rook project.</p>","title":"Storage Providers"},{"location":"Contributing/storage-providers/#rook-framework","text":"<p>Rook provides the following framework to assist storage providers in building an operator:</p> <ul> <li>Common golang packages shared by storage providers are in the main Rook repo.</li> <li>Common build scripts for building the operator images are in the main Rook repo.</li> <li>Each provider has its own repo under the Rook org.</li> <li>Multiple community members are given push access to the repo, including     owners of the storage provider, Rook steering committee members,     and other Rook maintainers if deemed helpful or necessary by the steering     committee. Maintainers for the new provider are added according to the governance.</li> <li>Providers added to Rook prior to 2020 are grandfathered into the main Rook repo.</li> <li>Storage providers must follow the Rook governance   in the interest of the good of the overall project. Storage providers have   autonomy in their feature work, while collaboration with the community   is expected for shared features.</li> <li>A quarterly release cadence is in place for the operators in the main Rook repo.   Operators in their own repo define their own cadence and versioning scheme as desired.</li> <li>Storage providers own their release process, while following Rook best practices to     ensure high quality.</li> <li>Each provider owns independent CI based on GitHub actions, with patterns and build     automation that can be re-used by providers</li> <li>Docker images are pushed to the Rook DockerHub where   each storage provider has its own repo.</li> <li>Helm charts are published to charts.rook.io</li> <li>Documentation for the storage provider is to be written by the storage provider   members. The build process will publish the documentation to the Rook website.</li> <li>All storage providers are added to the Rook.io website</li> <li>A great Slack community is available where you can communicate amongst developers and users</li> </ul>","title":"Rook Framework"},{"location":"Contributing/storage-providers/#considering-joining-rook","text":"<p>If you own a storage provider and are interested in joining the Rook project to create an operator, please consider the following:</p> <ul> <li>You are making a clear commitment to the development of the storage provider.   Creating an operator is not a one-time engineering cost, but is a long term commitment   to the community.</li> <li>Support for a storage provider in Rook requires dedication and community support.</li> <li>Do you really need an operator? Many storage applications (e.g. CSI drivers)   can be deployed with tools such as a Helm chart and don't really need the   flexibility of an operator.</li> <li>Joining Rook is also about community, not just the framework.</li> </ul>","title":"Considering Joining Rook?"},{"location":"Contributing/storage-providers/#engineering-requirements","text":"<p>The engineering costs of each storage provider include:</p> <ul> <li>Develop the operator</li> <li>Rook maintainers will help answer questions along the way, but ultimately   you own the development</li> <li>If there are test failures in the CI, they should be investigated in a timely manner</li> <li>If issues are opened in GitHub, they need investigation and triage to provide   expectations about the priority and timeline</li> <li>If users have questions in Slack, they should be answered in a timely manner.   Community members can also be redirected to other locations if desired for the provider.</li> <li>A regular cadence of releases is expected. Software always needs to evolve with new versions   of K8s, accommodate new features in the storage provider, etc.</li> <li>Each provider maintains a ROADMAP.md in the root of their repo, updates it regularly   (e.g. quarterly or with the release cadence), and provides input to the overall Rook roadmap for common features.</li> </ul>","title":"Engineering Requirements"},{"location":"Contributing/storage-providers/#inactive-providers","text":"<p>If a storage provider does not have engineering resources, Rook cannot claim to support it. After some months of inactivity Rook will deprecate a storage provider. The timing will be decided on a case by case basis by the steering committee. The repo and other artifacts for deprecated storage providers will be left intact for reference.</p>","title":"Inactive Providers"},{"location":"Getting%20Started/ceph-openshift/","text":"<p>OpenShift adds a number of security and other enhancements to Kubernetes. In particular, security context constraints allow the cluster admin to define exactly which permissions are allowed to pods running in the cluster. You will need to define those permissions that allow the Rook pods to run.</p> <p>The settings for Rook in OpenShift are described below, and are also included in the example yaml files:</p> <ul> <li><code>operator-openshift.yaml</code>: Creates the security context constraints and starts the operator deployment</li> <li><code>object-openshift.yaml</code>: Creates an object store with rgw listening on a valid port number for OpenShift</li> </ul>","title":"OpenShift"},{"location":"Getting%20Started/ceph-openshift/#tldr","text":"<p>To create an OpenShift cluster, the commands basically include:</p> <pre><code>oc create -f crds.yaml -f common.yaml\noc create -f operator-openshift.yaml\noc create -f cluster.yaml\n</code></pre>","title":"TL;DR"},{"location":"Getting%20Started/ceph-openshift/#rook-privileges","text":"<p>To orchestrate the storage platform, Rook requires the following access in the cluster:</p> <ul> <li>Create <code>hostPath</code> volumes, for persistence by the Ceph mon and osd pods</li> <li>Run pods in <code>privileged</code> mode, for access to <code>/dev</code> and <code>hostPath</code> volumes</li> <li>Host networking for the Rook agent and clusters that require host networking</li> <li>Ceph OSDs require host PIDs for communication on the same node</li> </ul>","title":"Rook Privileges"},{"location":"Getting%20Started/ceph-openshift/#security-context-constraints","text":"<p>Before starting the Rook operator or cluster, create the security context constraints needed by the Rook pods. The following yaml is found in <code>operator-openshift.yaml</code> under <code>/deploy/examples</code>.</p>  <p>NOTE: Older versions of OpenShift may require <code>apiVersion: v1</code>.</p>  <p>Important to note is that if you plan on running Rook in namespaces other than the default <code>rook-ceph</code>, the example scc will need to be modified to accommodate for your namespaces where the Rook pods are running.</p> <p>To create the scc you will need a privileged account:</p> <pre><code>oc login -u system:admin\n</code></pre>  <p>We will create the security context constraints with the operator in the next section.</p>","title":"Security Context Constraints"},{"location":"Getting%20Started/ceph-openshift/#rook-settings","text":"<p>There are some Rook settings that also need to be adjusted to work in OpenShift.</p>","title":"Rook Settings"},{"location":"Getting%20Started/ceph-openshift/#operator-settings","text":"<p>There is an environment variable that needs to be set in the operator spec that will allow Rook to run in OpenShift clusters.</p> <ul> <li><code>ROOK_HOSTPATH_REQUIRES_PRIVILEGED</code>: Must be set to <code>true</code>. Writing to the hostPath is required for the Ceph mon and osd pods. Given the restricted permissions in OpenShift with SELinux, the pod must be running privileged in order to write to the hostPath volume.</li> </ul> <pre><code>- name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED\n  value: \"true\"\n</code></pre>  <p>Now create the security context constraints and the operator:</p> <pre><code>oc create -f operator-openshift.yaml\n</code></pre>","title":"Operator Settings"},{"location":"Getting%20Started/ceph-openshift/#cluster-settings","text":"<p>The cluster settings in <code>cluster.yaml</code> are largely isolated from the differences in OpenShift. There is perhaps just one to take note of:</p> <ul> <li><code>dataDirHostPath</code>: Ensure that it points to a valid, writable path on the host systems.</li> </ul>","title":"Cluster Settings"},{"location":"Getting%20Started/ceph-openshift/#object-store-settings","text":"<p>In OpenShift, ports less than 1024 cannot be bound. In the object store CRD, ensure the port is modified to meet this requirement.</p> <pre><code>gateway:\n  port: 8080\n</code></pre>  <p>You can expose a different port such as <code>80</code> by creating a service.</p> <p>A sample object store can be created with these settings:</p> <pre><code>oc create -f object-openshift.yaml\n</code></pre>","title":"Object Store Settings"},{"location":"Getting%20Started/ceph-storage/","text":"<p>Ceph is a highly scalable distributed storage solution for block storage, object storage, and shared filesystems with years of production deployments.</p>","title":"Ceph Storage"},{"location":"Getting%20Started/ceph-storage/#design","text":"<p>Rook enables Ceph storage to run on Kubernetes using Kubernetes primitives. With Ceph running in the Kubernetes cluster, Kubernetes applications can mount block devices and filesystems managed by Rook, or can use the S3/Swift API for object storage. The Rook operator automates configuration of storage components and monitors the cluster to ensure the storage remains available and healthy.</p> <p>The Rook operator is a simple container that has all that is needed to bootstrap and monitor the storage cluster. The operator will start and monitor Ceph monitor pods, the Ceph OSD daemons to provide RADOS storage, as well as start and manage other Ceph daemons. The operator manages CRDs for pools, object stores (S3/Swift), and filesystems by initializing the pods and other resources necessary to run the services.</p> <p>The operator will monitor the storage daemons to ensure the cluster is healthy. Ceph mons will be started or failed over when necessary, and other adjustments are made as the cluster grows or shrinks.  The operator will also watch for desired state changes specified in the Ceph custom resources (CRs) and apply the changes.</p> <p>Rook automatically configures the Ceph-CSI driver to mount the storage to your pods.</p> <p></p> <p>The <code>rook/ceph</code> image includes all necessary tools to manage the cluster. Rook is not in the Ceph data path. Many of the Ceph concepts like placement groups and crush maps are hidden so you don't have to worry about them. Instead Rook creates a simplified user experience for admins that is in terms of physical resources, pools, volumes, filesystems, and buckets. At the same time, advanced configuration can be applied when needed with the Ceph tools.</p> <p>Rook is implemented in golang. Ceph is implemented in C++ where the data path is highly optimized. We believe this combination offers the best of both worlds.</p>","title":"Design"},{"location":"Getting%20Started/quickstart/","text":"<p>Welcome to Rook! We hope you have a great experience installing the Rook cloud-native storage orchestrator platform to enable highly available, durable Ceph storage in your Kubernetes cluster.</p> <p>If you have any questions along the way, please don't hesitate to ask us in our Slack channel. You can sign up for our Slack here.</p> <p>This guide will walk you through the basic setup of a Ceph cluster and enable you to consume block, object, and file storage from other pods running in your cluster.</p> <p>Always use a virtual machine when testing Rook. Never use your host system where local devices may mistakenly be consumed.</p>","title":"Ceph Quickstart"},{"location":"Getting%20Started/quickstart/#minimum-version","text":"<p>Kubernetes v1.17 or higher is supported by Rook.</p>","title":"Minimum Version"},{"location":"Getting%20Started/quickstart/#prerequisites","text":"<p>To make sure you have a Kubernetes cluster that is ready for <code>Rook</code>, you can follow these instructions.</p> <p>In order to configure the Ceph storage cluster, at least one of these local storage options are required: - Raw devices (no partitions or formatted filesystems)   - This requires <code>lvm2</code> to be installed on the host.     To avoid this dependency, you can create a single full-disk partition on the disk (see below) - Raw partitions (no formatted filesystem) - Persistent Volumes available from a storage class in <code>block</code> mode</p>","title":"Prerequisites"},{"location":"Getting%20Started/quickstart/#tldr","text":"<p>A simple Rook cluster can be created with the following kubectl commands and example manifests.</p> <pre><code>$ git clone --single-branch --branch {{ branchName }} https://github.com/rook/rook.git\ncd rook/deploy/examples\nkubectl create -f crds.yaml -f common.yaml -f operator.yaml\nkubectl create -f cluster.yaml\n</code></pre>  <p>After the cluster is running, you can create block, object, or file storage to be consumed by other applications in your cluster.</p>","title":"TL;DR"},{"location":"Getting%20Started/quickstart/#deploy-the-rook-operator","text":"<p>The first step is to deploy the Rook operator. Check that you are using the example yaml files that correspond to your release of Rook. For more options, see the examples documentation.</p> <pre><code>cd deploy/examples\nkubectl create -f crds.yaml -f common.yaml -f operator.yaml\n\n# verify the rook-ceph-operator is in the `Running` state before proceeding\nkubectl -n rook-ceph get pod\n</code></pre>  <p>You can also deploy the operator with the Rook Helm Chart.</p> <p>Before you start the operator in production, there are some settings that you may want to consider: 1. Consider if you want to enable certain Rook features that are disabled by default. See the operator.yaml for these and other advanced settings.    1. Device discovery: Rook will watch for new devices to configure if the <code>ROOK_ENABLE_DISCOVERY_DAEMON</code> setting is enabled, commonly used in bare metal clusters.    2. Node affinity and tolerations: The CSI driver by default will run on any node in the cluster. To configure the CSI driver affinity, several settings are available.</p> <p>If you wish to deploy into a namespace other than the default <code>rook-ceph</code>, see the Ceph advanced configuration section on the topic.</p>","title":"Deploy the Rook Operator"},{"location":"Getting%20Started/quickstart/#cluster-environments","text":"<p>The Rook documentation is focused around starting Rook in a production environment. Examples are also provided to relax some settings for test environments. When creating the cluster later in this guide, consider these example cluster manifests:</p> <ul> <li>cluster.yaml: Cluster settings for a production cluster running on bare metal. Requires at least three worker nodes.</li> <li>cluster-on-pvc.yaml: Cluster settings for a production cluster running in a dynamic cloud environment.</li> <li>cluster-test.yaml: Cluster settings for a test environment such as minikube.</li> </ul> <p>See the Ceph examples for more details.</p>","title":"Cluster Environments"},{"location":"Getting%20Started/quickstart/#create-a-ceph-cluster","text":"<p>Now that the Rook operator is running we can create the Ceph cluster. For the cluster to survive reboots, make sure you set the <code>dataDirHostPath</code> property that is valid for your hosts. For more settings, see the documentation on configuring the cluster.</p> <p>Create the cluster:</p> <pre><code>kubectl create -f cluster.yaml\n</code></pre>  <p>Use <code>kubectl</code> to list pods in the <code>rook-ceph</code> namespace. You should be able to see the following pods once they are all running. The number of osd pods will depend on the number of nodes in the cluster and the number of devices configured. If you did not modify the <code>cluster.yaml</code> above, it is expected that one OSD will be created per node.</p>  <p>If the <code>rook-ceph-mon</code>, <code>rook-ceph-mgr</code>, or <code>rook-ceph-osd</code> pods are not created, please refer to the Ceph common issues for more details and potential solutions.</p>  <pre><code>kubectl -n rook-ceph get pod\n</code></pre>   <pre><code>NAME                                                 READY   STATUS      RESTARTS   AGE\ncsi-cephfsplugin-provisioner-d77bb49c6-n5tgs         5/5     Running     0          140s\ncsi-cephfsplugin-provisioner-d77bb49c6-v9rvn         5/5     Running     0          140s\ncsi-cephfsplugin-rthrp                               3/3     Running     0          140s\ncsi-rbdplugin-hbsm7                                  3/3     Running     0          140s\ncsi-rbdplugin-provisioner-5b5cd64fd-nvk6c            6/6     Running     0          140s\ncsi-rbdplugin-provisioner-5b5cd64fd-q7bxl            6/6     Running     0          140s\nrook-ceph-crashcollector-minikube-5b57b7c5d4-hfldl   1/1     Running     0          105s\nrook-ceph-mgr-a-64cd7cdf54-j8b5p                     1/1     Running     0          77s\nrook-ceph-mon-a-694bb7987d-fp9w7                     1/1     Running     0          105s\nrook-ceph-mon-b-856fdd5cb9-5h2qk                     1/1     Running     0          94s\nrook-ceph-mon-c-57545897fc-j576h                     1/1     Running     0          85s\nrook-ceph-operator-85f5b946bd-s8grz                  1/1     Running     0          92m\nrook-ceph-osd-0-6bb747b6c5-lnvb6                     1/1     Running     0          23s\nrook-ceph-osd-1-7f67f9646d-44p7v                     1/1     Running     0          24s\nrook-ceph-osd-2-6cd4b776ff-v4d68                     1/1     Running     0          25s\nrook-ceph-osd-prepare-node1-vx2rz                    0/2     Completed   0          60s\nrook-ceph-osd-prepare-node2-ab3fd                    0/2     Completed   0          60s\nrook-ceph-osd-prepare-node3-w4xyz                    0/2     Completed   0          60s\n</code></pre>   <p>To verify that the cluster is in a healthy state, connect to the Rook toolbox and run the <code>ceph status</code> command.</p> <ul> <li>All mons should be in quorum</li> <li>A mgr should be active</li> <li>At least one OSD should be active</li> <li>If the health is not <code>HEALTH_OK</code>, the warnings or errors should be investigated</li> </ul> <pre><code>ceph status\n</code></pre>   <pre><code>  cluster:\n    id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 3m)\n    mgr: a(active, since 2m)\n    osd: 3 osds: 3 up (since 1m), 3 in (since 1m)\n...\n</code></pre>   <p>If the cluster is not healthy, please refer to the Ceph common issues for more details and potential solutions.</p>","title":"Create a Ceph Cluster"},{"location":"Getting%20Started/quickstart/#storage","text":"<p>For a walkthrough of the three types of storage exposed by Rook, see the guides for:</p> <ul> <li>Block: Create block storage to be consumed by a pod (RWO)</li> <li>Shared Filesystem: Create a filesystem to be shared across multiple pods (RWX)</li> <li>Object: Create an object store that is accessible inside or outside the Kubernetes cluster</li> </ul>","title":"Storage"},{"location":"Getting%20Started/quickstart/#ceph-dashboard","text":"<p>Ceph has a dashboard in which you can view the status of your cluster. Please see the dashboard guide for more details.</p>","title":"Ceph Dashboard"},{"location":"Getting%20Started/quickstart/#tools","text":"<p>Create a toolbox pod for full access to a ceph admin client for debugging and troubleshooting your Rook cluster.  Please see the toolbox documentation for setup and usage information. Also see our advanced configuration document for helpful maintenance and tuning examples.</p>","title":"Tools"},{"location":"Getting%20Started/quickstart/#monitoring","text":"<p>Each Rook cluster has some built in metrics collectors/exporters for monitoring with Prometheus. To learn how to set up monitoring for your Rook cluster, you can follow the steps in the monitoring guide.</p>","title":"Monitoring"},{"location":"Getting%20Started/quickstart/#teardown","text":"<p>When you are done with the test cluster, see these instructions to clean up the cluster.</p>","title":"Teardown"},{"location":"Helm%20Charts/helm-ceph-cluster/","text":"<p>Creates Rook resources to configure a Ceph cluster using the Helm package manager. This chart is a simple packaging of templates that will optionally create Rook resources such as: - CephCluster, CephFilesystem, and CephObjectStore CRs - Storage classes to expose Ceph RBD volumes, CephFS volumes, and RGW buckets - Ingress for external access to the dashboard - Toolbox</p>","title":"Ceph Cluster Helm Chart"},{"location":"Helm%20Charts/helm-ceph-cluster/#prerequisites","text":"<ul> <li>Kubernetes 1.17+</li> <li>Helm 3.x</li> <li>Install the Rook Operator chart</li> </ul>","title":"Prerequisites"},{"location":"Helm%20Charts/helm-ceph-cluster/#installing","text":"<p>The <code>helm install</code> command deploys rook on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. It is recommended that the rook operator be installed into the <code>rook-ceph</code> namespace. The clusters can be installed into the same namespace as the operator or a separate namespace.</p> <p>Rook currently publishes builds of this chart to the <code>release</code> and <code>master</code> channels.</p> <p>Before installing, review the values.yaml to confirm if the default settings need to be updated. * If the operator was installed in a namespace other than <code>rook-ceph</code>, the namespace   must be set in the <code>operatorNamespace</code> variable. * Set the desired settings in the <code>cephClusterSpec</code>. The defaults   are only an example and not likely to apply to your cluster. * The <code>monitoring</code> section should be removed from the <code>cephClusterSpec</code>, as it is specified separately in the helm settings. * The default values for <code>cephBlockPools</code>, <code>cephFileSystems</code>, and <code>CephObjectStores</code> will create one of each, and their corresponding storage classes. * All Ceph components now have default values for the pod resources. The resources may need to be adjusted in production clusters depending on the load. The resources can also be disabled if Ceph should not be limited (e.g. test clusters).</p>","title":"Installing"},{"location":"Helm%20Charts/helm-ceph-cluster/#release","text":"<p>The release channel is the most recent release of Rook that is considered stable for the community.</p> <p>The example install assumes you have first installed the Rook Operator chart and created your customized values-override.yaml.</p> <pre><code>helm repo add rook-release https://charts.rook.io/release\nhelm install --create-namespace --namespace rook-ceph rook-ceph-cluster \\\n   --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster -f values-override.yaml\n</code></pre>","title":"Release"},{"location":"Helm%20Charts/helm-ceph-cluster/#configuration","text":"<p>The following tables lists the configurable parameters of the rook-operator chart and their default values.</p>    Parameter Description Default     <code>operatorNamespace</code> Namespace of the Rook Operator <code>rook-ceph</code>   <code>kubeVersion</code> Optional override of the target kubernetes version ``   <code>configOverride</code> Cluster ceph.conf override    <code>toolbox.enabled</code> Enable Ceph debugging pod deployment. See toolbox <code>false</code>   <code>toolbox.tolerations</code> Toolbox tolerations <code>[]</code>   <code>toolbox.affinity</code> Toolbox affinity <code>{}</code>   <code>toolbox.resources</code> Toolbox resources see values.yaml   <code>monitoring.enabled</code> Enable Prometheus integration, will also create necessary RBAC rules <code>false</code>   <code>monitoring.createPrometheusRules</code> Whether to create the Prometheus rules for Ceph alerts <code>false</code>   <code>cephClusterSpec.*</code> Cluster configuration, see below See below   <code>ingress.dashboard</code> Enable an ingress for the ceph-dashboard <code>{}</code>   <code>cephBlockPools.[*]</code> A list of CephBlockPool configurations to deploy See below   <code>cephFileSystems.[*]</code> A list of CephFileSystem configurations to deploy See below   <code>cephObjectStores.[*]</code> A list of CephObjectStore configurations to deploy See below","title":"Configuration"},{"location":"Helm%20Charts/helm-ceph-cluster/#ceph-cluster-spec","text":"<p>The <code>CephCluster</code> CRD takes its spec from <code>cephClusterSpec.*</code>. This is not an exhaustive list of parameters. For the full list, see the Cluster CRD topic.</p>","title":"Ceph Cluster Spec"},{"location":"Helm%20Charts/helm-ceph-cluster/#ceph-block-pools","text":"<p>The <code>cephBlockPools</code> array in the values file will define a list of CephBlockPool as described in the table below.</p>    Parameter Description Default     <code>name</code> The name of the CephBlockPool <code>ceph-blockpool</code>   <code>spec</code> The CephBlockPool spec, see the CephBlockPool documentation. <code>{}</code>   <code>storageClass.enabled</code> Whether a storage class is deployed alongside the CephBlockPool <code>true</code>   <code>storageClass.isDefault</code> Whether the storage class will be the default storage class for PVCs. See the PersistentVolumeClaim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) documentation for details. <code>true</code>   <code>storageClass.name</code> The name of the storage class <code>ceph-block</code>   <code>storageClass.parameters</code> See Block Storage documentation or the helm values.yaml for suitable values see values.yaml   <code>storageClass.reclaimPolicy</code> The default Reclaim Policy to apply to PVCs created with this storage class. <code>Delete</code>   <code>storageClass.allowVolumeExpansion</code> Whether volume expansion is allowed by default. <code>true</code>   <code>storageClass.mountOptions</code> Specifies the mount options for storageClass <code>[]</code>","title":"Ceph Block Pools"},{"location":"Helm%20Charts/helm-ceph-cluster/#ceph-file-systems","text":"<p>The <code>cephFileSystems</code> array in the values file will define a list of CephFileSystem as described in the table below.</p>    Parameter Description Default     <code>name</code> The name of the CephFileSystem <code>ceph-filesystem</code>   <code>spec</code> The CephFileSystem spec, see the CephFilesystem CRD documentation. see values.yaml   <code>storageClass.enabled</code> Whether a storage class is deployed alongside the CephFileSystem <code>true</code>   <code>storageClass.name</code> The name of the storage class <code>ceph-filesystem</code>   <code>storageClass.pool</code> The name of Data Pool, without the filesystem name prefix <code>data0</code>   <code>storageClass.parameters</code> See Shared Filesystem documentation or the helm values.yaml for suitable values see values.yaml   <code>storageClass.reclaimPolicy</code> The default Reclaim Policy to apply to PVCs created with this storage class. <code>Delete</code>   <code>storageClass.mountOptions</code> Specifies the mount options for storageClass <code>[]</code>","title":"Ceph File Systems"},{"location":"Helm%20Charts/helm-ceph-cluster/#ceph-object-stores","text":"<p>The <code>cephObjectStores</code> array in the values file will define a list of CephObjectStore as described in the table below.</p>    Parameter Description Default     <code>name</code> The name of the CephObjectStore <code>ceph-objectstore</code>   <code>spec</code> The CephObjectStore spec, see the CephObjectStore CRD documentation. see values.yaml   <code>storageClass.enabled</code> Whether a storage class is deployed alongside the CephObjectStore <code>true</code>   <code>storageClass.name</code> The name of the storage class <code>ceph-bucket</code>   <code>storageClass.parameters</code> See Object Store storage class documentation or the helm values.yaml for suitable values see values.yaml   <code>storageClass.reclaimPolicy</code> The default Reclaim Policy to apply to PVCs created with this storage class. <code>Delete</code>","title":"Ceph Object Stores"},{"location":"Helm%20Charts/helm-ceph-cluster/#existing-clusters","text":"<p>If you have an existing CephCluster CR that was created without the helm chart and you want the helm chart to start managing the cluster:</p> <ol> <li> <p>Extract the <code>spec</code> section of your existing CephCluster CR and copy to the <code>cephClusterSpec</code>    section in <code>values-override.yaml</code>.</p> </li> <li> <p>Add the following annotations and label to your existing CephCluster CR:</p> </li> </ol> <pre><code>  annotations:\n    meta.helm.sh/release-name: rook-ceph-cluster\n    meta.helm.sh/release-namespace: rook-ceph\n  labels:\n    app.kubernetes.io/managed-by: Helm\n</code></pre>  <ol> <li> <p>Run the <code>helm install</code> command in the Installing section to create the chart.</p> </li> <li> <p>In the future when updates to the cluster are needed, ensure the values-override.yaml always    contains the desired CephCluster spec.</p> </li> </ol>","title":"Existing Clusters"},{"location":"Helm%20Charts/helm-ceph-cluster/#development-build","text":"<p>To deploy from a local build from your development environment:</p> <pre><code>cd deploy/charts/rook-ceph-cluster\nhelm install --create-namespace --namespace rook-ceph rook-ceph-cluster -f values-override.yaml .\n</code></pre>","title":"Development Build"},{"location":"Helm%20Charts/helm-ceph-cluster/#uninstalling-the-chart","text":"<p>To see the currently installed Rook chart:</p> <pre><code>helm ls --namespace rook-ceph\n</code></pre>  <p>To uninstall/delete the <code>rook-ceph-cluster</code> chart:</p> <pre><code>helm delete --namespace rook-ceph rook-ceph-cluster\n</code></pre>  <p>The command removes all the Kubernetes components associated with the chart and deletes the release. Removing the cluster chart does not remove the Rook operator. In addition, all data on hosts in the Rook data directory (<code>/var/lib/rook</code> by default) and on OSD raw devices is kept. To reuse disks, you will have to wipe them before recreating the cluster.</p> <p>See the teardown documentation for more information.</p>","title":"Uninstalling the Chart"},{"location":"Helm%20Charts/helm-operator/","text":"<p>Installs rook to create, configure, and manage Ceph clusters on Kubernetes.</p>","title":"Ceph Operator Helm Chart"},{"location":"Helm%20Charts/helm-operator/#introduction","text":"<p>This chart bootstraps a rook-ceph-operator deployment on a Kubernetes cluster using the Helm package manager.</p>","title":"Introduction"},{"location":"Helm%20Charts/helm-operator/#prerequisites","text":"<ul> <li>Kubernetes 1.17+</li> <li>Helm 3.x</li> </ul> <p>See the Helm support matrix for more details.</p>","title":"Prerequisites"},{"location":"Helm%20Charts/helm-operator/#installing","text":"<p>The Ceph Operator helm chart will install the basic components necessary to create a storage platform for your Kubernetes cluster. 1. Install the Helm chart 1. Create a Rook cluster.</p> <p>The <code>helm install</code> command deploys rook on the Kubernetes cluster in the default configuration. The configuration section lists the parameters that can be configured during installation. It is recommended that the rook operator be installed into the <code>rook-ceph</code> namespace (you will install your clusters into separate namespaces).</p> <p>Rook currently publishes builds of the Ceph operator to the <code>release</code> and <code>master</code> channels.</p>","title":"Installing"},{"location":"Helm%20Charts/helm-operator/#release","text":"<p>The release channel is the most recent release of Rook that is considered stable for the community.</p> <pre><code>helm repo add rook-release https://charts.rook.io/release\nhelm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f values.yaml\n</code></pre>  <p>For example settings, see the next section or values.yaml</p>","title":"Release"},{"location":"Helm%20Charts/helm-operator/#configuration","text":"<p>The following tables lists the configurable parameters of the rook-operator chart and their default values.</p>    Parameter Description Default     <code>image.repository</code> Image <code>rook/ceph</code>   <code>image.tag</code> Image tag <code>master</code>   <code>image.pullPolicy</code> Image pull policy <code>IfNotPresent</code>   <code>crds.enabled</code> If true, the helm chart will create the Rook CRDs. Do NOT change to <code>false</code> in a running cluster or CRs will be deleted! <code>true</code>   <code>rbacEnable</code> If true, create &amp; use RBAC resources <code>true</code>   <code>pspEnable</code> If true, create &amp; use PSP resources <code>true</code>   <code>resources</code> Pod resource requests &amp; limits <code>{}</code>   <code>annotations</code> Pod annotations <code>{}</code>   <code>logLevel</code> Global log level <code>INFO</code>   <code>nodeSelector</code> Kubernetes <code>nodeSelector</code> to add to the Deployment.    <code>tolerations</code> List of Kubernetes <code>tolerations</code> to add to the Deployment. <code>[]</code>   <code>unreachableNodeTolerationSeconds</code> Delay to use for the node.kubernetes.io/unreachable pod failure toleration to override the Kubernetes default of 5 minutes <code>5s</code>   <code>currentNamespaceOnly</code> Whether the operator should watch cluster CRD in its own namespace or not <code>false</code>   <code>hostpathRequiresPrivileged</code> Runs Ceph Pods as privileged to be able to write to <code>hostPath</code>s in OpenShift with SELinux restrictions. <code>false</code>   <code>discover.priorityClassName</code> The priority class name to add to the discover pods    <code>discover.toleration</code> Toleration for the discover pods    <code>discover.tolerationKey</code> The specific key of the taint to tolerate    <code>discover.tolerations</code> Array of tolerations in YAML format which will be added to discover deployment    <code>discover.nodeAffinity</code> The node labels for affinity of <code>discover-agent</code> (***)    <code>discover.podLabels</code> Labels to add to the discover pods.    <code>csi.enableRbdDriver</code> Enable Ceph CSI RBD driver. <code>true</code>   <code>csi.enableCephfsDriver</code> Enable Ceph CSI CephFS driver. <code>true</code>   <code>csi.enableCephfsSnapshotter</code> Enable Snapshotter in CephFS provisioner pod. <code>true</code>   <code>csi.enableRBDSnapshotter</code> Enable Snapshotter in RBD provisioner pod. <code>true</code>   <code>csi.pluginPriorityClassName</code> PriorityClassName to be set on csi driver plugin pods.    <code>csi.provisionerPriorityClassName</code> PriorityClassName to be set on csi driver provisioner pods.    <code>csi.enableOMAPGenerator</code> EnableOMAP generator deploys omap sidecar in CSI provisioner pod, to enable it set it to true <code>false</code>   <code>csi.rbdFSGroupPolicy</code> Policy for modifying a volume's ownership or permissions when the RBD PVC is being mounted ReadWriteOnceWithFSType   <code>csi.cephFSFSGroupPolicy</code> Policy for modifying a volume's ownership or permissions when the CephFS PVC is being mounted ReadWriteOnceWithFSType   <code>csi.nfsFSGroupPolicy</code> Policy for modifying a volume's ownership or permissions when the NFS PVC is being mounted ReadWriteOnceWithFSType   <code>csi.logLevel</code> Set logging level for csi containers. Supported values from 0 to 5. 0 for general useful logs, 5 for trace level verbosity. <code>0</code>   <code>csi.grpcTimeoutInSeconds</code> Set GRPC timeout for csi containers. <code>150</code>   <code>csi.provisionerReplicas</code> Set replicas for csi provisioner deployment. <code>2</code>   <code>csi.enableGrpcMetrics</code> Enable Ceph CSI GRPC Metrics. <code>false</code>   <code>csi.enableCSIHostNetwork</code> Enable Host Networking for Ceph CSI nodeplugins. <code>false</code>   <code>csi.enablePluginSelinuxHostMount</code> Enable Host mount for /etc/selinux directory for Ceph CSI nodeplugins. <code>false</code>   <code>csi.enableCSIEncryption</code> Enable Ceph CSI PVC encryption support. <code>false</code>   <code>csi.provisionerTolerations</code> Array of tolerations in YAML format which will be added to CSI provisioner deployment.    <code>csi.provisionerNodeAffinity</code> The node labels for affinity of the CSI provisioner deployment (***)    <code>csi.pluginTolerations</code> Array of tolerations in YAML format which will be added to CephCSI plugin DaemonSet    <code>csi.pluginNodeAffinity</code> The node labels for affinity of the CephCSI plugin DaemonSet (***)    <code>csi.rbdProvisionerTolerations</code> Array of tolerations in YAML format which will be added to CephCSI RBD provisioner deployment.    <code>csi.rbdProvisionerNodeAffinity</code> The node labels for affinity of the CephCSI RBD provisioner deployment (***)    <code>csi.rbdPluginTolerations</code> Array of tolerations in YAML format which will be added to CephCSI RBD plugin DaemonSet    <code>csi.rbdPluginNodeAffinity</code> The node labels for affinity of the CephCSI RBD plugin DaemonSet (***)    <code>csi.cephFSProvisionerTolerations</code> Array of tolerations in YAML format which will be added to CephCSI CephFS provisioner deployment.    <code>csi.cephFSProvisionerNodeAffinity</code> The node labels for affinity of the CephCSI CephFS provisioner deployment (***)    <code>csi.cephFSPluginTolerations</code> Array of tolerations in YAML format which will be added to CephCSI CephFS plugin DaemonSet    <code>csi.cephFSPluginNodeAffinity</code> The node labels for affinity of the CephCSI CephFS plugin DaemonSet (***)    <code>csi.nfsProvisionerTolerations</code> Array of tolerations in YAML format which will be added to CephCSI NFS provisioner deployment.    <code>csi.nfsProvisionerNodeAffinity</code> The node labels for affinity of the CephCSI NFS provisioner deployment (***)    <code>csi.nfsPluginTolerations</code> Array of tolerations in YAML format which will be added to CephCSI NFS plugin DaemonSet    <code>csi.nfsPluginNodeAffinity</code> The node labels for affinity of the CephCSI NFS plugin DaemonSet (***)    <code>csi.csiRBDProvisionerResource</code> CEPH CSI RBD provisioner resource requirement list.    <code>csi.csiRBDPluginResource</code> CEPH CSI RBD plugin resource requirement list.    <code>csi.csiCephFSProvisionerResource</code> CEPH CSI CephFS provisioner resource requirement list.    <code>csi.csiCephFSPluginResource</code> CEPH CSI CephFS plugin resource requirement list.    <code>csi.csiNFSProvisionerResource</code> CEPH CSI NFS provisioner resource requirement list.    <code>csi.csiNFSPluginResource</code> CEPH CSI NFS plugin resource requirement list.    <code>csi.cephfsGrpcMetricsPort</code> CSI CephFS driver GRPC metrics port. <code>9091</code>   <code>csi.cephfsLivenessMetricsPort</code> CSI CephFS driver metrics port. <code>9081</code>   <code>csi.rbdGrpcMetricsPort</code> Ceph CSI RBD driver GRPC metrics port. <code>9090</code>   <code>csi.csiAddonsPort</code> CSI Addons server port. <code>9070</code>   <code>csi.rbdLivenessMetricsPort</code> Ceph CSI RBD driver metrics port. <code>8080</code>   <code>csi.forceCephFSKernelClient</code> Enable Ceph Kernel clients on kernel &lt; 4.17 which support quotas for Cephfs. <code>true</code>   <code>csi.kubeletDirPath</code> Kubelet root directory path (if the Kubelet uses a different path for the <code>--root-dir</code> flag) <code>/var/lib/kubelet</code>   <code>csi.cephcsi.image</code> Ceph CSI image. <code>quay.io/cephcsi/cephcsi:v3.6.1</code>   <code>csi.rbdPluginUpdateStrategy</code> CSI Rbd plugin daemonset update strategy, supported values are OnDelete and RollingUpdate. <code>RollingUpdate</code>   <code>csi.cephFSPluginUpdateStrategy</code> CSI CephFS plugin daemonset update strategy, supported values are OnDelete and RollingUpdate. <code>RollingUpdate</code>   <code>csi.nfsPluginUpdateStrategy</code> CSI NFS plugin daemonset update strategy, supported values are OnDelete and RollingUpdate. <code>RollingUpdate</code>   <code>csi.registrar.image</code> Kubernetes CSI registrar image. <code>k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0</code>   <code>csi.resizer.image</code> Kubernetes CSI resizer image. <code>k8s.gcr.io/sig-storage/csi-resizer:v1.4.0</code>   <code>csi.provisioner.image</code> Kubernetes CSI provisioner image. <code>k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0</code>   <code>csi.snapshotter.image</code> Kubernetes CSI snapshotter image. <code>k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1</code>   <code>csi.attacher.image</code> Kubernetes CSI Attacher image. <code>k8s.gcr.io/sig-storage/csi-attacher:v3.4.0</code>   <code>csi.cephfsPodLabels</code> Labels to add to the CSI CephFS Pods.    <code>csi.rbdPodLabels</code> Labels to add to the CSI RBD Pods.    <code>csi.volumeReplication.enabled</code> Enable Volume Replication. <code>false</code>   <code>csi.volumeReplication.image</code> Volume Replication Controller image. <code>quay.io/csiaddons/volumereplication-operator:v0.3.0</code>   <code>csi.csiAddons.enabled</code> Enable CSIAddons <code>false</code>   <code>csi.csiAddons.image</code> CSIAddons Sidecar image. <code>quay.io/csiaddons/k8s-sidecar:v0.2.1</code>   <code>csi.nfs.enabled</code> Enable nfs driver. <code>false</code>   <code>csi.nfs.image</code> NFS nodeplugin image. <code>k8s.gcr.io/sig-storage/nfsplugin:v3.1.0</code>   <code>admissionController.tolerations</code> Array of tolerations in YAML format which will be added to admission controller deployment.    <code>admissionController.nodeAffinity</code> The node labels for affinity of the admission controller deployment (***)    <code>monitoring.enabled</code> Create necessary RBAC rules for Rook to integrate with Prometheus monitoring in the operator namespace. Requires Prometheus to be pre-installed. <code>false</code>    <p>* * * <code>nodeAffinity</code> and <code>*NodeAffinity</code> options should have the format <code>\"role=storage,rook; storage=ceph\"</code> or <code>storage=;role=rook-example</code> or <code>storage=;</code> (checks only for presence of key)</p>","title":"Configuration"},{"location":"Helm%20Charts/helm-operator/#development-build","text":"<p>To deploy from a local build from your development environment:</p> <ol> <li>Build the Rook docker image: <code>make</code></li> <li>Copy the image to your K8s cluster, such as with the <code>docker save</code> then the <code>docker load</code> commands</li> <li>Install the helm chart:</li> </ol> <pre><code>cd deploy/charts/rook-ceph\nhelm install --create-namespace --namespace rook-ceph rook-ceph .\n</code></pre>","title":"Development Build"},{"location":"Helm%20Charts/helm-operator/#uninstalling-the-chart","text":"<p>To see the currently installed Rook chart:</p> <pre><code>helm ls --namespace rook-ceph\n</code></pre>  <p>To uninstall/delete the <code>rook-ceph</code> deployment:</p> <pre><code>helm delete --namespace rook-ceph rook-ceph\n</code></pre>  <p>The command removes all the Kubernetes components associated with the chart and deletes the release.</p> <p>After uninstalling you may want to clean up the CRDs as described on the teardown documentation.</p>","title":"Uninstalling the Chart"},{"location":"Helm%20Charts/helm/","text":"<p>Rook has published the following Helm charts for the Ceph storage provider:</p> <ul> <li>Rook Ceph Operator: Starts the Ceph Operator, which will watch for Ceph CRs (custom resources)</li> <li>Rook Ceph Cluster: Creates Ceph CRs that the operator will use to configure the cluster</li> </ul> <p>The Helm charts are intended to simplify deployment and upgrades. Configuring the Rook resources without Helm is also fully supported by creating the manifests directly.</p>","title":"Helm Charts"},{"location":"Prerequisites/authenticated-registry/","text":"","title":"Authenticated Registries"},{"location":"Prerequisites/authenticated-registry/#authenticated-docker-registries","text":"<p>If you want to use an image from authenticated docker registry (e.g. for image cache/mirror), you'll need to add an <code>imagePullSecret</code> to all relevant service accounts. This way all pods created by the operator (for service account: <code>rook-ceph-system</code>) or all new pods in the namespace (for service account: <code>default</code>) will have the <code>imagePullSecret</code> added to their spec.</p> <p>The whole process is described in the official kubernetes documentation.</p>","title":"Authenticated docker registries"},{"location":"Prerequisites/authenticated-registry/#example-setup-for-a-ceph-cluster","text":"<p>To get you started, here's a quick rundown for the ceph example from the quickstart guide.</p> <p>First, we'll create the secret for our registry as described here:</p> <pre><code># for namespace rook-ceph\n$ kubectl -n rook-ceph create secret docker-registry my-registry-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL\n\n# and for namespace rook-ceph (cluster)\n$ kubectl -n rook-ceph create secret docker-registry my-registry-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL\n</code></pre>  <p>Next we'll add the following snippet to all relevant service accounts as described here:</p> <pre><code>imagePullSecrets:\n- name: my-registry-secret\n</code></pre>  <p>The service accounts are:</p> <ul> <li><code>rook-ceph-system</code> (namespace: <code>rook-ceph</code>): Will affect all pods created by the rook operator in the <code>rook-ceph</code> namespace.</li> <li><code>default</code> (namespace: <code>rook-ceph</code>): Will affect most pods in the <code>rook-ceph</code> namespace.</li> <li><code>rook-ceph-mgr</code> (namespace: <code>rook-ceph</code>): Will affect the MGR pods in the <code>rook-ceph</code> namespace.</li> <li><code>rook-ceph-osd</code> (namespace: <code>rook-ceph</code>): Will affect the OSD pods in the <code>rook-ceph</code> namespace.</li> <li><code>rook-ceph-rgw</code> (namespace: <code>rook-ceph</code>): Will affect the RGW pods in the <code>rook-ceph</code> namespace.</li> </ul> <p>You can do it either via e.g. <code>kubectl -n &lt;namespace&gt; edit serviceaccount default</code> or by modifying the <code>operator.yaml</code> and <code>cluster.yaml</code> before deploying them.</p> <p>Since it's the same procedure for all service accounts, here is just one example:</p> <pre><code>kubectl -n rook-ceph edit serviceaccount default\n</code></pre>  <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: default\n  namespace: rook-ceph\nsecrets:\n- name: default-token-12345\nimagePullSecrets:                # here are the new\n- name: my-registry-secret       # parts\n</code></pre>  <p>After doing this for all service accounts all pods should be able to pull the image from your registry.</p>","title":"Example setup for a ceph cluster"},{"location":"Prerequisites/pod-security-policies/","text":"","title":"Pod Security Policies"},{"location":"Prerequisites/pod-security-policies/#pod-security-policies","text":"<p>Rook requires privileges to manage the storage in your cluster. If you have Pod Security Policies enabled please review this document. By default, Kubernetes clusters do not have PSPs enabled so you may be able to skip this document.</p> <p>If you are configuring Ceph on OpenShift, the Ceph walkthrough will configure the PSPs as well when you start the operator with operator-openshift.yaml.</p> <p>Creating the Rook operator requires privileges for setting up RBAC. To launch the operator you need to have created your user certificate that is bound to ClusterRole <code>cluster-admin</code>.</p>","title":"Pod Security Policies"},{"location":"Prerequisites/pod-security-policies/#rbac-for-podsecuritypolicies","text":"<p>If you have activated the PodSecurityPolicy Admission Controller and thus are using PodSecurityPolicies, you will require additional <code>(Cluster)RoleBindings</code> for the different <code>ServiceAccounts</code> Rook uses to start the Rook Storage Pods.</p> <p>Security policies will differ for different backends. See Ceph's Pod Security Policies set up in common.yaml for an example of how this is done in practice.</p>","title":"RBAC for PodSecurityPolicies"},{"location":"Prerequisites/pod-security-policies/#podsecuritypolicy","text":"<p>You need at least one <code>PodSecurityPolicy</code> that allows privileged <code>Pod</code> execution. Here is an example which should be more permissive than is needed for any backend:</p> <pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: privileged\nspec:\n  fsGroup:\n    rule: RunAsAny\n  privileged: true\n  runAsUser:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n    - '*'\n  allowedCapabilities:\n    - '*'\n  hostPID: true\n  # hostNetwork is required for using host networking\n  hostNetwork: false\n</code></pre>  <p>Hint: Allowing <code>hostNetwork</code> usage is required when using <code>hostNetwork: true</code> in a Cluster <code>CustomResourceDefinition</code>! You are then also required to allow the usage of <code>hostPorts</code> in the <code>PodSecurityPolicy</code>. The given port range will allow all ports:</p> <pre><code>   hostPorts:\n     # Ceph msgr2 port\n     - min: 1\n       max: 65535\n</code></pre>","title":"PodSecurityPolicy"},{"location":"Prerequisites/prerequisites/","text":"<p>Rook can be installed on any existing Kubernetes cluster as long as it meets the minimum version and Rook is granted the required privileges (see below for more information).</p>","title":"Prerequisites"},{"location":"Prerequisites/prerequisites/#minimum-version","text":"<p>Kubernetes v1.16 or higher is supported for the Ceph operator.</p>","title":"Minimum Version"},{"location":"Prerequisites/prerequisites/#ceph-prerequisites","text":"<p>In order to configure the Ceph storage cluster, at least one of these local storage options are required: - Raw devices (no partitions or formatted filesystems) - Raw partitions (no formatted filesystem) - PVs available from a storage class in <code>block</code> mode</p> <p>You can confirm whether your partitions or devices are formatted with filesystems with the following command.</p> <pre><code>lsblk -f\n</code></pre>   <pre><code>NAME                  FSTYPE      LABEL UUID                                   MOUNTPOINT\nvda\n\u2514\u2500vda1                LVM2_member       &gt;eSO50t-GkUV-YKTH-WsGq-hNJY-eKNf-3i07IB\n  \u251c\u2500ubuntu--vg-root   ext4              c2366f76-6e21-4f10-a8f3-6776212e2fe4   /\n  \u2514\u2500ubuntu--vg-swap_1 swap              9492a3dc-ad75-47cd-9596-678e8cf17ff9   [SWAP]\nvdb\n</code></pre>   <p>If the <code>FSTYPE</code> field is not empty, there is a filesystem on top of the corresponding device. In this example, you can use <code>vdb</code> for Ceph and can't use <code>vda</code> or its partitions.</p>","title":"Ceph Prerequisites"},{"location":"Prerequisites/prerequisites/#admission-controller","text":"<p>Enabling the Rook admission controller is recommended to provide an additional level of validation that Rook is configured correctly with the custom resource (CR) settings. An admission controller intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p> <p>To deploy the Rook admission controllers, install the cert manager before Rook is installed:</p> <pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.yaml\n</code></pre>","title":"Admission Controller"},{"location":"Prerequisites/prerequisites/#lvm-package","text":"<p>Ceph OSDs have a dependency on LVM in the following scenarios: - OSDs are created on raw devices or partitions - If encryption is enabled (<code>encryptedDevice: true</code> in the cluster CR) - A <code>metadata</code> device is specified</p> <p>LVM is not required for OSDs in these scenarios: - Creating OSDs on PVCs using the <code>storageClassDeviceSets</code></p> <p>If LVM is required for your scenario, LVM needs to be available on the hosts where OSDs will be running. Some Linux distributions do not ship with the <code>lvm2</code> package. This package is required on all storage nodes in your k8s cluster to run Ceph OSDs. Without this package even though Rook will be able to successfully create the Ceph OSDs, when a node is rebooted the OSD pods running on the restarted node will fail to start. Please install LVM using your Linux distribution's package manager. For example:</p> <p>CentOS:</p> <pre><code>sudo yum install -y lvm2\n</code></pre>  <p>Ubuntu:</p> <pre><code>sudo apt-get install -y lvm2\n</code></pre>  <p>RancherOS:</p> <ul> <li>Since version 1.5.0 LVM is supported</li> <li>Logical volumes will not be activated during the boot process. You need to add an runcmd command for that.</li> </ul> <pre><code>runcmd:\n- [ vgchange, -ay ]\n</code></pre>","title":"LVM package"},{"location":"Prerequisites/prerequisites/#kernel","text":"","title":"Kernel"},{"location":"Prerequisites/prerequisites/#rbd","text":"<p>Ceph requires a Linux kernel built with the RBD module. Many Linux distributions have this module, but not all distributions. For example, the GKE Container-Optimised OS (COS) does not have RBD.</p> <p>You can test your Kubernetes nodes by running <code>modprobe rbd</code>. If it says 'not found', you may have to rebuild your kernel or choose a different Linux distribution.</p>","title":"RBD"},{"location":"Prerequisites/prerequisites/#cephfs","text":"<p>If you will be creating volumes from a Ceph shared file system (CephFS), the recommended minimum kernel version is 4.17. If you have a kernel version less than 4.17, the requested PVC sizes will not be enforced. Storage quotas will only be enforced on newer kernels.</p>","title":"CephFS"},{"location":"Upgrade-Guide/ceph-upgrade/","text":"<p>This guide will walk you through the steps to upgrade the software in a Rook-Ceph cluster from one version to the next. This includes both the Rook-Ceph operator software itself as well as the Ceph cluster software.</p> <p>Upgrades for both the operator and for Ceph are nearly entirely automated save for where Rook's permissions need to be explicitly updated by an admin or when incompatibilities need to be addressed manually due to customizations.</p> <p>We welcome feedback and opening issues!</p>","title":"Rook-Ceph Upgrades"},{"location":"Upgrade-Guide/ceph-upgrade/#supported-versions","text":"<p>This guide is for upgrading from Rook v1.8.x to Rook v1.9.x.</p> <p>Please refer to the upgrade guides from previous releases for supported upgrade paths. Rook upgrades are only supported between official releases. Upgrades to and from <code>master</code> are not supported.</p> <p>For a guide to upgrade previous versions of Rook, please refer to the version of documentation for those releases.</p> <ul> <li>Upgrade 1.7 to 1.8</li> <li>Upgrade 1.6 to 1.7</li> <li>Upgrade 1.5 to 1.6</li> <li>Upgrade 1.4 to 1.5</li> <li>Upgrade 1.3 to 1.4</li> <li>Upgrade 1.2 to 1.3</li> <li>Upgrade 1.1 to 1.2</li> <li>Upgrade 1.0 to 1.1</li> <li>Upgrade 0.9 to 1.0</li> <li>Upgrade 0.8 to 0.9</li> <li>Upgrade 0.7 to 0.8</li> <li>Upgrade 0.6 to 0.7</li> <li>Upgrade 0.5 to 0.6</li> </ul>","title":"Supported Versions"},{"location":"Upgrade-Guide/ceph-upgrade/#breaking-changes-in-this-release","text":"<ul> <li> <p>Helm charts now define default resource requests and limits for Rook-Ceph Pods. If you use Helm,   ensure you have defined an override for these in your <code>values.yaml</code> if you don't wish to use the   recommended defaults. Setting resource requests and limits could mean that Kubernetes will not   allow Pods to be scheduled in some cases. If sufficient resources are not available, you can   reduce or remove the requests and limits.</p> </li> <li> <p>MDS liveness and startup probes are now configured by the CephFilesystem resource instead of   CephCluster. Upgrade instructions are below.</p> </li> <li> <p>Rook no longer deploys Prometheus rules from the operator. If you have been relying on Rook to   deploy prometheus rules in the past, please follow the upgrade instructions below.</p> </li> <li> <p>Due to a number of Ceph issues and changes, Rook officially only supports Ceph   v16.2.7 or higher for CephNFS. If you are using an earlier version, upgrade your Ceph version   following the advice given in Rook's v1.8 NFS docs.</p> </li> </ul>","title":"Breaking changes in this release"},{"location":"Upgrade-Guide/ceph-upgrade/#considerations","text":"<p>With this upgrade guide, there are a few notes to consider:</p> <ul> <li>WARNING: Upgrading a Rook cluster is not without risk. There may be unexpected issues or   obstacles that damage the integrity and health of your storage cluster, including data loss.</li> <li>The Rook cluster's storage may be unavailable for short periods during the upgrade process for   both Rook operator updates and for Ceph version updates.</li> <li>We recommend that you read this document in full before you undertake a Rook cluster upgrade.</li> </ul>","title":"Considerations"},{"location":"Upgrade-Guide/ceph-upgrade/#patch-release-upgrades","text":"<p>Unless otherwise noted due to extenuating requirements, upgrades from one patch release of Rook to another are as simple as updating the common resources and the image of the Rook operator. For example, when Rook v1.9.1 is released, the process of updating from v1.9.0 is as simple as running the following:</p> <p>First get the latest common resources manifests that contain the latest changes for Rook v1.9. <pre><code>git clone --single-branch --depth=1 --branch v1.9.1 https://github.com/rook/rook.git\ncd rook/deploy/examples\n</code></pre> </p> <p>If you have deployed the Rook Operator or the Ceph cluster into a different namespace than <code>rook-ceph</code>, see the Update common resources and CRDs section for instructions on how to change the default namespaces in <code>common.yaml</code>.</p> <p>Then apply the latest changes from v1.9 and update the Rook Operator image. <pre><code>kubectl apply -f common.yaml -f crds.yaml\nkubectl -n rook-ceph set image deploy/rook-ceph-operator rook-ceph-operator=rook/ceph:v1.9.1\n</code></pre> </p> <p>As exemplified above, it is a good practice to update Rook-Ceph common resources from the example manifests before any update. The common resources and CRDs might not be updated with every release, but K8s will only apply updates to the ones that changed.</p> <p>Also update optional resources like Prometheus monitoring noted more fully in the upgrade section below.</p>","title":"Patch Release Upgrades"},{"location":"Upgrade-Guide/ceph-upgrade/#helm","text":"<ul> <li>The minimum supported Helm version is v3.2.0</li> </ul> <p>If you have installed Rook via the Helm chart, Helm will handle some details of the upgrade for you. The upgrade steps in this guide will clarify if Helm manages the step for you.</p> <p>The <code>rook-ceph</code> helm chart upgrade performs the Rook upgrade. The <code>rook-ceph-cluster</code> helm chart upgrade performs a Ceph upgrade if the Ceph image is updated.</p>","title":"Helm"},{"location":"Upgrade-Guide/ceph-upgrade/#upgrading-from-v18-to-v19","text":"<p>Rook releases from master are expressly unsupported. It is strongly recommended that you use official releases of Rook. Unreleased versions from the master branch are subject to changes and incompatibilities that will not be supported in the official releases. Builds from the master branch can have functionality changed or removed at any time without compatibility support and without prior notice.</p>","title":"Upgrading from v1.8 to v1.9"},{"location":"Upgrade-Guide/ceph-upgrade/#prerequisites","text":"<p>We will do all our work in the Ceph example manifests directory.</p> <pre><code>$ cd $YOUR_ROOK_REPO/deploy/examples/\n</code></pre>  <p>Unless your Rook cluster was created with customized namespaces, namespaces for Rook clusters are likely to be:</p> <ul> <li>Clusters created by v0.7 or earlier: <code>rook-system</code> and <code>rook</code></li> <li>Clusters created in v0.8 or v0.9: <code>rook-ceph-system</code> and <code>rook-ceph</code></li> <li>Clusters created in v1.0 or newer: only <code>rook-ceph</code></li> </ul> <p>With this guide, we do our best not to assume the namespaces in your cluster. To make things as easy as possible, modify and use the below snippet to configure your environment. We will use these environment variables throughout this document.</p> <pre><code># Parameterize the environment\nexport ROOK_OPERATOR_NAMESPACE=\"rook-ceph\"\nexport ROOK_CLUSTER_NAMESPACE=\"rook-ceph\"\n</code></pre>  <p>In order to successfully upgrade a Rook cluster, the following prerequisites must be met:</p> <ul> <li>The cluster should be in a healthy state with full functionality. Review the health verification section in order to verify your cluster is in a good   starting state.</li> <li>All pods consuming Rook storage should be created, running, and in a steady state. No Rook   persistent volumes should be in the act of being created or deleted.</li> </ul>","title":"Prerequisites"},{"location":"Upgrade-Guide/ceph-upgrade/#health-verification","text":"<p>Before we begin the upgrade process, let's first review some ways that you can verify the health of your cluster, ensuring that the upgrade is going smoothly after each step. Most of the health verification checks for your cluster during the upgrade process can be performed with the Rook toolbox. For more information about how to run the toolbox, please visit the Rook toolbox readme.</p> <p>See the common issues pages for troubleshooting and correcting health issues:</p> <ul> <li>General troubleshooting</li> <li>Ceph troubleshooting</li> </ul>","title":"Health Verification"},{"location":"Upgrade-Guide/ceph-upgrade/#pods-all-running","text":"<p>In a healthy Rook cluster, the operator, the agents and all Rook namespace pods should be in the <code>Running</code> state and have few, if any, pod restarts. To verify this, run the following commands:</p> <pre><code>kubectl -n $ROOK_CLUSTER_NAMESPACE get pods\n</code></pre>","title":"Pods all Running"},{"location":"Upgrade-Guide/ceph-upgrade/#status-output","text":"<p>The Rook toolbox contains the Ceph tools that can give you status details of the cluster with the <code>ceph status</code> command. Let's look at an output sample and review some of the details:</p> <pre><code>TOOLS_POD=$(kubectl -n $ROOK_CLUSTER_NAMESPACE get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[*].metadata.name}')\nkubectl -n $ROOK_CLUSTER_NAMESPACE exec -it $TOOLS_POD -- ceph status\n</code></pre>   <pre><code>  cluster:\n    id:     a3f4d647-9538-4aff-9fd1-b845873c3fe9\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum b,c,a\n    mgr: a(active)\n    mds: myfs-1/1/1 up  {0=myfs-a=up:active}, 1 up:standby-replay\n    osd: 6 osds: 6 up, 6 in\n    rgw: 1 daemon active\n\n  data:\n    pools:   9 pools, 900 pgs\n    objects: 67  objects, 11 KiB\n    usage:   6.1 GiB used, 54 GiB / 60 GiB avail\n    pgs:     900 active+clean\n\n  io:\n    client:   7.4 KiB/s rd, 681 B/s wr, 11 op/s rd, 4 op/s wr\n    recovery: 164 B/s, 1 objects/s\n</code></pre>   <p>In the output above, note the following indications that the cluster is in a healthy state:</p> <ul> <li>Cluster health: The overall cluster status is <code>HEALTH_OK</code> and there are no warning or error status   messages displayed.</li> <li>Monitors (mon):  All of the monitors are included in the <code>quorum</code> list.</li> <li>Manager (mgr): The Ceph manager is in the <code>active</code> state.</li> <li>OSDs (osd): All OSDs are <code>up</code> and <code>in</code>.</li> <li>Placement groups (pgs): All PGs are in the <code>active+clean</code> state.</li> <li>(If applicable) Ceph filesystem metadata server (mds): all MDSes are <code>active</code> for all filesystems</li> <li>(If applicable) Ceph object store RADOS gateways (rgw): all daemons are <code>active</code></li> </ul> <p>If your <code>ceph status</code> output has deviations from the general good health described above, there may be an issue that needs to be investigated further. There are other commands you may run for more details on the health of the system, such as <code>ceph osd status</code>. See the Ceph troubleshooting docs for help.</p> <p>Rook will prevent the upgrade of the Ceph daemons if the health is in a <code>HEALTH_ERR</code> state. If you desired to proceed with the upgrade anyway, you will need to set either <code>skipUpgradeChecks: true</code> or <code>continueUpgradeAfterChecksEvenIfNotHealthy: true</code> as described in the cluster CR settings.</p>","title":"Status Output"},{"location":"Upgrade-Guide/ceph-upgrade/#container-versions","text":"<p>The container version running in a specific pod in the Rook cluster can be verified in its pod spec output. For example for the monitor pod <code>mon-b</code>, we can verify the container version it is running with the below commands:</p> <pre><code>POD_NAME=$(kubectl -n $ROOK_CLUSTER_NAMESPACE get pod -o custom-columns=name:.metadata.name --no-headers | grep rook-ceph-mon-b)\nkubectl -n $ROOK_CLUSTER_NAMESPACE get pod ${POD_NAME} -o jsonpath='{.spec.containers[0].image}'\n</code></pre>  <p>The status and container versions for all Rook pods can be collected all at once with the following commands:</p> <pre><code>kubectl -n $ROOK_OPERATOR_NAMESPACE get pod -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\\t\"}{.status.phase}{\"\\t\\t\"}{.spec.containers[0].image}{\"\\t\"}{.spec.initContainers[0]}{\"\\n\"}{end}' &amp;&amp; \\\nkubectl -n $ROOK_CLUSTER_NAMESPACE get pod -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\\t\"}{.status.phase}{\"\\t\\t\"}{.spec.containers[0].image}{\"\\t\"}{.spec.initContainers[0].image}{\"\\n\"}{end}'\n</code></pre>  <p>The <code>rook-version</code> label exists on Ceph controller resources. For various resource controllers, a summary of the resource controllers can be gained with the commands below. These will report the requested, updated, and currently available replicas for various Rook-Ceph resources in addition to the version of Rook for resources managed by the updated Rook-Ceph operator. Note that the operator and toolbox deployments do not have a <code>rook-version</code> label set.</p> <pre><code>kubectl -n $ROOK_CLUSTER_NAMESPACE get deployments -o jsonpath='{range .items[*]}{.metadata.name}{\"  \\treq/upd/avl: \"}{.spec.replicas}{\"/\"}{.status.updatedReplicas}{\"/\"}{.status.readyReplicas}{\"  \\trook-version=\"}{.metadata.labels.rook-version}{\"\\n\"}{end}'\n\nkubectl -n $ROOK_CLUSTER_NAMESPACE get jobs -o jsonpath='{range .items[*]}{.metadata.name}{\"  \\tsucceeded: \"}{.status.succeeded}{\"      \\trook-version=\"}{.metadata.labels.rook-version}{\"\\n\"}{end}'\n</code></pre>","title":"Container Versions"},{"location":"Upgrade-Guide/ceph-upgrade/#rook-volume-health","text":"<p>Any pod that is using a Rook volume should also remain healthy:</p> <ul> <li>The pod should be in the <code>Running</code> state with few, if any, restarts</li> <li>There should be no errors in its logs</li> <li>The pod should still be able to read and write to the attached Rook volume.</li> </ul>","title":"Rook Volume Health"},{"location":"Upgrade-Guide/ceph-upgrade/#rook-operator-upgrade-process","text":"<p>In the examples given in this guide, we will be upgrading a live Rook cluster running <code>v1.8.8</code> to the version <code>v1.9.0</code>. This upgrade should work from any official patch release of Rook v1.8 to any official patch release of v1.9.</p> <p>Rook releases from <code>master</code> are expressly unsupported. It is strongly recommended that you use official releases of Rook. Unreleased versions from the master branch are subject to changes and incompatibilities that will not be supported in the official releases. Builds from the master branch can have functionality changed or removed at any time without compatibility support and without prior notice.</p> <p>These methods should work for any number of Rook-Ceph clusters and Rook Operators as long as you parameterize the environment correctly. Merely repeat these steps for each Rook-Ceph cluster (<code>ROOK_CLUSTER_NAMESPACE</code>), and be sure to update the <code>ROOK_OPERATOR_NAMESPACE</code> parameter each time if applicable.</p> <p>Let's get started!</p>","title":"Rook Operator Upgrade Process"},{"location":"Upgrade-Guide/ceph-upgrade/#1-update-common-resources-and-crds","text":"<p>Automatically updated if you are upgrading via the helm chart</p>  <p>First apply updates to Rook-Ceph common resources. This includes modified privileges (RBAC) needed by the Operator. Also update the Custom Resource Definitions (CRDs).</p> <p>Get the latest common resources manifests that contain the latest changes. <pre><code>git clone --single-branch --depth=1 --branch v1.9.0 https://github.com/rook/rook.git\ncd rook/deploy/examples\n</code></pre> </p> <p>If you have deployed the Rook Operator or the Ceph cluster into a different namespace than <code>rook-ceph</code>, update the common resource manifests to use your <code>ROOK_OPERATOR_NAMESPACE</code> and <code>ROOK_CLUSTER_NAMESPACE</code> using <code>sed</code>. <pre><code>sed -i.bak \\\n    -e \"s/\\(.*\\):.*# namespace:operator/\\1: $ROOK_OPERATOR_NAMESPACE # namespace:operator/g\" \\\n    -e \"s/\\(.*\\):.*# namespace:cluster/\\1: $ROOK_CLUSTER_NAMESPACE # namespace:cluster/g\" \\\n  common.yaml\n</code></pre> </p> <p>Then apply the latest changes. <pre><code>kubectl apply -f common.yaml -f crds.yaml\n</code></pre> </p>","title":"1. Update common resources and CRDs"},{"location":"Upgrade-Guide/ceph-upgrade/#updates-for-optional-resources","text":"","title":"Updates for optional resources"},{"location":"Upgrade-Guide/ceph-upgrade/#prometheus","text":"<p>If you have Prometheus monitoring enabled, follow the step to upgrade the Prometheus RBAC resources as well.</p> <pre><code>kubectl apply -f deploy/examples/monitoring/rbac.yaml\n</code></pre>  <p>Rook no longer deploys Prometheus rules from the operator.</p> <p>If you use the Helm chart <code>monitoring.enabled</code> value to deploy Prometheus rules, you may now additionally use <code>monitoring.createPrometheusRules</code> to instruct Helm to deploy the rules. You may alternately deploy the rules manually if you wish.</p> <p>To see the latest information about manually deploying rules, see the Prometheus monitoring docs.</p>","title":"Prometheus"},{"location":"Upgrade-Guide/ceph-upgrade/#mds-liveness-and-startup-probes","text":"<p>If you configure MDS probes in the CephCluster resource, copy them to the CephFilesystem <code>metadataServer</code> settings at this point. Do not remove them from the CephCluster until after the Rook upgrade is fully complete.</p>","title":"MDS liveness and startup probes"},{"location":"Upgrade-Guide/ceph-upgrade/#2-update-ceph-csi-versions","text":"<p>Automatically updated if you are upgrading via the helm chart</p>  <p>If you have specified custom CSI images in the Rook-Ceph Operator deployment, we recommended you update to use the latest Ceph-CSI drivers. See the CSI Version section for more details.</p>  <p>Note: If using snapshots, refer to the Upgrade Snapshot API guide.</p>","title":"2. Update Ceph CSI versions"},{"location":"Upgrade-Guide/ceph-upgrade/#3-update-the-rook-operator","text":"<p>Automatically updated if you are upgrading via the helm chart</p>  <p>The largest portion of the upgrade is triggered when the operator's image is updated to <code>v1.9.x</code>. When the operator is updated, it will proceed to update all of the Ceph daemons.</p> <pre><code>kubectl -n $ROOK_OPERATOR_NAMESPACE set image deploy/rook-ceph-operator rook-ceph-operator=rook/ceph:v1.9.0\n</code></pre>","title":"3. Update the Rook Operator"},{"location":"Upgrade-Guide/ceph-upgrade/#4-wait-for-the-upgrade-to-complete","text":"<p>Watch now in amazement as the Ceph mons, mgrs, OSDs, rbd-mirrors, MDSes and RGWs are terminated and replaced with updated versions in sequence. The cluster may be offline very briefly as mons update, and the Ceph Filesystem may fall offline a few times while the MDSes are upgrading. This is normal.</p> <p>The versions of the components can be viewed as they are updated:</p> <pre><code>watch --exec kubectl -n $ROOK_CLUSTER_NAMESPACE get deployments -l rook_cluster=$ROOK_CLUSTER_NAMESPACE -o jsonpath='{range .items[*]}{.metadata.name}{\"  \\treq/upd/avl: \"}{.spec.replicas}{\"/\"}{.status.updatedReplicas}{\"/\"}{.status.readyReplicas}{\"  \\trook-version=\"}{.metadata.labels.rook-version}{\"\\n\"}{end}'\n</code></pre>  <p>As an example, this cluster is midway through updating the OSDs. When all deployments report <code>1/1/1</code> availability and <code>rook-version=v1.9.0</code>, the Ceph cluster's core components are fully updated.</p>  <pre><code>Every 2.0s: kubectl -n rook-ceph get deployment -o j...\n\nrook-ceph-mgr-a         req/upd/avl: 1/1/1      rook-version=v1.9.0\nrook-ceph-mon-a         req/upd/avl: 1/1/1      rook-version=v1.9.0\nrook-ceph-mon-b         req/upd/avl: 1/1/1      rook-version=v1.9.0\nrook-ceph-mon-c         req/upd/avl: 1/1/1      rook-version=v1.9.0\nrook-ceph-osd-0         req/upd/avl: 1//        rook-version=v1.9.0\nrook-ceph-osd-1         req/upd/avl: 1/1/1      rook-version=v1.8.8\nrook-ceph-osd-2         req/upd/avl: 1/1/1      rook-version=v1.8.8\n</code></pre>   <p>An easy check to see if the upgrade is totally finished is to check that there is only one <code>rook-version</code> reported across the cluster.</p> <pre><code># kubectl -n $ROOK_CLUSTER_NAMESPACE get deployment -l rook_cluster=$ROOK_CLUSTER_NAMESPACE -o jsonpath='{range .items[*]}{\"rook-version=\"}{.metadata.labels.rook-version}{\"\\n\"}{end}' | sort | uniq\nThis cluster is not yet finished:\n  rook-version=v1.8.8\n  rook-version=v1.9.0\nThis cluster is finished:\n  rook-version=v1.9.0\n</code></pre>","title":"4. Wait for the upgrade to complete"},{"location":"Upgrade-Guide/ceph-upgrade/#5-verify-the-updated-cluster","text":"<p>At this point, your Rook operator should be running version <code>rook/ceph:v1.9.0</code>.</p> <p>Verify the Ceph cluster's health using the health verification section.</p>","title":"5. Verify the updated cluster"},{"location":"Upgrade-Guide/ceph-upgrade/#ceph-version-upgrades","text":"<p>Rook v1.9 supports the following Ceph versions: - Ceph Quincy v17.2.0 or newer - Ceph Pacific v16.2.0 or newer - Ceph Octopus v15.2.0 or newer</p> <p>These are the only supported versions of Ceph. Rook v1.10 is planning to drop support for Ceph Octopus (15.2.x), so please consider upgrading your Ceph cluster.</p>  <p>IMPORTANT: When an update is requested, the operator will check Ceph's status, if it is in <code>HEALTH_ERR</code> it will refuse to do the upgrade.</p>  <p>Rook is cautious when performing upgrades. When an upgrade is requested (the Ceph image has been updated in the CR), Rook will go through all the daemons one by one and will individually perform checks on them. It will make sure a particular daemon can be stopped before performing the upgrade. Once the deployment has been updated, it checks if this is ok to continue. After each daemon is updated we wait for things to settle (monitors to be in a quorum, PGs to be clean for OSDs, up for MDSes, etc.), then only when the condition is met we move to the next daemon. We repeat this process until all the daemons have been updated.</p> <p>We recommend updating to v16.2.7 or newer. If you require updating to v16.2.0-v16.2.6, please see the v1.8 upgrade guide for a special upgrade consideration.</p>","title":"Ceph Version Upgrades"},{"location":"Upgrade-Guide/ceph-upgrade/#rename-cephblockpool-device_health_metrics-pool-when-upgrading-to-quincy-v17","text":"<p>In Ceph Quincy (v17), the <code>device_health_metrics</code> pool was renamed to <code>.mgr</code>. Ceph will perform this migration automatically. If you do not use CephBlockPool to customize the configuration of the <code>device_health_metrics</code> pool, you don't need to do anything further here.</p> <p>If you do use CephBlockPool to customize the configuration of the <code>device_health_metrics</code> pool, you will need two extra steps after the Ceph upgrade is complete. Once upgrade is complete: 1. Create a new CephBlockPool to configure the <code>.mgr</code> built-in pool. You can reference the example builtin mgr pool. 2. Delete the old CephBlockPool that represents the <code>device_health_metrics</code> pool.</p>","title":"Rename CephBlockPool device_health_metrics pool when upgrading to Quincy v17"},{"location":"Upgrade-Guide/ceph-upgrade/#important-consideration-for-cephnfs-users","text":"<p>Ceph Quincy v17.2.0 has a potentially breaking regression with CephNFS. See the NFS documentation's known issue for more detail.</p>","title":"Important consideration for CephNFS users"},{"location":"Upgrade-Guide/ceph-upgrade/#ceph-images","text":"<p>Official Ceph container images can be found on Quay. Prior to August 2021, official images were on docker.io. While those images will remain on Docker Hub, all new images are being pushed to Quay.</p> <p>These images are tagged in a few ways:</p> <ul> <li>The most explicit form of tags are full-ceph-version-and-build tags (e.g., <code>v16.2.7-20220216</code>).   These tags are recommended for production clusters, as there is no possibility for the cluster to   be heterogeneous with respect to the version of Ceph running in containers.</li> <li>Ceph major version tags (e.g., <code>v16</code>) are useful for development and test clusters so that the   latest version of Ceph is always available.</li> </ul> <p>Ceph containers other than the official images from the registry above will not be supported.</p>","title":"Ceph images"},{"location":"Upgrade-Guide/ceph-upgrade/#example-upgrade-to-ceph-pacific","text":"","title":"Example upgrade to Ceph Pacific"},{"location":"Upgrade-Guide/ceph-upgrade/#1-update-the-main-ceph-daemons","text":"<p>The majority of the upgrade will be handled by the Rook operator. Begin the upgrade by changing the Ceph image field in the cluster CRD (<code>spec.cephVersion.image</code>).</p> <pre><code>NEW_CEPH_IMAGE='quay.io/ceph/ceph:v16.2.7-20220216'\nCLUSTER_NAME=\"$ROOK_CLUSTER_NAMESPACE\"  # change if your cluster name is not the Rook namespace\nkubectl -n $ROOK_CLUSTER_NAMESPACE patch CephCluster $CLUSTER_NAME --type=merge -p \"{\\\"spec\\\": {\\\"cephVersion\\\": {\\\"image\\\": \\\"$NEW_CEPH_IMAGE\\\"}}}\"\n</code></pre>","title":"1. Update the main Ceph daemons"},{"location":"Upgrade-Guide/ceph-upgrade/#2-wait-for-the-daemon-pod-updates-to-complete","text":"<p>As with upgrading Rook, you must now wait for the upgrade to complete. Status can be determined in a similar way to the Rook upgrade as well.</p> <pre><code>watch --exec kubectl -n $ROOK_CLUSTER_NAMESPACE get deployments -l rook_cluster=$ROOK_CLUSTER_NAMESPACE -o jsonpath='{range .items[*]}{.metadata.name}{\"  \\treq/upd/avl: \"}{.spec.replicas}{\"/\"}{.status.updatedReplicas}{\"/\"}{.status.readyReplicas}{\"  \\tceph-version=\"}{.metadata.labels.ceph-version}{\"\\n\"}{end}'\n</code></pre>  <p>Determining when the Ceph has fully updated is rather simple.</p> <pre><code>kubectl -n $ROOK_CLUSTER_NAMESPACE get deployment -l rook_cluster=$ROOK_CLUSTER_NAMESPACE -o jsonpath='{range .items[*]}{\"ceph-version=\"}{.metadata.labels.ceph-version}{\"\\n\"}{end}' | sort | uniq\nThis cluster is not yet finished:\n    ceph-version=15.2.13-0\n    ceph-version=16.2.6-0\nThis cluster is finished:\n    ceph-version=16.2.6-0\n</code></pre>","title":"2. Wait for the daemon pod updates to complete"},{"location":"Upgrade-Guide/ceph-upgrade/#3-verify-the-updated-cluster","text":"<p>Verify the Ceph cluster's health using the health verification section.</p>","title":"3. Verify the updated cluster"},{"location":"Upgrade-Guide/ceph-upgrade/#csi-version","text":"<p>If you have a cluster running with CSI drivers enabled and you want to configure Rook to use non-default CSI images, the following settings will need to be applied for the desired version of CSI.</p> <p>The operator configuration variables are found in the <code>rook-ceph-operator-config</code> ConfigMap. These settings can also be specified as environment variables on the operator deployment, but the env vars will be overridden if configmap values are specified.</p> <pre><code>kubectl -n $ROOK_OPERATOR_NAMESPACE edit configmap rook-ceph-operator-config\n</code></pre>  <p>The default upstream images are included below, which you can change to your desired images.</p> <pre><code>ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.6.1\"\nROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0\"\nROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0\"\nROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.4.0\"\nROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.4.0\"\nROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1\"\nCSI_VOLUME_REPLICATION_IMAGE: \"quay.io/csiaddons/volumereplication-operator:v0.3.0\"\nROOK_CSIADDONS_IMAGE: \"quay.io/csiaddons/k8s-sidecar:v0.2.1\"\n</code></pre>","title":"CSI Version"},{"location":"Upgrade-Guide/ceph-upgrade/#use-default-images","text":"<p>If you would like Rook to use the inbuilt default upstream images, then you may simply remove all variables matching <code>ROOK_CSI_*_IMAGE</code> from the above ConfigMap and/or the operator deployment.</p>","title":"Use default images"},{"location":"Upgrade-Guide/ceph-upgrade/#verifying-updates","text":"<p>You can use the below command to see the CSI images currently being used in the cluster. Note that not all images (like <code>volumereplication-operator</code>) may be present in every cluster depending on which CSI features are enabled.</p> <pre><code>kubectl --namespace rook-ceph get pod -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{\"\\n\"}' -l 'app in (csi-rbdplugin,csi-rbdplugin-provisioner,csi-cephfsplugin,csi-cephfsplugin-provisioner)' | sort | uniq\n</code></pre>  <p>The default images can also be found with each release in the images list</p>","title":"Verifying updates"},{"location":"Usages-and-Examples/ceph-advanced-configuration/","text":"<p>These examples show how to perform advanced configuration tasks on your Rook storage cluster.</p> <ul> <li>Prerequisites</li> <li>Using alternate namespaces</li> <li>Deploying a second cluster</li> <li>Log Collection</li> <li>OSD Information</li> <li>Separate Storage Groups</li> <li>Configuring Pools</li> <li>Custom ceph.conf Settings</li> <li>Custom CSI ceph.conf Settings</li> <li>OSD CRUSH Settings</li> <li>OSD Dedicated Network</li> <li>Phantom OSD Removal</li> <li>Auto Expansion of OSDs</li> </ul>","title":"Advanced Configuration"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#prerequisites","text":"<p>Most of the examples make use of the <code>ceph</code> client command.  A quick way to use the Ceph client suite is from a Rook Toolbox container.</p> <p>The Kubernetes based examples assume Rook OSD pods are in the <code>rook-ceph</code> namespace. If you run them in a different namespace, modify <code>kubectl -n rook-ceph [...]</code> to fit your situation.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#using-alternate-namespaces","text":"<p>If you wish to deploy the Rook Operator and/or Ceph clusters to namespaces other than the default <code>rook-ceph</code>, the manifests are commented to allow for easy <code>sed</code> replacements. Change <code>ROOK_CLUSTER_NAMESPACE</code> to tailor the manifests for additional Ceph clusters. You can choose to also change <code>ROOK_OPERATOR_NAMESPACE</code> to create a new Rook Operator for each Ceph cluster (don't forget to set <code>ROOK_CURRENT_NAMESPACE_ONLY</code>), or you can leave it at the same value for every Ceph cluster if you only wish to have one Operator manage all Ceph clusters.</p> <p>This will help you manage namespaces more easily, but you should still make sure the resources are configured to your liking.</p> <pre><code>cd deploy/examples\n\nexport ROOK_OPERATOR_NAMESPACE=\"rook-ceph\"\nexport ROOK_CLUSTER_NAMESPACE=\"rook-ceph\"\n\nsed -i.bak \\\n    -e \"s/\\(.*\\):.*# namespace:operator/\\1: $ROOK_OPERATOR_NAMESPACE # namespace:operator/g\" \\\n    -e \"s/\\(.*\\):.*# namespace:cluster/\\1: $ROOK_CLUSTER_NAMESPACE # namespace:cluster/g\" \\\n    -e \"s/\\(.*serviceaccount\\):.*:\\(.*\\) # serviceaccount:namespace:operator/\\1:$ROOK_OPERATOR_NAMESPACE:\\2 # serviceaccount:namespace:operator/g\" \\\n    -e \"s/\\(.*serviceaccount\\):.*:\\(.*\\) # serviceaccount:namespace:cluster/\\1:$ROOK_CLUSTER_NAMESPACE:\\2 # serviceaccount:namespace:cluster/g\" \\\n    -e \"s/\\(.*\\): [-_A-Za-z0-9]*\\.\\(.*\\) # driver:namespace:operator/\\1: $ROOK_OPERATOR_NAMESPACE.\\2 # driver:namespace:operator/g\" \\\n    -e \"s/\\(.*\\): [-_A-Za-z0-9]*\\.\\(.*\\) # driver:namespace:cluster/\\1: $ROOK_CLUSTER_NAMESPACE.\\2 # driver:namespace:cluster/g\" \\\n  common.yaml operator.yaml cluster.yaml # add other files or change these as desired for your config\n\n# You need to use `apply` for all Ceph clusters after the first if you have only one Operator\nkubectl apply -f common.yaml -f operator.yaml -f cluster.yaml # add other files as desired for yourconfig\n</code></pre>","title":"Using alternate namespaces"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#deploying-a-second-cluster","text":"<p>If you wish to create a new CephCluster in a different namespace than <code>rook-ceph</code> while using a single operator to manage both clusters execute the following:</p> <pre><code>cd deploy/examples\n\nNAMESPACE=rook-ceph-secondary envsubst &lt; common-second-cluster.yaml | kubectl create -f -\n</code></pre>  <p>This will create all the necessary RBACs as well as the new namespace. The script assumes that <code>common.yaml</code> was already created. When you create the second CephCluster CR, use the same <code>NAMESPACE</code> and the operator will configure the second cluster.</p>","title":"Deploying a second cluster"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#log-collection","text":"<p>All Rook logs can be collected in a Kubernetes environment with the following command:</p> <pre><code>for p in $(kubectl -n rook-ceph get pods -o jsonpath='{.items[*].metadata.name}')\ndo\n    for c in $(kubectl -n rook-ceph get pod ${p} -o jsonpath='{.spec.containers[*].name}')\n    do\n        echo \"BEGIN logs from pod: ${p} ${c}\"\n        kubectl -n rook-ceph logs -c ${c} ${p}\n        echo \"END logs from pod: ${p} ${c}\"\n    done\ndone\n</code></pre>  <p>This gets the logs for every container in every Rook pod and then compresses them into a <code>.gz</code> archive for easy sharing.  Note that instead of <code>gzip</code>, you could instead pipe to <code>less</code> or to a single text file.</p>","title":"Log Collection"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#osd-information","text":"<p>Keeping track of OSDs and their underlying storage devices can be difficult. The following scripts will clear things up quickly.</p>","title":"OSD Information"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#kubernetes","text":"<pre><code># Get OSD Pods\n# This uses the example/default cluster name \"rook\"\nOSD_PODS=$(kubectl get pods --all-namespaces -l \\\n  app=rook-ceph-osd,rook_cluster=rook-ceph -o jsonpath='{.items[*].metadata.name}')\n\n# Find node and drive associations from OSD pods\nfor pod in $(echo ${OSD_PODS})\ndo\n echo \"Pod:  ${pod}\"\n echo \"Node: $(kubectl -n rook-ceph get pod ${pod} -o jsonpath='{.spec.nodeName}')\"\n kubectl -n rook-ceph exec ${pod} -- sh -c '\\\n  for i in /var/lib/ceph/osd/ceph-*; do\n    [ -f ${i}/ready ] || continue\n    echo -ne \"-$(basename ${i}) \"\n    echo $(lsblk -n -o NAME,SIZE ${i}/block 2&gt; /dev/null || \\\n    findmnt -n -v -o SOURCE,SIZE -T ${i}) $(cat ${i}/type)\n  done | sort -V\n  echo'\ndone\n</code></pre>  <p>The output should look something like this.</p>  <pre><code>Pod:  osd-m2fz2\nNode: node1.zbrbdl\n-osd0  sda3  557.3G  bluestore\n-osd1  sdf3  110.2G  bluestore\n-osd2  sdd3  277.8G  bluestore\n-osd3  sdb3  557.3G  bluestore\n-osd4  sde3  464.2G  bluestore\n-osd5  sdc3  557.3G  bluestore\n\nPod:  osd-nxxnq\nNode: node3.zbrbdl\n-osd6   sda3  110.7G  bluestore\n-osd17  sdd3  1.8T    bluestore\n-osd18  sdb3  231.8G  bluestore\n-osd19  sdc3  231.8G  bluestore\n\nPod:  osd-tww1h\nNode: node2.zbrbdl\n-osd7   sdc3  464.2G  bluestore\n-osd8   sdj3  557.3G  bluestore\n-osd9   sdf3  66.7G   bluestore\n-osd10  sdd3  464.2G  bluestore\n-osd11  sdb3  147.4G  bluestore\n-osd12  sdi3  557.3G  bluestore\n-osd13  sdk3  557.3G  bluestore\n-osd14  sde3  66.7G   bluestore\n-osd15  sda3  110.2G  bluestore\n-osd16  sdh3  135.1G  bluestore\n</code></pre>","title":"Kubernetes"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#separate-storage-groups","text":"<p>DEPRECATED: Instead of manually needing to set this, the <code>deviceClass</code> property can be used on Pool structures in <code>CephBlockPool</code>, <code>CephFilesystem</code> and <code>CephObjectStore</code> CRD objects.</p>  <p>By default Rook/Ceph puts all storage under one replication rule in the CRUSH Map which provides the maximum amount of storage capacity for a cluster.  If you would like to use different storage endpoints for different purposes, you'll have to create separate storage groups.</p> <p>In the following example we will separate SSD drives from spindle-based drives, a common practice for those looking to target certain workloads onto faster (database) or slower (file archive) storage.</p>","title":"Separate Storage Groups"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#configuring-pools","text":"","title":"Configuring Pools"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#placement-group-sizing","text":"<p>NOTE: Since Ceph Nautilus (v14.x), you can use the Ceph MGR <code>pg_autoscaler</code> module to auto scale the PGs as needed. If you want to enable this feature, please refer to Default PG and PGP counts.</p>  <p>The general rules for deciding how many PGs your pool(s) should contain is:</p> <ul> <li>Less than 5 OSDs set pg_num to 128</li> <li>Between 5 and 10 OSDs set pg_num to 512</li> <li>Between 10 and 50 OSDs set pg_num to 1024</li> </ul> <p>If you have more than 50 OSDs, you need to understand the tradeoffs and how to calculate the pg_num value by yourself. For calculating pg_num yourself please make use of the pgcalc tool.</p> <p>If you're already using a pool it is generally safe to increase its PG count on-the-fly. Decreasing the PG count is not recommended on a pool that is in use. The safest way to decrease the PG count is to back-up the data, delete the pool, and recreate it.  With backups you can try a few potentially unsafe tricks for live pools, documented here.</p>","title":"Placement Group Sizing"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#setting-pg-count","text":"<p>Be sure to read the placement group sizing section before changing the number of PGs.</p> <pre><code># Set the number of PGs in the rbd pool to 512\nceph osd pool set rbd pg_num 512\n</code></pre>","title":"Setting PG Count"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#custom-cephconf-settings","text":"<p>WARNING: The advised method for controlling Ceph configuration is to manually use the Ceph CLI or the Ceph dashboard because this offers the most flexibility. It is highly recommended that this only be used when absolutely necessary and that the <code>config</code> be reset to an empty string if/when the configurations are no longer necessary. Configurations in the config file will make the Ceph cluster less configurable from the CLI and dashboard and may make future tuning or debugging difficult.</p>  <p>Setting configs via Ceph's CLI requires that at least one mon be available for the configs to be set, and setting configs via dashboard requires at least one mgr to be available. Ceph may also have a small number of very advanced settings that aren't able to be modified easily via CLI or dashboard. In order to set configurations before monitors are available or to set problematic configuration settings, the <code>rook-config-override</code> ConfigMap exists, and the <code>config</code> field can be set with the contents of a <code>ceph.conf</code> file. The contents will be propagated to all mon, mgr, OSD, MDS, and RGW daemons as an <code>/etc/ceph/ceph.conf</code> file.</p>  <p>WARNING: Rook performs no validation on the config, so the  validity of the settings is the user's responsibility.</p>  <p>If the <code>rook-config-override</code> ConfigMap is created before the cluster is started, the Ceph daemons will automatically pick up the settings. If you add the settings to the ConfigMap after the cluster has been initialized, each daemon will need to be restarted where you want the settings applied:</p> <ul> <li>mons: ensure all three mons are online and healthy before restarting each mon pod, one at a time.</li> <li>mgrs: the pods are stateless and can be restarted as needed, but note that this will disrupt the   Ceph dashboard during restart.</li> <li>OSDs: restart your the pods by deleting them, one at a time, and running <code>ceph -s</code> between each restart to ensure the cluster goes back to \"active/clean\" state.</li> <li>RGW: the pods are stateless and can be restarted as needed.</li> <li>MDS: the pods are stateless and can be restarted as needed.</li> </ul> <p>After the pod restart, the new settings should be in effect. Note that if the ConfigMap in the Ceph cluster's namespace is created before the cluster is created, the daemons will pick up the settings at first launch.</p> <p>To automate the restart of the Ceph daemon pods, you will need to trigger an update to the pod specs. The simplest way to trigger the update is to add annotations or labels to the CephCluster CR for the daemons you want to restart. The operator will then proceed with a rolling update, similar to any other update to the cluster.</p>","title":"Custom ceph.conf Settings"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#example","text":"<p>In this example we will set the default pool <code>size</code> to two, and tell OSD daemons not to change the weight of OSDs on startup.</p>  <p>WARNING: Modify Ceph settings carefully. You are leaving the sandbox tested by Rook. Changing the settings could result in unhealthy daemons or even data loss if used incorrectly.</p>  <p>When the Rook Operator creates a cluster, a placeholder ConfigMap is created that will allow you to override Ceph configuration settings. When the daemon pods are started, the settings specified in this ConfigMap will be merged with the default settings generated by Rook.</p> <p>The default override settings are blank. Cutting out the extraneous properties, we would see the following defaults after creating a cluster:</p> <pre><code>kubectl -n rook-ceph get ConfigMap rook-config-override -o yaml\n</code></pre>  <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: rook-config-override\n  namespace: rook-ceph\ndata:\n  config: \"\"\n</code></pre>  <p>To apply your desired configuration, you will need to update this ConfigMap. The next time the daemon pod(s) start, they will use the updated configs.</p> <pre><code>kubectl -n rook-ceph edit configmap rook-config-override\n</code></pre>  <p>Modify the settings and save. Each line you add should be indented from the <code>config</code> property as such:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rook-config-override\n  namespace: rook-ceph\ndata:\n  config: |\n    [global]\n    osd crush update on start = false\n    osd pool default size = 2\n</code></pre>","title":"Example"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#custom-csi-cephconf-settings","text":"<p>WARNING: It is highly recommended to use the default setting that comes with CephCSI and this can only be used when absolutely necessary. The <code>ceph.conf</code> should be reset back to default values if/when the configurations are no longer necessary.</p>  <p>If the <code>csi-ceph-conf-override</code> ConfigMap is created before the cluster is started, the CephCSI pods will automatically pick up the settings. If you add the settings to the ConfigMap after the cluster has been initialized, you can restart the Rook operator pod and wait for Rook to recreate CSI pods to take immediate effect.</p> <p>After the CSI pods are restarted, the new settings should be in effect.</p>","title":"Custom CSI ceph.conf Settings"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#example_1","text":"<p>In this Example we will set the <code>rbd_validate_pool</code> to <code>false</code> to skip rbd pool validation.</p>  <p>WARNING: Modify Ceph settings carefully to avoid modifying the default configuration. Changing the settings could result in unexpected results if used incorrectly.</p>  <pre><code>kubectl create -f csi-ceph-conf-override.yaml\n</code></pre>  <p>Restart the Rook operator pod and wait for CSI pods to be recreated.</p>","title":"Example"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#osd-crush-settings","text":"<p>A useful view of the CRUSH Map is generated with the following command:</p> <pre><code>ceph osd tree\n</code></pre>  <p>In this section we will be tweaking some of the values seen in the output.</p>","title":"OSD CRUSH Settings"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#osd-weight","text":"<p>The CRUSH weight controls the ratio of data that should be distributed to each OSD.  This also means a higher or lower amount of disk I/O operations for an OSD with higher/lower weight, respectively.</p> <p>By default OSDs get a weight relative to their storage capacity, which maximizes overall cluster capacity by filling all drives at the same rate, even if drive sizes vary.  This should work for most use-cases, but the following situations could warrant weight changes:</p> <ul> <li>Your cluster has some relatively slow OSDs or nodes. Lowering their weight can   reduce the impact of this bottleneck.</li> <li>You're using bluestore drives provisioned with Rook v0.3.1 or older.  In this   case you may notice OSD weights did not get set relative to their storage   capacity.  Changing the weight can fix this and maximize cluster capacity.</li> </ul> <p>This example sets the weight of osd.0 which is 600GiB</p> <pre><code>ceph osd crush reweight osd.0 .600\n</code></pre>","title":"OSD Weight"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#osd-primary-affinity","text":"<p>When pools are set with a size setting greater than one, data is replicated between nodes and OSDs.  For every chunk of data a Primary OSD is selected to be used for reading that data to be sent to clients.  You can control how likely it is for an OSD to become a Primary using the Primary Affinity setting.  This is similar to the OSD weight setting, except it only affects reads on the storage device, not capacity or writes.</p> <p>In this example we will make sure <code>osd.0</code> is only selected as Primary if all other OSDs holding replica data are unavailable:</p> <pre><code>ceph osd primary-affinity osd.0 0\n</code></pre>","title":"OSD Primary Affinity"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#osd-dedicated-network","text":"<p>It is possible to configure ceph to leverage a dedicated network for the OSDs to communicate across. A useful overview is the CEPH Networks section of the Ceph documentation. If you declare a cluster network, OSDs will route heartbeat, object replication and recovery traffic over the cluster network. This may improve performance compared to using a single network.</p> <p>Two changes are necessary to the configuration to enable this capability:</p>","title":"OSD Dedicated Network"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#use-hostnetwork-in-the-rook-ceph-cluster-configuration","text":"<p>Enable the <code>hostNetwork</code> setting in the Ceph Cluster CRD configuration. For example,</p> <pre><code>  network:\n    provider: host\n</code></pre>   <p>IMPORTANT: Changing this setting is not supported in a running Rook cluster. Host networking should be configured when the cluster is first created.</p>","title":"Use hostNetwork in the rook ceph cluster configuration"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#define-the-subnets-to-use-for-public-and-private-osd-networks","text":"<p>Edit the <code>rook-config-override</code> configmap to define the custom network configuration:</p> <pre><code>kubectl -n rook-ceph edit configmap rook-config-override\n</code></pre>  <p>In the editor, add a custom configuration to instruct ceph which subnet is the public network and which subnet is the private network. For example:</p> <pre><code>apiVersion: v1\ndata:\n  config: |\n    [global]\n    public network =  10.0.7.0/24\n    cluster network = 10.0.10.0/24\n    public addr = \"\"\n    cluster addr = \"\"\n</code></pre>  <p>After applying the updated rook-config-override configmap, it will be necessary to restart the OSDs by deleting the OSD pods in order to apply the change. Restart the OSD pods by deleting them, one at a time, and running ceph -s between each restart to ensure the cluster goes back to \"active/clean\" state.</p>","title":"Define the subnets to use for public and private OSD networks"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#phantom-osd-removal","text":"<p>If you have OSDs in which are not showing any disks, you can remove those \"Phantom OSDs\" by following the instructions below. To check for \"Phantom OSDs\", you can run:</p> <pre><code>ceph osd tree\n</code></pre>  <p>An example output looks like this:</p>  <pre><code>ID  CLASS WEIGHT  TYPE NAME STATUS REWEIGHT PRI-AFF\n-1       57.38062 root default\n-13        7.17258     host node1.example.com\n2   hdd  3.61859         osd.2                up  1.00000 1.00000\n-7              0     host node2.example.com   down    0    1.00000\n</code></pre>   <p>The host <code>node2.example.com</code> in the output has no disks, so it is most likely a \"Phantom OSD\".</p> <p>Now to remove it, use the ID in the first column of the output and replace <code>&lt;ID&gt;</code> with it. In the example output above the ID would be <code>-7</code>. The commands are:</p> <pre><code>$ ceph osd out &lt;ID&gt;\n$ ceph osd crush remove osd.&lt;ID&gt;\n$ ceph auth del osd.&lt;ID&gt;\n$ ceph osd rm &lt;ID&gt;\n</code></pre>  <p>To recheck that the Phantom OSD was removed, re-run the following command and check if the OSD with the ID doesn't show up anymore:</p> <pre><code>ceph osd tree\n</code></pre>","title":"Phantom OSD Removal"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#auto-expansion-of-osds","text":"","title":"Auto Expansion of OSDs"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#prerequisites_1","text":"<p>1) A PVC-based cluster deployed in dynamic provisioning environment with a <code>storageClassDeviceSet</code>.</p> <p>2) Create the Rook Toolbox.</p>  <p>Note: Prometheus Operator and Prometheus Instances are Prerequisites that are created by the auto-grow-storage script.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#to-scale-osds-vertically","text":"<p>Run the following script to auto-grow the size of OSDs on a PVC-based Rook-Ceph cluster whenever the OSDs have reached the storage near-full threshold. <pre><code>tests/scripts/auto-grow-storage.sh size  --max maxSize --growth-rate percent\n</code></pre> </p>  <p>growth-rate percentage represents the percent increase you want in the OSD capacity and maxSize represent the maximum disk size.</p>  <p>For example, if you need to increase the size of OSD by 30% and max disk size is 1Ti <pre><code>./auto-grow-storage.sh size  --max 1Ti --growth-rate 30\n</code></pre> </p>","title":"To scale OSDs Vertically"},{"location":"Usages-and-Examples/ceph-advanced-configuration/#to-scale-osds-horizontally","text":"<p>Run the following script to auto-grow the number of OSDs on a PVC-based Rook-Ceph cluster whenever the OSDs have reached the storage near-full threshold. <pre><code>tests/scripts/auto-grow-storage.sh count --max maxCount --count rate\n</code></pre> </p>  <p>Count of OSD represents the number of OSDs you need to add and maxCount represents the number of disks a storage cluster will support.</p>  <p>For example, if you need to increase the number of OSDs by 3 and maxCount is 10 <pre><code>./auto-grow-storage.sh count --max 10 --count 3\n</code></pre> </p>","title":"To scale OSDs Horizontally"},{"location":"Usages-and-Examples/ceph-dashboard/","text":"<p>The dashboard is a very helpful tool to give you an overview of the status of your Ceph cluster, including overall health, status of the mon quorum, status of the mgr, osd, and other Ceph daemons, view pools and PG status, show logs for the daemons, and more. Rook makes it simple to enable the dashboard.</p> <p></p>","title":"Ceph Dashboard"},{"location":"Usages-and-Examples/ceph-dashboard/#enable-the-ceph-dashboard","text":"<p>The dashboard can be enabled with settings in the CephCluster CRD. The CephCluster CRD must have the dashboard <code>enabled</code> setting set to <code>true</code>. This is the default setting in the example manifests.</p> <pre><code>  spec:\n    dashboard:\n      enabled: true\n</code></pre>  <p>The Rook operator will enable the ceph-mgr dashboard module. A service object will be created to expose that port inside the Kubernetes cluster. Rook will enable port 8443 for https access.</p> <p>This example shows that port 8443 was configured.</p> <pre><code>kubectl -n rook-ceph get service\n</code></pre>   <pre><code>NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nrook-ceph-mgr                ClusterIP   10.108.111.192   &lt;none&gt;        9283/TCP         3h\nrook-ceph-mgr-dashboard      ClusterIP   10.110.113.240   &lt;none&gt;        8443/TCP         3h\n</code></pre>   <p>The first service is for reporting the Prometheus metrics, while the latter service is for the dashboard. If you are on a node in the cluster, you will be able to connect to the dashboard by using either the DNS name of the service at <code>https://rook-ceph-mgr-dashboard-https:8443</code> or by connecting to the cluster IP, in this example at <code>https://10.110.113.240:8443</code>.</p>  <p>IMPORTANT: Please note the dashboard will only be enabled for the first Ceph object store created by Rook.</p>","title":"Enable the Ceph Dashboard"},{"location":"Usages-and-Examples/ceph-dashboard/#login-credentials","text":"<p>After you connect to the dashboard you will need to login for secure access. Rook creates a default user named <code>admin</code> and generates a secret called <code>rook-ceph-dashboard-password</code> in the namespace where the Rook Ceph cluster is running. To retrieve the generated password, you can run the following:</p> <pre><code>kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echo\n</code></pre>","title":"Login Credentials"},{"location":"Usages-and-Examples/ceph-dashboard/#configure-the-dashboard","text":"<p>The following dashboard configuration settings are supported:</p> <pre><code>  spec:\n    dashboard:\n      urlPrefix: /ceph-dashboard\n      port: 8443\n      ssl: true\n</code></pre>  <ul> <li><code>urlPrefix</code> If you are accessing the dashboard via a reverse proxy, you may   wish to serve it under a URL prefix.  To get the dashboard to use hyperlinks   that include your prefix, you can set the <code>urlPrefix</code> setting.</li> <li><code>port</code> The port that the dashboard is served on may be changed from the   default using the <code>port</code> setting. The corresponding K8s service exposing the   port will automatically be updated.</li> <li><code>ssl</code> The dashboard may be served without SSL (useful for when you deploy the   dashboard behind a proxy already served using SSL) by setting the <code>ssl</code> option   to be false.</li> </ul>","title":"Configure the Dashboard"},{"location":"Usages-and-Examples/ceph-dashboard/#viewing-the-dashboard-external-to-the-cluster","text":"<p>Commonly you will want to view the dashboard from outside the cluster. For example, on a development machine with the cluster running inside minikube you will want to access the dashboard from the host.</p> <p>There are several ways to expose a service that will depend on the environment you are running in. You can use an Ingress Controller or other methods for exposing services such as NodePort, LoadBalancer, or ExternalIPs.</p>","title":"Viewing the Dashboard External to the Cluster"},{"location":"Usages-and-Examples/ceph-dashboard/#node-port","text":"<p>The simplest way to expose the service in minikube or similar environment is using the NodePort to open a port on the VM that can be accessed by the host. To create a service with the NodePort, save this yaml as <code>dashboard-external-https.yaml</code>.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: rook-ceph-mgr-dashboard-external-https\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-mgr\n    rook_cluster: rook-ceph\nspec:\n  ports:\n  - name: dashboard\n    port: 8443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    app: rook-ceph-mgr\n    rook_cluster: rook-ceph\n  sessionAffinity: None\n  type: NodePort\n</code></pre>  <p>Now create the service:</p> <pre><code>kubectl create -f dashboard-external-https.yaml\n</code></pre>  <p>You will see the new service <code>rook-ceph-mgr-dashboard-external-https</code> created:</p> <pre><code>kubectl -n rook-ceph get service\n</code></pre>   <pre><code>NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nrook-ceph-mgr                           ClusterIP   10.108.111.192   &lt;none&gt;        9283/TCP         4h\nrook-ceph-mgr-dashboard                 ClusterIP   10.110.113.240   &lt;none&gt;        8443/TCP         4h\nrook-ceph-mgr-dashboard-external-https  NodePort    10.101.209.6     &lt;none&gt;        8443:31176/TCP   4h\n</code></pre>   <p>In this example, port <code>31176</code> will be opened to expose port <code>8443</code> from the ceph-mgr pod. Find the ip address of the VM. If using minikube, you can run <code>minikube ip</code> to find the ip address. Now you can enter the URL in your browser such as <code>https://192.168.99.110:31176</code> and the dashboard will appear.</p>","title":"Node Port"},{"location":"Usages-and-Examples/ceph-dashboard/#load-balancer","text":"<p>If you have a cluster on a cloud provider that supports load balancers, you can create a service that is provisioned with a public hostname. The yaml is the same as <code>dashboard-external-https.yaml</code> except for the following property:</p> <pre><code>spec:\n[...]\n  type: LoadBalancer\n</code></pre>  <p>Now create the service:</p> <pre><code>kubectl create -f dashboard-loadbalancer.yaml\n</code></pre>  <p>You will see the new service <code>rook-ceph-mgr-dashboard-loadbalancer</code> created:</p> <pre><code>kubectl -n rook-ceph get service\n</code></pre>   <pre><code>NAME                                     TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)             AGE\nrook-ceph-mgr                            ClusterIP      172.30.11.40     &lt;none&gt;                                                                    9283/TCP            4h\nrook-ceph-mgr-dashboard                  ClusterIP      172.30.203.185   &lt;none&gt;                                                                    8443/TCP            4h\nrook-ceph-mgr-dashboard-loadbalancer     LoadBalancer   172.30.27.242    a7f23e8e2839511e9b7a5122b08f2038-1251669398.us-east-1.elb.amazonaws.com   8443:32747/TCP      4h\n</code></pre>   <p>Now you can enter the URL in your browser such as <code>https://a7f23e8e2839511e9b7a5122b08f2038-1251669398.us-east-1.elb.amazonaws.com:8443</code> and the dashboard will appear.</p>","title":"Load Balancer"},{"location":"Usages-and-Examples/ceph-dashboard/#ingress-controller","text":"<p>If you have a cluster with an nginx Ingress Controller and a Certificate Manager (e.g. cert-manager) then you can create an Ingress like the one below. This example achieves four things:</p> <ol> <li>Exposes the dashboard on the Internet (using an reverse proxy)</li> <li>Issues an valid TLS Certificate for the specified domain name (using ACME)</li> <li>Tells the reverse proxy that the dashboard itself uses HTTPS</li> <li>Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed)</li> </ol> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rook-ceph-mgr-dashboard\n  namespace: rook-ceph\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    kubernetes.io/tls-acme: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/server-snippet: |\n      proxy_ssl_verify off;\nspec:\n  tls:\n   - hosts:\n     - rook-ceph.example.com\n     secretName: rook-ceph.example.com\n  rules:\n  - host: rook-ceph.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: rook-ceph-mgr-dashboard\n            port:\n              name: https-dashboard\n</code></pre>  <p>Customise the Ingress resource to match your cluster. Replace the example domain name <code>rook-ceph.example.com</code> with a domain name that will resolve to your Ingress Controller (creating the DNS entry if required).</p> <p>Now create the Ingress:</p> <pre><code>kubectl create -f dashboard-ingress-https.yaml\n</code></pre>  <p>You will see the new Ingress <code>rook-ceph-mgr-dashboard</code> created:</p> <pre><code>kubectl -n rook-ceph get ingress\n</code></pre>   <pre><code>NAME                      HOSTS                      ADDRESS   PORTS     AGE\nrook-ceph-mgr-dashboard   rook-ceph.example.com      80, 443   5m\n</code></pre>   <p>And the new Secret for the TLS certificate:</p> <pre><code>kubectl -n rook-ceph get secret rook-ceph.example.com\n</code></pre>   <pre><code>NAME                       TYPE                DATA      AGE\nrook-ceph.example.com      kubernetes.io/tls   2         4m\n</code></pre>   <p>You can now browse to <code>https://rook-ceph.example.com/</code> to log into the dashboard.</p>","title":"Ingress Controller"},{"location":"Usages-and-Examples/ceph-examples/","text":"<p>Configuration for Rook and Ceph can be configured in multiple ways to provide block devices, shared filesystem volumes or object storage in a kubernetes namespace. We have provided several examples to simplify storage setup, but remember there are many tunables and you will need to decide what settings work for your use case and environment.</p> <p>See the example yaml files folder for all the rook/ceph setup example spec files.</p>","title":"Ceph Examples"},{"location":"Usages-and-Examples/ceph-examples/#common-resources","text":"<p>The first step to deploy Rook is to create the CRDs and other common resources. The configuration for these resources will be the same for most deployments. The crds.yaml and common.yaml sets these resources up.</p> <pre><code>kubectl create -f crds.yaml -f common.yaml\n</code></pre>  <p>The examples all assume the operator and all Ceph daemons will be started in the same namespace. If you want to deploy the operator in a separate namespace, see the comments throughout <code>common.yaml</code>.</p>","title":"Common Resources"},{"location":"Usages-and-Examples/ceph-examples/#operator","text":"<p>After the common resources are created, the next step is to create the Operator deployment. Several spec file examples are provided in this directory:</p> <ul> <li><code>operator.yaml</code>: The most common settings for production deployments</li> <li><code>kubectl create -f operator.yaml</code></li> <li><code>operator-openshift.yaml</code>: Includes all of the operator settings for running a basic Rook cluster in an OpenShift environment. You will also want to review the OpenShift Prerequisites to confirm the settings.</li> <li><code>oc create -f operator-openshift.yaml</code></li> </ul> <p>Settings for the operator are configured through environment variables on the operator deployment. The individual settings are documented in operator.yaml.</p>","title":"Operator"},{"location":"Usages-and-Examples/ceph-examples/#cluster-crd","text":"<p>Now that your operator is running, let's create your Ceph storage cluster. This CR contains the most critical settings that will influence how the operator configures the storage. It is important to understand the various ways to configure the cluster. These examples represent a very small set of the different ways to configure the storage.</p> <ul> <li><code>cluster.yaml</code>: This file contains common settings for a production storage cluster. Requires at least three worker nodes.</li> <li><code>cluster-test.yaml</code>: Settings for a test cluster where redundancy is not configured. Requires only a single node.</li> <li><code>cluster-on-pvc.yaml</code>: This file contains common settings for backing the Ceph Mons and OSDs by PVs. Useful when running in cloud environments or where local PVs have been created for Ceph to consume.</li> <li><code>cluster-external.yaml</code>: Connect to an external Ceph cluster with minimal access to monitor the health of the cluster and connect to the storage.</li> <li><code>cluster-external-management.yaml</code>: Connect to an external Ceph cluster with the admin key of the external cluster to enable   remote creation of pools and configure services such as an Object Store or a Shared Filesystem.</li> <li><code>cluster-stretched.yaml</code>: Create a cluster in \"stretched\" mode, with five mons stretched across three zones, and the OSDs across two zones. See the Stretch documentation.</li> </ul> <p>See the Cluster CRD topic for more details and more examples for the settings.</p>","title":"Cluster CRD"},{"location":"Usages-and-Examples/ceph-examples/#setting-up-consumable-storage","text":"<p>Now we are ready to setup block, shared filesystem or object storage in the Rook Ceph cluster. These kinds of storage are respectively referred to as CephBlockPool, CephFilesystem and CephObjectStore in the spec files.</p>","title":"Setting up consumable storage"},{"location":"Usages-and-Examples/ceph-examples/#block-devices","text":"<p>Ceph can provide raw block device volumes to pods. Each example below sets up a storage class which can then be used to provision a block device in kubernetes pods. The storage class is defined with a pool which defines the level of data redundancy in Ceph:</p> <ul> <li><code>storageclass.yaml</code>: This example illustrates replication of 3 for production scenarios and requires at least three worker nodes. Your data is replicated on three different kubernetes worker nodes and intermittent or long-lasting single node failures will not result in data unavailability or loss.</li> <li><code>storageclass-ec.yaml</code>: Configures erasure coding for data durability rather than replication. Ceph's erasure coding is more efficient than replication so you can get high reliability without the 3x replication cost of the preceding example (but at the cost of higher computational encoding and decoding costs on the worker nodes). Erasure coding requires at least three worker nodes. See the Erasure coding documentation for more details.</li> <li><code>storageclass-test.yaml</code>: Replication of 1 for test scenarios and it requires only a single node. Do not use this for applications that store valuable data or have high-availability storage requirements, since a single node failure can result in data loss.</li> </ul> <p>The storage classes are found in different sub-directories depending on the driver:</p> <ul> <li><code>csi/rbd</code>: The CSI driver for block devices. This is the preferred driver going forward.</li> </ul> <p>See the Ceph Pool CRD topic for more details on the settings.</p>","title":"Block Devices"},{"location":"Usages-and-Examples/ceph-examples/#shared-filesystem","text":"<p>Ceph filesystem (CephFS) allows the user to 'mount' a shared posix-compliant folder into one or more hosts (pods in the container world). This storage is similar to NFS shared storage or CIFS shared folders, as explained here.</p> <p>File storage contains multiple pools that can be configured for different scenarios:</p> <ul> <li><code>filesystem.yaml</code>: Replication of 3 for production scenarios. Requires at least three worker nodes.</li> <li><code>filesystem-ec.yaml</code>: Erasure coding for production scenarios. Requires at least three worker nodes.</li> <li><code>filesystem-test.yaml</code>: Replication of 1 for test scenarios. Requires only a single node.</li> </ul> <p>Dynamic provisioning is possible with the CSI driver. The storage class for shared filesystems is found in the <code>csi/cephfs</code> directory.</p> <p>See the Shared Filesystem CRD topic for more details on the settings.</p>","title":"Shared Filesystem"},{"location":"Usages-and-Examples/ceph-examples/#object-storage","text":"<p>Ceph supports storing blobs of data called objects that support HTTP(s)-type get/put/post and delete semantics. This storage is similar to AWS S3 storage, for example.</p> <p>Object storage contains multiple pools that can be configured for different scenarios:</p> <ul> <li><code>object.yaml</code>: Replication of 3 for production scenarios.  Requires at least three worker nodes.</li> <li><code>object-openshift.yaml</code>: Replication of 3 with rgw in a port range valid for OpenShift. Requires at least three worker nodes.</li> <li><code>object-ec.yaml</code>: Erasure coding rather than replication for production scenarios. Requires at least three worker nodes.</li> <li><code>object-test.yaml</code>: Replication of 1 for test scenarios. Requires only a single node.</li> </ul> <p>See the Object Store CRD topic for more details on the settings.</p>","title":"Object Storage"},{"location":"Usages-and-Examples/ceph-examples/#object-storage-user","text":"<ul> <li><code>object-user.yaml</code>: Creates a simple object storage user and generates credentials for the S3 API</li> </ul>","title":"Object Storage User"},{"location":"Usages-and-Examples/ceph-examples/#object-storage-buckets","text":"<p>The Ceph operator also runs an object store bucket provisioner which can grant access to existing buckets or dynamically provision new buckets.</p> <ul> <li>object-bucket-claim-retain.yaml Creates a request for a new bucket by referencing a StorageClass which saves the bucket when the initiating OBC is deleted.</li> <li>object-bucket-claim-delete.yaml Creates a request for a new bucket by referencing a StorageClass which deletes the bucket when the initiating OBC is deleted.</li> <li>storageclass-bucket-retain.yaml Creates a new StorageClass which defines the Ceph Object Store and retains the bucket after the initiating OBC is deleted.</li> <li>storageclass-bucket-delete.yaml Creates a new StorageClass which defines the Ceph Object Store and deletes the bucket after the initiating OBC is deleted.</li> </ul>","title":"Object Storage Buckets"},{"location":"Usages-and-Examples/ceph-monitoring/","text":"<p>Each Rook Ceph cluster has some built in metrics collectors/exporters for monitoring with Prometheus.</p> <p>If you do not have Prometheus running, follow the steps below to enable monitoring of Rook. If your cluster already contains a Prometheus instance, it will automatically discover Rook's scrape endpoint using the standard <code>prometheus.io/scrape</code> and <code>prometheus.io/port</code> annotations.</p>  <p>NOTE: This assumes that the Prometheus instances is searching all your Kubernetes namespaces for Pods with these annotations. If prometheus is already installed in a cluster, it may not be configured to watch for third-party service monitors such as for Rook. Normally you should be able to add the prometheus annotations \"prometheus.io/scrape=true\" and prometheus.io/port={port} and prometheus would automatically configure the scrape points and start gathering metrics. If prometheus isn't configured to do this, see the prometheus operator docs.</p>","title":"Prometheus Monitoring"},{"location":"Usages-and-Examples/ceph-monitoring/#prometheus-operator","text":"<p>First the Prometheus operator needs to be started in the cluster so it can watch for our requests to start monitoring Rook and respond by deploying the correct Prometheus pods and configuration. A full explanation can be found in the Prometheus operator repository on GitHub, but the quick instructions can be found here:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml\n</code></pre>  <p>This will start the Prometheus operator, but before moving on, wait until the operator is in the <code>Running</code> state:</p> <pre><code>kubectl get pod\n</code></pre>  <p>Once the Prometheus operator is in the <code>Running</code> state, proceed to the next section to create a Prometheus instance.</p>","title":"Prometheus Operator"},{"location":"Usages-and-Examples/ceph-monitoring/#prometheus-instances","text":"<p>With the Prometheus operator running, we can create a service monitor that will watch the Rook cluster and collect metrics regularly. From the root of your locally cloned Rook repo, go the monitoring directory:</p> <pre><code>$ git clone --single-branch --branch {{ branchName }} https://github.com/rook/rook.git\ncd rook/deploy/examples/monitoring\n</code></pre>  <p>Create the service monitor as well as the Prometheus server pod and service:</p> <pre><code>kubectl create -f service-monitor.yaml\nkubectl create -f prometheus.yaml\nkubectl create -f prometheus-service.yaml\n</code></pre>  <p>Ensure that the Prometheus server pod gets created and advances to the <code>Running</code> state before moving on:</p> <pre><code>kubectl -n rook-ceph get pod prometheus-rook-prometheus-0\n</code></pre>   <p>NOTE: It is not recommended to consume storage from the Ceph cluster for Prometheus. If the Ceph cluster fails, Prometheus would become unresponsive and thus not alert you of the failure.</p>","title":"Prometheus Instances"},{"location":"Usages-and-Examples/ceph-monitoring/#prometheus-web-console","text":"<p>Once the Prometheus server is running, you can open a web browser and go to the URL that is output from this command:</p> <pre><code>echo \"http://$(kubectl -n rook-ceph -o jsonpath={.status.hostIP} get pod prometheus-rook-prometheus-0):30900\"\n</code></pre>  <p>You should now see the Prometheus monitoring website.</p> <p></p> <p>Click on <code>Graph</code> in the top navigation bar.</p> <p></p> <p>In the dropdown that says <code>insert metric at cursor</code>, select any metric you would like to see, for example <code>ceph_cluster_total_used_bytes</code></p> <p></p> <p>Click on the <code>Execute</code> button.</p> <p></p> <p>Below the <code>Execute</code> button, ensure the <code>Graph</code> tab is selected and you should now see a graph of your chosen metric over time.</p> <p></p>","title":"Prometheus Web Console"},{"location":"Usages-and-Examples/ceph-monitoring/#prometheus-consoles","text":"<p>You can find Prometheus Consoles for and from Ceph here: GitHub ceph/cephmetrics - dashboards/current directory.</p> <p>A guide to how you can write your own Prometheus consoles can be found on the official Prometheus site here: Prometheus.io Documentation - Console Templates.</p>","title":"Prometheus Consoles"},{"location":"Usages-and-Examples/ceph-monitoring/#prometheus-alerts","text":"<p>To enable the Ceph Prometheus alerts via the helm charts, set the following properties in values.yaml: - rook-ceph chart: <code>monitoring.enabled: true</code> - rook-ceph-cluster chart: <code>monitoring.enabled: true</code> <code>monitoring.createPrometheusRules: true</code></p> <p>Alternatively, to enable the Ceph Prometheus alerts with example manifests follow these steps:</p> <ol> <li>Create the RBAC and prometheus rules:</li> </ol> <pre><code>kubectl create -f deploy/examples/monitoring/rbac.yaml\nkubectl create -f deploy/examples/monitoring/localrules.yaml\n</code></pre>  <ol> <li>Make following changes to your CephCluster object (e.g., <code>cluster.yaml</code>).</li> </ol> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\n[...]\nspec:\n[...]\n  monitoring:\n    enabled: true\n[...]\n</code></pre>  <ol> <li>Deploy or update the CephCluster object.</li> </ol> <pre><code>kubectl apply -f cluster.yaml\n</code></pre>   <p>NOTE: This expects the Prometheus Operator and a Prometheus instance to be pre-installed by the admin.</p>","title":"Prometheus Alerts"},{"location":"Usages-and-Examples/ceph-monitoring/#customize-alerts","text":"<p>The Prometheus alerts can be customized with a post-processor using tools such as Kustomize. For example, first extract the helm chart:</p> <pre><code>helm template -f values.yaml rook-release/rook-ceph-cluster &gt; cluster-chart.yaml\n</code></pre>  <p>Now create the desired customization configuration files. This simple example will show how to update the severity of a rule, add a label to a rule, and change the <code>for</code> time value.</p> <p>Create a file named kustomization.yaml:</p> <pre><code>patches:\n- path: modifications.yaml\n  target:\n    group: monitoring.coreos.com\n    kind: PrometheusRule\n    name: prometheus-ceph-rules\n    version: v1\nresources:\n- cluster-chart.yaml\n</code></pre>  <p>Create a file named modifications.yaml</p> <pre><code>- op: add\n  path: /spec/groups/0/rules/0/labels\n  value:\n    my-label: foo\n    severity: none\n- op: add\n  path: /spec/groups/0/rules/0/for\n  value: 15m\n</code></pre>  <p>Finally, run kustomize to update the desired prometheus rules:</p> <pre><code>kustomize build . &gt; updated-chart.yaml\nkubectl create -f updated-chart.yaml\n</code></pre>","title":"Customize Alerts"},{"location":"Usages-and-Examples/ceph-monitoring/#grafana-dashboards","text":"<p>The dashboards have been created by @galexrt. For feedback on the dashboards please reach out to him on the Rook.io Slack.</p>  <p>NOTE: The dashboards are only compatible with Grafana 7.2.0 or higher.</p> <p>Also note that the dashboards are updated from time to time, to fix issues and improve them.</p>  <p>The following Grafana dashboards are available:</p> <ul> <li>Ceph - Cluster</li> <li>Ceph - OSD (Single)</li> <li>Ceph - Pools</li> </ul>","title":"Grafana Dashboards"},{"location":"Usages-and-Examples/ceph-monitoring/#updates-and-upgrades","text":"<p>When updating Rook, there may be updates to RBAC for monitoring. It is easy to apply the changes with each update or upgrade. This should be done at the same time you update Rook common resources like <code>common.yaml</code>.</p> <pre><code>kubectl apply -f deploy/examples/monitoring/rbac.yaml\n</code></pre>   <p>This is updated automatically if you are upgrading via the helm chart</p>","title":"Updates and Upgrades"},{"location":"Usages-and-Examples/ceph-monitoring/#teardown","text":"<p>To clean up all the artifacts created by the monitoring walk-through, copy/paste the entire block below (note that errors about resources \"not found\" can be ignored):</p> <pre><code>kubectl delete -f service-monitor.yaml\nkubectl delete -f prometheus.yaml\nkubectl delete -f prometheus-service.yaml\nkubectl delete -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml\n</code></pre>  <p>Then the rest of the instructions in the Prometheus Operator docs can be followed to finish cleaning up.</p>","title":"Teardown"},{"location":"Usages-and-Examples/ceph-monitoring/#special-cases","text":"","title":"Special Cases"},{"location":"Usages-and-Examples/ceph-monitoring/#tectonic-bare-metal","text":"<p>Tectonic strongly discourages the <code>tectonic-system</code> Prometheus instance to be used outside their intentions, so you need to create a new Prometheus Operator yourself. After this you only need to create the service monitor as stated above.</p>","title":"Tectonic Bare Metal"},{"location":"Usages-and-Examples/ceph-monitoring/#csi-liveness","text":"<p>To integrate CSI liveness and grpc into ceph monitoring we will need to deploy a service and service monitor.</p> <pre><code>kubectl create -f csi-metrics-service-monitor.yaml\n</code></pre>  <p>This will create the service monitor to have promethues monitor CSI</p>","title":"CSI Liveness"},{"location":"Usages-and-Examples/ceph-monitoring/#collecting-rbd-per-image-io-statistics","text":"<p>RBD per-image IO statistics collection is disabled by default. This can be enabled by setting <code>enableRBDStats: true</code> in the CephBlockPool spec. Prometheus does not need to be restarted after enabling it.</p>","title":"Collecting RBD per-image IO statistics"},{"location":"Usages-and-Examples/ceph-monitoring/#using-custom-label-selectors-in-prometheus","text":"<p>If Prometheus needs to select specific resources, we can do so by injecting labels into these objects and using it as label selector.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\n  [...]\nspec:\n  [...]\n  labels:\n    monitoring:\n      prometheus: k8s\n  [...]\n</code></pre>","title":"Using custom label selectors in Prometheus"},{"location":"Usages-and-Examples/ceph-monitoring/#horizontal-pod-scaling-using-kubernetes-event-driven-autoscaling-keda","text":"<p>Using metrics exported from the Prometheus service, the horizontal pod scaling can use the custom metrics other than CPU and memory consumption. It can be done with help of Prometheus Scaler provided by the KEDA. See the KEDA deployment guide for details.</p> <p>Following is an example to autoscale RGW: <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n name: rgw-scale\n namespace: rook-ceph\nspec:\n scaleTargetRef:\n   kind: Deployment\n   name: rook-ceph-rgw-my-store-a # deployment for the autoscaling\n minReplicaCount: 1\n maxReplicaCount: 5\n triggers:\n - type: prometheus\n   metadata:\n     serverAddress: http://rook-prometheus.rook-ceph.svc:9090\n     metricName: collecting_ceph_rgw_put\n     query: |\n       sum(rate(ceph_rgw_put[2m])) # promethues query used for autoscaling\n     threshold: \"90\"\n</code></pre> </p>","title":"Horizontal Pod Scaling using Kubernetes Event-driven Autoscaling (KEDA)"},{"location":"Usages-and-Examples/ceph-teardown/","text":"<p>If you want to tear down the cluster and bring up a new one, be aware of the following resources that will need to be cleaned up:</p> <ul> <li><code>rook-ceph</code> namespace: The Rook operator and cluster created by <code>operator.yaml</code> and <code>cluster.yaml</code> (the cluster CRD)</li> <li><code>/var/lib/rook</code>: Path on each host in the cluster where configuration is cached by the ceph mons and osds</li> </ul> <p>Note that if you changed the default namespaces or paths such as <code>dataDirHostPath</code> in the sample yaml files, you will need to adjust these namespaces and paths throughout these instructions.</p> <p>If you see issues tearing down the cluster, see the Troubleshooting section below.</p> <p>If you are tearing down a cluster frequently for development purposes, it is instead recommended to use an environment such as Minikube that can easily be reset without worrying about any of these steps.</p>","title":"Cleaning up a Cluster"},{"location":"Usages-and-Examples/ceph-teardown/#delete-the-block-and-file-artifacts","text":"<p>First you will need to clean up the resources created on top of the Rook cluster.</p> <p>These commands will clean up the resources from the block and file walkthroughs (unmount volumes, delete volume claims, etc). If you did not complete those parts of the walkthrough, you can skip these instructions:</p> <pre><code>kubectl delete -f ../wordpress.yaml\nkubectl delete -f ../mysql.yaml\nkubectl delete -n rook-ceph cephblockpool replicapool\nkubectl delete storageclass rook-ceph-block\nkubectl delete -f csi/cephfs/kube-registry.yaml\nkubectl delete storageclass csi-cephfs\n</code></pre>  <p>After those block and file resources have been cleaned up, you can then delete your Rook cluster. This is important to delete before removing the Rook operator and agent or else resources may not be cleaned up properly.</p>","title":"Delete the Block and File artifacts"},{"location":"Usages-and-Examples/ceph-teardown/#delete-the-cephcluster-crd","text":"<p>Edit the <code>CephCluster</code> and add the <code>cleanupPolicy</code></p> <p>WARNING: DATA WILL BE PERMANENTLY DELETED AFTER DELETING THE <code>CephCluster</code> CR WITH <code>cleanupPolicy</code>.</p> <pre><code>kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n</code></pre>  <p>Once the cleanup policy is enabled, any new configuration changes in the CephCluster will be blocked. Nothing will happen until the deletion of the CR is requested, so this <code>cleanupPolicy</code> change can still be reverted if needed.</p> <p>Checkout more details about the <code>cleanupPolicy</code> here</p> <p>Delete the <code>CephCluster</code> CR.</p> <pre><code>kubectl -n rook-ceph delete cephcluster rook-ceph\n</code></pre>  <p>Verify that the cluster CR has been deleted before continuing to the next step.</p> <pre><code>kubectl -n rook-ceph get cephcluster\n</code></pre>  <p>If the <code>cleanupPolicy</code> was applied, then wait for the <code>rook-ceph-cleanup</code> jobs to be completed on all the nodes. These jobs will perform the following operations: - Delete the directory <code>/var/lib/rook</code> (or the path specified by the <code>dataDirHostPath</code>) on all the nodes - Wipe the data on the drives on all the nodes where OSDs were running in this cluster</p> <p>Note: The cleanup jobs might not start if the resources created on top of Rook Cluster are not deleted completely. See</p>","title":"Delete the CephCluster CRD"},{"location":"Usages-and-Examples/ceph-teardown/#delete-the-operator-and-related-resources","text":"<p>This will begin the process of the Rook Ceph operator and all other resources being cleaned up. This includes related resources such as the agent and discover daemonsets with the following commands:</p> <pre><code>kubectl delete -f operator.yaml\nkubectl delete -f common.yaml\nkubectl delete -f crds.yaml\n</code></pre>  <p>If the <code>cleanupPolicy</code> was applied and the cleanup jobs have completed on all the nodes, then the cluster tear down has been successful. If you skipped adding the <code>cleanupPolicy</code> then follow the manual steps mentioned below to tear down the cluster.</p>","title":"Delete the Operator and related Resources"},{"location":"Usages-and-Examples/ceph-teardown/#delete-the-data-on-hosts","text":"<p>IMPORTANT: The final cleanup step requires deleting files on each host in the cluster. All files under the <code>dataDirHostPath</code> property specified in the cluster CRD will need to be deleted. Otherwise, inconsistent state will remain when a new cluster is started.</p>  <p>Connect to each machine and delete <code>/var/lib/rook</code>, or the path specified by the <code>dataDirHostPath</code>.</p> <p>In the future this step will not be necessary when we build on the K8s local storage feature.</p> <p>If you modified the demo settings, additional cleanup is up to you for devices, host paths, etc.</p>","title":"Delete the data on hosts"},{"location":"Usages-and-Examples/ceph-teardown/#zapping-devices","text":"<p>Disks on nodes used by Rook for osds can be reset to a usable state with methods suggested below. Note that these scripts are not one-size-fits-all. Please use them with discretion to ensure you are not removing data unrelated to Rook and/or Ceph.</p> <p>Disks can be zapped fairly easily. A single disk can usually be cleared with some or all of the steps below.</p> <pre><code>DISK=\"/dev/sdX\"\n\n# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)\nsgdisk --zap-all $DISK\n\n# Wipe a large portion of the beginning of the disk to remove more LVM metadata that may be present\ndd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync\n\n# SSDs may be better cleaned with blkdiscard instead of dd\nblkdiscard $DISK\n\n# Inform the OS of partition table changes\npartprobe $DISK\n</code></pre>  <p>Ceph can leave LVM and device mapper data that can lock the disks, preventing the disks from being used again. These steps can help to free up old Ceph disks for re-use. Note that this only needs to be run once on each node and assumes that all Ceph disks are being wiped. If only some disks are being wiped, you will have to manually determine which disks map to which device mapper devices.</p> <pre><code># This command hangs on some systems: with caution, 'dmsetup remove_all --force' can be used\nls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %\n\n# ceph-volume setup can leave ceph-&lt;UUID&gt; directories in /dev and /dev/mapper (unnecessary clutter)\nrm -rf /dev/ceph-*\nrm -rf /dev/mapper/ceph--*\n</code></pre>  <p>If disks are still reported locked, rebooting the node often helps clear LVM-related holds on disks.</p>","title":"Zapping Devices"},{"location":"Usages-and-Examples/ceph-teardown/#troubleshooting","text":"<p>If the cleanup instructions are not executed in the order above, or you otherwise have difficulty cleaning up the cluster, here are a few things to try.</p> <p>The most common issue cleaning up the cluster is that the <code>rook-ceph</code> namespace or the cluster CRD remain indefinitely in the <code>terminating</code> state. A namespace cannot be removed until all of its resources are removed, so look at which resources are pending termination.</p> <p>Look at the pods:</p> <pre><code>kubectl -n rook-ceph get pod\n</code></pre>  <p>If a pod is still terminating, you will need to wait or else attempt to forcefully terminate it (<code>kubectl delete pod &lt;name&gt;</code>).</p> <p>Now look at the cluster CRD:</p> <pre><code>kubectl -n rook-ceph get cephcluster\n</code></pre>  <p>If the cluster CRD still exists even though you have executed the delete command earlier, see the next section on removing the finalizer.</p>","title":"Troubleshooting"},{"location":"Usages-and-Examples/ceph-teardown/#removing-the-cluster-crd-finalizer","text":"<p>When a Cluster CRD is created, a finalizer is added automatically by the Rook operator. The finalizer will allow the operator to ensure that before the cluster CRD is deleted, all block and file mounts will be cleaned up. Without proper cleanup, pods consuming the storage will be hung indefinitely until a system reboot.</p> <p>The operator is responsible for removing the finalizer after the mounts have been cleaned up. If for some reason the operator is not able to remove the finalizer (i.e., the operator is not running anymore), you can delete the finalizer manually with the following command:</p> <pre><code>for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do\n    kubectl get -n rook-ceph \"$CRD\" -o name | \\\n    xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{\"metadata\":{\"finalizers\": [null]}}'\ndone\n</code></pre>  <p>This command will patch the following CRDs on v1.3:</p>  <pre><code> cephblockpools.ceph.rook.io\n cephclients.ceph.rook.io\n cephfilesystems.ceph.rook.io\n cephnfses.ceph.rook.io\n cephobjectstores.ceph.rook.io\n cephobjectstoreusers.ceph.rook.io\n</code></pre>   <p>Within a few seconds you should see that the cluster CRD has been deleted and will no longer block other cleanup such as deleting the <code>rook-ceph</code> namespace.</p> <p>If the namespace is still stuck in Terminating state, you can check which resources are holding up the deletion and remove the finalizers and delete those</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph\n</code></pre>","title":"Removing the Cluster CRD Finalizer"},{"location":"Usages-and-Examples/ceph-teardown/#remove-critical-resource-finalizers","text":"<p>Rook adds a finalizer <code>ceph.rook.io/disaster-protection</code> to resources critical to the Ceph cluster so that the resources will not be accidentally deleted.</p> <p>The operator is responsible for removing the finalizers when a CephCluster is deleted. If for some reason the operator is not able to remove the finalizers (i.e., the operator is not running anymore), you can remove the finalizers manually with the following commands:</p> <pre><code>kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{\"metadata\":{\"finalizers\": [null]}}'\nkubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{\"metadata\":{\"finalizers\": [null]}}'\n</code></pre>","title":"Remove critical resource finalizers"},{"location":"Usages-and-Examples/ceph-toolbox/","text":"<p>The Rook toolbox is a container with common tools used for rook debugging and testing. The toolbox is based on CentOS, so more tools of your choosing can be easily installed with <code>yum</code>.</p> <p>The toolbox can be run in two modes: 1. Interactive: Start a toolbox pod where you can connect and execute Ceph commands from a shell 2. One-time job: Run a script with Ceph commands and collect the results from the job log</p>  <p>Prerequisite: Before running the toolbox you should have a running Rook cluster deployed (see the Quickstart Guide).</p>","title":"Rook Toolbox"},{"location":"Usages-and-Examples/ceph-toolbox/#interactive-toolbox","text":"<p>The rook toolbox can run as a deployment in a Kubernetes cluster where you can connect and run arbitrary Ceph commands.</p> <p>Launch the rook-ceph-tools pod:</p> <pre><code>kubectl create -f deploy/examples/toolbox.yaml\n</code></pre>  <p>Wait for the toolbox pod to download its container and get to the <code>running</code> state:</p> <pre><code>kubectl -n rook-ceph rollout status deploy/rook-ceph-tools\n</code></pre>  <p>Once the rook-ceph-tools pod is running, you can connect to it with:</p> <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash\n</code></pre>  <p>All available tools in the toolbox are ready for your troubleshooting needs.</p> <p>Example:</p> <ul> <li><code>ceph status</code></li> <li><code>ceph osd status</code></li> <li><code>ceph df</code></li> <li><code>rados df</code></li> </ul> <p>When you are done with the toolbox, you can remove the deployment:</p> <pre><code>kubectl -n rook-ceph delete deploy/rook-ceph-tools\n</code></pre>","title":"Interactive Toolbox"},{"location":"Usages-and-Examples/ceph-toolbox/#toolbox-job","text":"<p>If you want to run Ceph commands as a one-time operation and collect the results later from the logs, you can run a script as a Kubernetes Job. The toolbox job will run a script that is embedded in the job spec. The script has the full flexibility of a bash script.</p> <p>In this example, the <code>ceph status</code> command is executed when the job is created. Create the toolbox job:</p> <pre><code>kubectl create -f deploy/examples/toolbox-job.yaml\n</code></pre>  <p>After the job completes, see the results of the script:</p> <pre><code>kubectl -n rook-ceph logs -l job-name=rook-ceph-toolbox-job\n</code></pre>","title":"Toolbox Job"},{"location":"Usages-and-Examples/ceph-tools/","text":"<p>Rook provides a number of tools and troubleshooting docs to help you manage your cluster.</p> <ul> <li>Toolbox: A pod from which you can run all of the tools to troubleshoot the storage cluster</li> <li>Common Issues: Common issues and their potential solutions</li> <li>OSD Management: Common configuration issues for Ceph OSDs such as adding and removing storage</li> <li>Direct Tools: Run ceph commands to test directly mounting block and file storage</li> <li>Advanced Configuration: Tips and tricks for configuring for cluster</li> <li>Openshift Common Issues: Common troubleshooting tips for OpenShift clusters</li> <li>Disaster Recovery: In the worst case scenario if the ceph mons lose quorum, follow these steps to recover</li> </ul>","title":"Ceph Tools"},{"location":"Usages-and-Examples/direct-tools/","text":"<p>Rook is designed with Kubernetes design principles from the ground up. This topic is going to escape the bounds of Kubernetes storage and show you how to use block and file storage directly from a pod without any of the Kubernetes magic. The purpose of this topic is to help you quickly test a new configuration, although it is not meant to be used in production. All of the benefits of Kubernetes storage including failover, detach, and attach will not be available. If your pod dies, your mount will die with it.</p>","title":"Direct Tools"},{"location":"Usages-and-Examples/direct-tools/#start-the-direct-mount-pod","text":"<p>To test mounting your Ceph volumes, start a pod with the necessary mounts. An example is provided in the examples test directory:</p> <pre><code>kubectl create -f deploy/examples/direct-mount.yaml\n</code></pre>  <p>After the pod is started, connect to it like this:</p> <pre><code>kubectl -n rook-ceph get pod -l app=rook-direct-mount\n$ kubectl -n rook-ceph exec -it &lt;pod&gt; bash\n</code></pre>","title":"Start the Direct Mount Pod"},{"location":"Usages-and-Examples/direct-tools/#block-storage-tools","text":"<p>After you have created a pool as described in the Block Storage topic, you can create a block image and mount it directly in a pod. This example will show how the Ceph rbd volume can be mounted in the direct mount pod.</p> <p>Create the Direct Mount Pod.</p> <p>Create a volume image (10MB):</p> <pre><code>rbd create replicapool/test --size 10\nrbd info replicapool/test\n\n# Disable the rbd features that are not in the kernel module\nrbd feature disable replicapool/test fast-diff deep-flatten object-map\n</code></pre>  <p>Map the block volume and format it and mount it:</p> <pre><code># Map the rbd device. If the Direct Mount Pod was started with \"hostNetwork: false\" this hangs and you have to stop it with Ctrl-C,\n# however the command still succeeds; see https://github.com/rook/rook/issues/2021\nrbd map replicapool/test\n\n# Find the device name, such as rbd0\nlsblk | grep rbd\n\n# Format the volume (only do this the first time or you will lose data)\nmkfs.ext4 -m0 /dev/rbd0\n\n# Mount the block device\nmkdir /tmp/rook-volume\nmount /dev/rbd0 /tmp/rook-volume\n</code></pre>  <p>Write and read a file:</p> <pre><code>echo \"Hello Rook\" &gt; /tmp/rook-volume/hello\ncat /tmp/rook-volume/hello\n</code></pre>","title":"Block Storage Tools"},{"location":"Usages-and-Examples/direct-tools/#unmount-the-block-device","text":"<p>Unmount the volume and unmap the kernel device:</p> <pre><code>umount /tmp/rook-volume\nrbd unmap /dev/rbd0\n</code></pre>","title":"Unmount the Block device"},{"location":"Usages-and-Examples/direct-tools/#shared-filesystem-tools","text":"<p>After you have created a filesystem as described in the Shared Filesystem topic, you can mount the filesystem from multiple pods. The the other topic you may have mounted the filesystem already in the registry pod. Now we will mount the same filesystem in the Direct Mount pod. This is just a simple way to validate the Ceph filesystem and is not recommended for production Kubernetes pods.</p> <p>Follow Direct Mount Pod to start a pod with the necessary mounts and then proceed with the following commands after connecting to the pod.</p> <pre><code># Create the directory\nmkdir /tmp/registry\n\n# Detect the mon endpoints and the user secret for the connection\nmon_endpoints=$(grep mon_host /etc/ceph/ceph.conf | awk '{print $3}')\nmy_secret=$(grep key /etc/ceph/keyring | awk '{print $3}')\n\n# Mount the filesystem\nmount -t ceph -o mds_namespace=myfs,name=admin,secret=$my_secret $mon_endpoints:/ /tmp/registry\n\n# See your mounted filesystem\ndf -h\n</code></pre>  <p>Now you should have a mounted filesystem. If you have pushed images to the registry you will see a directory called <code>docker</code>.</p> <pre><code>ls /tmp/registry\n</code></pre>  <p>Try writing and reading a file to the shared filesystem.</p> <pre><code>echo \"Hello Rook\" &gt; /tmp/registry/hello\ncat /tmp/registry/hello\n\n# delete the file when you're done\nrm -f /tmp/registry/hello\n</code></pre>","title":"Shared Filesystem Tools"},{"location":"Usages-and-Examples/direct-tools/#unmount-the-filesystem","text":"<p>To unmount the shared filesystem from the Direct Mount Pod:</p> <pre><code>umount /tmp/registry\nrmdir /tmp/registry\n</code></pre>  <p>No data will be deleted by unmounting the filesystem.</p>","title":"Unmount the Filesystem"},{"location":"Usages-and-Examples/key-management-system/","text":"<p>Rook has the ability to encrypt OSDs of clusters running on PVC via the flag (<code>encrypted: true</code>) in your <code>storageClassDeviceSets</code> template. By default, the Key Encryption Keys (also known as Data Encryption Keys) are stored in a Kubernetes Secret. However, if a Key Management System exists Rook is capable of using it.</p> <p>The <code>security</code> section contains settings related to encryption of the cluster.</p> <ul> <li><code>security</code>:</li> <li><code>kms</code>: Key Management System settings<ul> <li><code>connectionDetails</code>: the list of parameters representing kms connection details</li> <li><code>tokenSecretName</code>: the name of the Kubernetes Secret containing the kms authentication token</li> </ul> </li> </ul> <p>Supported KMS providers:</p> <ul> <li>Vault</li> <li>IBM Key Protect</li> </ul>","title":"Key Management System"},{"location":"Usages-and-Examples/key-management-system/#vault","text":"<p>Rook supports storing OSD encryption keys in HashiCorp Vault KMS.</p>","title":"Vault"},{"location":"Usages-and-Examples/key-management-system/#authentication-methods","text":"<p>Rook support two authentication methods:</p> <ul> <li>token-based: a token is provided by the user and is stored in a Kubernetes Secret. It's used to   authenticate the KMS by the Rook operator. This has several pitfalls such as:<ul> <li>when the token expires it must be renewed, so the secret holding it must be updated</li> <li>no token automatic rotation</li> </ul> </li> <li>Kubernetes Service Account uses Vault Kubernetes native   authentication mechanism and alleviate some of the limitations from the token authentication such as token automatic renewal. This method is   generally recommended over the token-based authentication.</li> </ul>","title":"Authentication methods"},{"location":"Usages-and-Examples/key-management-system/#token-based-authentication","text":"<p>When using the token-based authentication, a Kubernetes Secret must be created to hold the token. This is governed by the <code>tokenSecretName</code> parameter.</p> <p>Note: Rook supports all the Vault environment variables.</p> <p>The Kubernetes Secret <code>rook-vault-token</code> should contain:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: rook-vault-token\n  namespace: rook-ceph\ndata:\n  token: &lt;TOKEN&gt; # base64 of a token to connect to Vault, for example: cy5GWXpsbzAyY2duVGVoRjhkWG5Bb3EyWjkK\n</code></pre>  <p>You can create a token in Vault by running the following command:</p> <pre><code>vault token create -policy=rook\n</code></pre>  <p>Refer to the official vault document for more details on how to create a token. For which policy to apply see the next section.</p> <p>In order for Rook to connect to Vault, you must configure the following in your <code>CephCluster</code> template:</p> <pre><code>security:\n  kms:\n    # name of the k8s config map containing all the kms connection details\n    connectionDetails:\n      KMS_PROVIDER: vault\n      VAULT_ADDR: https://vault.default.svc.cluster.local:8200\n      VAULT_BACKEND_PATH: rook\n      VAULT_SECRET_ENGINE: kv\n      VAULT_AUTH_METHOD: token\n    # name of the k8s secret containing the kms authentication token\n    tokenSecretName: rook-vault-token\n</code></pre>","title":"Token-based authentication"},{"location":"Usages-and-Examples/key-management-system/#kubernetes-based-authentication","text":"<p>In order to use the Kubernetes Service Account authentication method, the following must be run to properly configure Vault:</p> <pre><code>ROOK_NAMESPACE=rook-ceph\nROOK_VAULT_SA=rook-vault-auth\nROOK_SYSTEM_SA=rook-ceph-system\nROOK_OSD_SA=rook-ceph-osd\nVAULT_POLICY_NAME=rook\n\n# create service account for vault to validate API token\nkubectl -n \"$ROOK_NAMESPACE\" create serviceaccount \"$ROOK_VAULT_SA\"\n\n# create the RBAC for this SA\nkubectl -n \"$ROOK_NAMESPACE\" create clusterrolebinding vault-tokenreview-binding --clusterrole=system:auth-delegator --serviceaccount=\"$ROOK_NAMESPACE\":\"$ROOK_VAULT_SA\"\n\n# get the service account common.yaml created earlier\nVAULT_SA_SECRET_NAME=$(kubectl -n \"$ROOK_NAMESPACE\" get sa \"$ROOK_VAULT_SA\" -o jsonpath=\"{.secrets[*]['name']}\")\n\n# Set SA_JWT_TOKEN value to the service account JWT used to access the TokenReview API\nSA_JWT_TOKEN=$(kubectl -n \"$ROOK_NAMESPACE\" get secret \"$VAULT_SA_SECRET_NAME\" -o jsonpath=\"{.data.token}\" | base64 --decode)\n\n# Set SA_CA_CRT to the PEM encoded CA cert used to talk to Kubernetes API\nSA_CA_CRT=$(kubectl -n \"$ROOK_NAMESPACE\" get secret \"$VAULT_SA_SECRET_NAME\" -o jsonpath=\"{.data['ca\\.crt']}\" | base64 --decode)\n\n# get kubernetes endpoint\nK8S_HOST=$(kubectl config view --minify --flatten -o jsonpath=\"{.clusters[0].cluster.server}\")\n\n# enable kubernetes auth\nvault auth enable kubernetes\n\n# To fetch the service account issuer\nkubectl proxy &amp;\nproxy_pid=$!\n\n# configure the kubernetes auth\nvault write auth/kubernetes/config \\\n    token_reviewer_jwt=\"$SA_JWT_TOKEN\" \\\n    kubernetes_host=\"$K8S_HOST\" \\\n    kubernetes_ca_cert=\"$SA_CA_CRT\" \\\n    issuer=\"$(curl --silent http://127.0.0.1:8001/.well-known/openid-configuration | jq -r .issuer)\"\n\nkill $proxy_pid\n\n# configure a role for rook\nvault write auth/kubernetes/role/\"$ROOK_NAMESPACE\" \\\n    bound_service_account_names=\"$ROOK_SYSTEM_SA\",\"$ROOK_OSD_SA\" \\\n    bound_service_account_namespaces=\"$ROOK_NAMESPACE\" \\\n    policies=\"$VAULT_POLICY_NAME\" \\\n    ttl=1440h\n</code></pre>  <p>Once done, your <code>CephCluster</code> CR should look like:</p> <pre><code>security:\n  kms:\n    connectionDetails:\n        KMS_PROVIDER: vault\n        VAULT_ADDR: https://vault.default.svc.cluster.local:8200\n        VAULT_BACKEND_PATH: rook\n        VAULT_SECRET_ENGINE: kv\n        VAULT_AUTH_METHOD: kubernetes\n        VAULT_AUTH_KUBERNETES_ROLE: rook-ceph\n</code></pre>   <p>Note: The <code>VAULT_ADDR</code> value above assumes that Vault is accessible within the cluster itself on the default port (8200). If running elsewhere, please update the URL accordingly.</p>","title":"Kubernetes-based authentication"},{"location":"Usages-and-Examples/key-management-system/#general-vault-configuration","text":"<p>As part of the token, here is an example of a policy that can be used:</p> <pre><code>path \"rook/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"]\n}\npath \"sys/mounts\" {\ncapabilities = [\"read\"]\n}\n</code></pre>  <p>You can write the policy like so and then create a token:</p> <pre><code>vault policy write rook /tmp/rook.hcl\nvault token create -policy=rook\n</code></pre>   <pre><code>Key                  Value\n---                  -----\ntoken                s.FYzlo02cgnTehF8dXnAoq2Z9\ntoken_accessor       oMo7sAXQKbYtxU4HtO8k3pko\ntoken_duration       768h\ntoken_renewable      true\ntoken_policies       [\"default\" \"rook\"]\nidentity_policies    []\npolicies             [\"default\" \"rook\"]\n</code></pre>   <p>In the above example, Vault's secret backend path name is <code>rook</code>. It must be enabled with the following:</p> <pre><code>vault secrets enable -path=rook kv\n</code></pre>  <p>If a different path is used, the <code>VAULT_BACKEND_PATH</code> key in <code>connectionDetails</code> must be changed.</p>","title":"General Vault configuration"},{"location":"Usages-and-Examples/key-management-system/#tls-configuration","text":"<p>This is an advanced but recommended configuration for production deployments, in this case the <code>vault-connection-details</code> will look like:</p> <pre><code>security:\n  kms:\n    # name of the k8s config map containing all the kms connection details\n    connectionDetails:\n      KMS_PROVIDER: vault\n      VAULT_ADDR: https://vault.default.svc.cluster.local:8200\n      VAULT_CACERT: &lt;name of the k8s secret containing the PEM-encoded CA certificate&gt;\n      VAULT_CLIENT_CERT: &lt;name of the k8s secret containing the PEM-encoded client certificate&gt;\n      VAULT_CLIENT_KEY: &lt;name of the k8s secret containing the PEM-encoded private key&gt;\n    # name of the k8s secret containing the kms authentication token\n    tokenSecretName: rook-vault-token\n</code></pre>  <p>Each secret keys are expected to be:</p> <ul> <li>VAULT_CACERT: <code>cert</code></li> <li>VAULT_CLIENT_CERT: <code>cert</code></li> <li>VAULT_CLIENT_KEY: <code>key</code></li> </ul> <p>For instance <code>VAULT_CACERT</code> Secret named <code>vault-tls-ca-certificate</code> will look like:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vault-tls-ca-certificate\n  namespace: rook-ceph\ndata:\n  cert: &lt;PEM base64 encoded CA certificate&gt;\n</code></pre>  <p>Note: if you are using self-signed certificates (not known/approved by a proper CA) you must pass <code>VAULT_SKIP_VERIFY: true</code>. Communications will remain encrypted but the validity of the certificate will not be verified.</p>","title":"TLS configuration"},{"location":"Usages-and-Examples/key-management-system/#ibm-key-protect","text":"<p>Rook supports storing OSD encryption keys in IBM Key Protect. The current implementation stores OSD encryption keys as Standard Keys using the Bring Your Own Key (BYOK) method. This means that the Key Protect instance policy must have Standard Imported Key enabled.</p>","title":"IBM Key Protect"},{"location":"Usages-and-Examples/key-management-system/#configuration","text":"<p>First, you need to provision the Key Protect service on the IBM Cloud. Once completed, retrieve the instance ID. Make a record of it; we need it in the CRD.</p> <p>On the IBM Cloud, the user must create a Service ID, then assign an Access Policy to this service. Ultimately, a Service API Key needs to be generated. All the steps are summarized in the official documentation.</p> <p>The Service ID must be granted access to the Key Protect Service. Once the Service API Key is generated, store it in a Kubernetes Secret.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ibm-kp-svc-api-key\n  namespace: rook-ceph\ndata:\n  IBM_KP_SERVICE_API_KEY: &lt;service API Key&gt;\n</code></pre>  <p>In order for Rook to connect to IBM Key Protect, you must configure the following in your <code>CephCluster</code> template:</p> <pre><code>security:\n  kms:\n    # name of the k8s config map containing all the kms connection details\n    connectionDetails:\n      KMS_PROVIDER: ibmkeyprotect\n      IBM_KP_SERVICE_INSTANCE_ID: &lt;instance ID that was retrieved in the first paragraph&gt;\n    # name of the k8s secret containing the service API Key\n    tokenSecretName: ibm-kp-svc-api-key\n</code></pre>  <p>More options are supported such as:</p> <ul> <li><code>IBM_BASE_URL</code>: the base URL of the Key Protect instance, depending on your region. Defaults to <code>https://us-south.kms.cloud.ibm.com</code>.</li> <li><code>IBM_TOKEN_URL</code>: the URL of the Key Protect instance to retrieve the token. Defaults to <code>https://iam.cloud.ibm.com/oidc/token</code>. Only needed for private instances.</li> </ul>","title":"Configuration"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/","text":"<p>There are three CSI drivers integrated with Rook that will enable different scenarios:</p> <ul> <li>RBD: This block storage driver is optimized for RWO pod access where only one pod may access the   storage. More information.</li> <li>CephFS: This file storage driver allows for RWX with one or more pods accessing the same storage. More information.</li> <li>NFS (experimental): This file storage driver allows creating NFS exports that can be mounted to   pods, or the exports can be mounted directly via an NFS client from inside or outside the   Kubernetes cluster. More information.</li> </ul> <p>The Ceph Filesysetem (CephFS) and RADOS Block Device (RBD) drivers are enabled automatically with the Rook operator. The NFS driver is disabled by default. All drivers will be started in the same namespace as the operator when the first CephCluster CR is created.</p>","title":"Ceph CSI Drivers"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#supported-versions","text":"<p>The supported Ceph CSI version is 3.3.0 or greater with Rook. Refer to ceph csi releases for more information.</p>","title":"Supported Versions"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#static-provisioning","text":"<p>Both drivers also support the creation of static PV and static PVC from existing RBD image/CephFS volume. Refer to static PVC for more information.</p>","title":"Static Provisioning"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#configure-csi-drivers-in-non-default-namespace","text":"<p>If you've deployed the Rook operator in a namespace other than \"rook-ceph\", change the prefix in the provisioner to match the namespace you used. For example, if the Rook operator is running in the namespace \"my-namespace\" the provisioner value should be \"my-namespace.rbd.csi.ceph.com\". The same provisioner name needs to be set in both the storageclass and snapshotclass.</p>","title":"Configure CSI Drivers in non-default namespace"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#liveness-sidecar","text":"<p>All CSI pods are deployed with a sidecar container that provides a prometheus metric for tracking if the CSI plugin is alive and running. These metrics are meant to be collected by prometheus but can be accesses through a GET request to a specific node ip. for example <code>curl -X get http://[pod ip]:[liveness-port][liveness-path]  2&gt;/dev/null | grep csi</code> the expected output should be</p> <pre><code>curl -X GET http://10.109.65.142:9080/metrics 2&gt;/dev/null | grep csi\n</code></pre>   <pre><code># HELP csi_liveness Liveness Probe\n# TYPE csi_liveness gauge\ncsi_liveness 1\n</code></pre>   <p>Check the monitoring doc to see how to integrate CSI liveness and grpc metrics into ceph monitoring.</p>","title":"Liveness Sidecar"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#dynamically-expand-volume","text":"","title":"Dynamically Expand Volume"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#prerequisite","text":"<ul> <li>For filesystem resize to be supported for your Kubernetes cluster, the   kubernetes version running in your cluster should be &gt;= v1.15 and for block   volume resize support the Kubernetes version should be &gt;= v1.16. Also, <code>ExpandCSIVolumes</code> feature gate has to be enabled for the volume resize   functionality to work.</li> </ul> <p>To expand the PVC the controlling StorageClass must have <code>allowVolumeExpansion</code> set to <code>true</code>. <code>csi.storage.k8s.io/controller-expand-secret-name</code> and <code>csi.storage.k8s.io/controller-expand-secret-namespace</code> values set in storageclass. Now expand the PVC by editing the PVC <code>pvc.spec.resource.requests.storage</code> to a higher values than the current size. Once PVC is expanded on backend and same is reflected size is reflected on application mountpoint, the status capacity <code>pvc.status.capacity.storage</code> of PVC will be updated to new size.</p>","title":"Prerequisite"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#rbd-mirroring","text":"<p>To support RBD Mirroring, the Volume Replication Operator will be started in the RBD provisioner pod. The Volume Replication Operator is a kubernetes operator that provides common and reusable APIs for storage disaster recovery. It is based on csi-addons/spec specification and can be used by any storage provider. It follows the controller pattern and provides extended APIs for storage disaster recovery. The extended APIs are provided via Custom Resource Definitions (CRDs).</p>","title":"RBD Mirroring"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#prerequisites","text":"<p>Kubernetes version 1.21 or greater is required.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#enable-volume-replication","text":"<ol> <li>Install the volume replication CRDs:</li> </ol> <pre><code>kubectl create -f https://raw.githubusercontent.com/csi-addons/volume-replication-operator/v0.3.0/config/crd/bases/replication.storage.openshift.io_volumereplications.yaml\nkubectl create -f https://raw.githubusercontent.com/csi-addons/volume-replication-operator/v0.3.0/config/crd/bases/replication.storage.openshift.io_volumereplicationclasses.yaml\n</code></pre>  <ol> <li>Enable the volume replication controller:</li> <li>For Helm deployments see the csi.volumeReplication.enabled setting.</li> <li>For non-Helm deployments set <code>CSI_ENABLE_VOLUME_REPLICATION: \"true\"</code> in operator.yaml</li> </ol>","title":"Enable volume replication"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#ephemeral-volume-support","text":"<p>The generic ephemeral volume feature adds support for specifying PVCs in the <code>volumes</code> field to indicate a user would like to create a Volume as part of the pod spec. This feature requires the GenericEphemeralVolume feature gate to be enabled.</p> <p>For example:</p> <pre><code>kind: Pod\napiVersion: v1\n...\n  volumes:\n    - name: mypvc\n      ephemeral:\n        volumeClaimTemplate:\n          spec:\n            accessModes: [\"ReadWriteOnce\"]\n            storageClassName: \"rook-ceph-block\"\n            resources:\n              requests:\n                storage: 1Gi\n</code></pre>  <p>A volume claim template is defined inside the pod spec which refers to a volume provisioned and used by the pod with its lifecycle. The volumes are provisioned when pod get spawned and destroyed at time of pod delete.</p> <p>Refer to ephemeral-doc for more info. Also, See the example manifests for an RBD ephemeral volume and a CephFS ephemeral volume.</p>","title":"Ephemeral volume support"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#csi-addons-controller","text":"<p>The CSI-Addons Controller handles the requests from users to initiate an operation. Users create a CR that the controller inspects, and forwards a request to one or more CSI-Addons side-cars for execution.</p>","title":"CSI-Addons Controller"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#deploying-the-controller","text":"<p>Users can deploy the controller by running the following commands:</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/csi-addons/kubernetes-csi-addons/v0.3.0/deploy/controller/crds.yaml\nkubectl create -f https://raw.githubusercontent.com/csi-addons/kubernetes-csi-addons/v0.3.0/deploy/controller/rbac.yaml\nkubectl create -f https://raw.githubusercontent.com/csi-addons/kubernetes-csi-addons/v0.3.0/deploy/controller/setup-controller.yaml\n</code></pre>  <p>This creates the required crds and configure permissions.</p>","title":"Deploying the controller"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#enable-the-csi-addons-sidecar","text":"<p>To use the features provided by the CSI-Addons, the <code>csi-addons</code> containers need to be deployed in the RBD provisioner and nodeplugin pods, which are not enabled by default.</p> <p>Execute the following command in the cluster to enable the CSI-Addons sidecar:</p> <ul> <li>Update the <code>rook-ceph-operator-config</code> configmap and patch the  following configurations</li> </ul> <pre><code>kubectl patch cm rook-ceph-operator-config -nrook-ceph -p $'data:\\n \"CSI_ENABLE_CSIADDONS\": \"true\"'\n</code></pre>  <ul> <li>After enabling <code>CSI_ENABLE_CSIADDONS</code> in the configmap, a new sidecar container with name <code>csi-addons</code>  should now start automatically in the RBD CSI provisioner and nodeplugin pods.</li> </ul>  <p>NOTE: Make sure the version of ceph-csi used is v3.5.0+</p>","title":"Enable the CSI-Addons Sidecar"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#csi-addons-operation","text":"<p>CSI-Addons supports the following operations:</p> <ul> <li>Reclaim Space</li> <li>Creating a ReclaimSpaceJob</li> <li>Creating a ReclaimSpaceCronJob</li> <li>Annotating PersistentVolumeClaims</li> <li>Network Fencing</li> <li>Creating a NetworkFence</li> </ul>","title":"CSI-ADDONS Operation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-drivers/#enable-rbd-encryption-support","text":"<p>Ceph-CSI supports encrypting individual RBD PersistentVolumeClaim with LUKS encryption. More details can be found here with full list of supported encryption configurations. A sample configmap can be found here.</p>  <p>NOTE: Rook also supports OSD encryption (see <code>encryptedDevice</code> option here). Using both RBD PVC encryption and OSD encryption together will lead to double encryption and may reduce read/write performance.</p>  <p>Unlike OSD encryption, existing ceph clusters can also enable Ceph-CSI RBD PVC encryption support and multiple kinds of encryption KMS can be used on the same ceph cluster using different storageclasses.</p> <p>Following steps demonstrate how to enable support for encryption:</p> <ul> <li>Create the <code>rook-ceph-csi-kms-config</code> configmap with required encryption configuration in the same namespace where the Rook operator is deployed. An example is shown below:</li> </ul> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rook-ceph-csi-kms-config\n  namespace: rook-ceph\ndata:\n  config.json: |-\n    {\n      \"user-secret-metadata\": {\n        \"encryptionKMSType\": \"metadata\",\n        \"secretName\": \"storage-encryption-secret\"\n      }\n    }\n</code></pre>  <ul> <li>Update the <code>rook-ceph-operator-config</code> configmap and patch the  following configurations</li> </ul> <pre><code>kubectl patch cm rook-ceph-operator-config -nrook-ceph -p $'data:\\n \"CSI_ENABLE_ENCRYPTION\": \"true\"'\n</code></pre>  <ul> <li>Create necessary resources (secrets, configmaps etc) as required by the encryption type. In this case, create <code>storage-encryption-secret</code> secret in the namespace of pvc as shown:</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: storage-encryption-secret\n  namespace: rook-ceph\nstringData:\n  encryptionPassphrase: test-encryption\n</code></pre>  <ul> <li>Create a new storageclass with additional parameters <code>encrypted: \"true\"</code> and <code>encryptionKMSID: \"&lt;key used in configmap&gt;\"</code>. An example is show below:</li> </ul> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-ceph-block-encrypted\nparameters:\n  # additional parameters required for encryption\n  encrypted: \"true\"\n  encryptionKMSID: \"user-secret-metadata\"\n# ...\n</code></pre>  <ul> <li>PVCs created using the new storageclass will be encrypted.</li> </ul>","title":"Enable RBD Encryption Support"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/","text":"","title":"Snapshots"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#prerequisites","text":"<ul> <li> <p>Rook officially supports v1/v1beta1 snapshots for kubernetes v1.17+.</p> </li> <li> <p>Install the snapshot controller and snapshot v1/v1beta1 CRD as required. More info can be found here.</p> </li> </ul> <p>Note: If only Alpha snapshots are available, enable snapshotter in <code>rook-ceph-operator-config</code> or helm chart <code>values.yaml</code>, change the external-snapshotter image to <code>k8s.gcr.io/sig-storage/csi-snapshotter:v1.2.2</code> and refer to the alpha snapshots documentation</p> <ul> <li>We also need a <code>VolumeSnapshotClass</code> for volume snapshot to work. The purpose of a <code>VolumeSnapshotClass</code> is defined in the kubernetes documentation. In short, as the documentation describes it:</li> </ul>  <p>Just like StorageClass provides a way for administrators to describe the \"classes\" of storage they offer when provisioning a volume, VolumeSnapshotClass provides a way to describe the \"classes\" of storage when provisioning a volume snapshot.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#upgrade-snapshot-api","text":"<p>If your Kubernetes version is updated to a newer version of the snapshot API, follow the upgrade guide here to upgrade from v1alpha1 to v1beta1, or v1beta1 to v1.</p>","title":"Upgrade Snapshot API"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshots","text":"","title":"RBD Snapshots"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#volumesnapshotclass","text":"<p>In VolumeSnapshotClass, the <code>csi.storage.k8s.io/snapshotter-secret-name</code> parameter should reference the name of the secret created for the rbdplugin and <code>pool</code> to reflect the Ceph pool name.</p> <p>Update the value of the <code>clusterID</code> field to match the namespace that Rook is running in. When Ceph CSI is deployed by Rook, the operator will automatically maintain a configmap whose contents will match this key. By default this is \"rook-ceph\".</p> <pre><code>kubectl create -f deploy/examples/csi/rbd/snapshotclass.yaml\n</code></pre>","title":"VolumeSnapshotClass"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#volumesnapshot","text":"<p>In snapshot, <code>volumeSnapshotClassName</code> should be the name of the <code>VolumeSnapshotClass</code> previously created. The <code>persistentVolumeClaimName</code> should be the name of the PVC which is already created by the RBD CSI driver.</p> <pre><code>kubectl create -f deploy/examples/csi/rbd/snapshot.yaml\n</code></pre>","title":"Volumesnapshot"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#verify-rbd-snapshot-creation","text":"<pre><code>kubectl get volumesnapshotclass\n</code></pre>   <pre><code>NAME                      DRIVER                       DELETIONPOLICY   AGE\ncsi-rbdplugin-snapclass   rook-ceph.rbd.csi.ceph.com   Delete           3h55m\n</code></pre>   <pre><code>kubectl get volumesnapshot\n</code></pre>   <pre><code>NAME               READYTOUSE   SOURCEPVC   SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS             SNAPSHOTCONTENT                                    CREATIONTIME   AGE\nrbd-pvc-snapshot   true         rbd-pvc                             1Gi           csi-rbdplugin-snapclass   snapcontent-79090db0-7c66-4b18-bf4a-634772c7cac7   3h50m          3h51m\n</code></pre>   <p>The snapshot will be ready to restore to a new PVC when the <code>READYTOUSE</code> field of the <code>volumesnapshot</code> is set to true.</p>","title":"Verify RBD Snapshot Creation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#restore-the-snapshot-to-a-new-pvc","text":"<p>In pvc-restore, <code>dataSource</code> should be the name of the <code>VolumeSnapshot</code> previously created. The <code>dataSource</code> kind should be the <code>VolumeSnapshot</code>.</p> <p>Create a new PVC from the snapshot</p> <pre><code>kubectl create -f deploy/examples/csi/rbd/pvc-restore.yaml\n</code></pre>","title":"Restore the snapshot to a new PVC"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#verify-rbd-clone-pvc-creation","text":"<pre><code>kubectl get pvc\n</code></pre>   <pre><code>NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\nrbd-pvc           Bound    pvc-84294e34-577a-11e9-b34f-525400581048   1Gi        RWO            rook-ceph-block       34m\nrbd-pvc-restore   Bound    pvc-575537bf-577f-11e9-b34f-525400581048   1Gi        RWO            rook-ceph-block       8s\n</code></pre>","title":"Verify RBD Clone PVC Creation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshot-resource-cleanup","text":"<p>To clean your cluster of the resources created by this example, run the following:</p> <pre><code>kubectl delete -f deploy/examples/csi/rbd/pvc-restore.yaml\nkubectl delete -f deploy/examples/csi/rbd/snapshot.yaml\nkubectl delete -f deploy/examples/csi/rbd/snapshotclass.yaml\n</code></pre>","title":"RBD snapshot resource Cleanup"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#cephfs-snapshots","text":"","title":"CephFS Snapshots"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#volumesnapshotclass_1","text":"<p>In VolumeSnapshotClass, the <code>csi.storage.k8s.io/snapshotter-secret-name</code> parameter should reference the name of the secret created for the cephfsplugin.</p> <p>In the volumesnapshotclass, update the value of the <code>clusterID</code> field to match the namespace that Rook is running in. When Ceph CSI is deployed by Rook, the operator will automatically maintain a configmap whose contents will match this key. By default this is \"rook-ceph\".</p> <pre><code>kubectl create -f deploy/examples/csi/cephfs/snapshotclass.yaml\n</code></pre>","title":"VolumeSnapshotClass"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#volumesnapshot_1","text":"<p>In snapshot, <code>volumeSnapshotClassName</code> should be the name of the <code>VolumeSnapshotClass</code> previously created. The <code>persistentVolumeClaimName</code> should be the name of the PVC which is already created by the CephFS CSI driver.</p> <pre><code>kubectl create -f deploy/examples/csi/cephfs/snapshot.yaml\n</code></pre>","title":"Volumesnapshot"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#verify-cephfs-snapshot-creation","text":"<pre><code>kubectl get volumesnapshotclass\n</code></pre>   <p><pre><code>NAME                        DRIVER                          DELETIONPOLICY   AGE\ncsi-cephfslugin-snapclass   rook-ceph.cephfs.csi.ceph.com   Delete           3h55m\n</code></pre>  <pre><code>kubectl get volumesnapshot\n</code></pre> </p> <pre><code>NAME                  READYTOUSE   SOURCEPVC   SOURCESNAPSHOTCONTENT  RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                   CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         cephfs-pvc                         1Gi           csi-cephfsplugin-snapclass   snapcontent-34476204-a14a-4d59-bfbc-2bbba695652c  3h50m          3h51m\n</code></pre>   <p>The snapshot will be ready to restore to a new PVC when <code>READYTOUSE</code> field of the <code>volumesnapshot</code> is set to true.</p>","title":"Verify CephFS Snapshot Creation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#restore-the-snapshot-to-a-new-pvc_1","text":"<p>In pvc-restore, <code>dataSource</code> should be the name of the <code>VolumeSnapshot</code> previously created. The <code>dataSource</code> kind should be the <code>VolumeSnapshot</code>.</p> <p>Create a new PVC from the snapshot</p> <pre><code>kubectl create -f deploy/examples/csi/cephfs/pvc-restore.yaml\n</code></pre>","title":"Restore the snapshot to a new PVC"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#verify-cephfs-restore-pvc-creation","text":"<pre><code>kubectl get pvc\n</code></pre>   <pre><code>NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\ncephfs-pvc           Bound    pvc-74734901-577a-11e9-b34f-525400581048   1Gi        RWX            rook-cephfs       55m\ncephfs-pvc-restore   Bound    pvc-95308c75-6c93-4928-a551-6b5137192209   1Gi        RWX            rook-cephfs       34s\n</code></pre>","title":"Verify CephFS Restore PVC Creation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#cephfs-snapshot-resource-cleanup","text":"<p>To clean your cluster of the resources created by this example, run the following:</p> <pre><code>kubectl delete -f deploy/examples/csi/cephfs/pvc-restore.yaml\nkubectl delete -f deploy/examples/csi/cephfs/snapshot.yaml\nkubectl delete -f deploy/examples/csi/cephfs/snapshotclass.yaml\n</code></pre>","title":"CephFS snapshot resource Cleanup"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-snapshot/#limitations","text":"<ul> <li>There is a limit of 400 snapshots per cephFS filesystem.</li> <li>The PVC cannot be deleted if it has snapshots. make sure all the snapshots on the PVC are deleted before you delete the PVC.</li> </ul>","title":"Limitations"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/","text":"<p>Issues when provisioning volumes with the Ceph CSI driver can happen for many reasons such as:</p> <ul> <li>Network connectivity between CSI pods and ceph</li> <li>Cluster health issues</li> <li>Slow operations</li> <li>Kubernetes issues</li> <li>Ceph-CSI configuration or bugs</li> </ul> <p>The following troubleshooting steps can help identify a number of issues.</p>","title":"CSI Common Issues"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#block-rbd","text":"<p>If you are mounting block volumes (usually RWO), these are referred to as <code>RBD</code> volumes in Ceph. See the sections below for RBD if you are having block volume issues.</p>","title":"Block (RBD)"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#shared-filesystem-cephfs","text":"<p>If you are mounting shared filesystem volumes (usually RWX), these are referred to as <code>CephFS</code> volumes in Ceph. See the sections below for CephFS if you are having filesystem volume issues.</p>","title":"Shared Filesystem (CephFS)"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#network-connectivity","text":"<p>The Ceph monitors are the most critical component of the cluster to check first. Retrieve the mon endpoints from the services:</p> <pre><code>kubectl -n rook-ceph get svc -l app=rook-ceph-mon\n</code></pre>   <pre><code>NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nrook-ceph-mon-a   ClusterIP   10.104.165.31   &lt;none&gt;        6789/TCP,3300/TCP   18h\nrook-ceph-mon-b   ClusterIP   10.97.244.93    &lt;none&gt;        6789/TCP,3300/TCP   21s\nrook-ceph-mon-c   ClusterIP   10.99.248.163   &lt;none&gt;        6789/TCP,3300/TCP   8s\n</code></pre>   <p>If host networking is enabled in the CephCluster CR, you will instead need to find the node IPs for the hosts where the mons are running.</p> <p>The <code>clusterIP</code> is the mon IP and <code>3300</code> is the port that will be used by Ceph-CSI to connect to the ceph cluster. These endpoints must be accessible by all clients in the cluster, including the CSI driver.</p> <p>If you are seeing issues provisioning the PVC then you need to check the network connectivity from the provisioner pods.</p> <ul> <li>For CephFS PVCs, check network connectivity from the <code>csi-cephfsplugin</code> container of the <code>csi-cephfsplugin-provisioner</code> pods</li> <li>For Block PVCs, check network connectivity from the <code>csi-rbdplugin</code> container of the <code>csi-rbdplugin-provisioner</code> pods</li> </ul> <p>For redundancy, there are two provisioner pods for each type. Make sure to test connectivity from all provisioner pods.</p> <p>Connect to the provisioner pods and verify the connection to the mon endpoints such as the following:</p> <pre><code># Connect to the csi-cephfsplugin container in the provisioner pod\nkubectl -n rook-ceph exec -ti deploy/csi-cephfsplugin-provisioner -c csi-cephfsplugin -- bash\n\n# Test the network connection to the mon endpoint\ncurl 10.104.165.31:3300 2&gt;/dev/null\nceph v2\n</code></pre>  <p>If you see the response \"ceph v2\", the connection succeeded. If there is no response then there is a network issue connecting to the ceph cluster.</p> <p>Check network connectivity for all monitor IP\u2019s and ports which are passed to ceph-csi.</p>","title":"Network Connectivity"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#ceph-health","text":"<p>Sometimes an unhealthy Ceph cluster can contribute to the issues in creating or mounting the PVC. Check that your Ceph cluster is healthy by connecting to the Toolbox and running the <code>ceph</code> commands:</p> <pre><code>ceph health detail\n</code></pre>  <pre><code>HEALTH_OK\n</code></pre>","title":"Ceph Health"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#slow-operations","text":"<p>Even slow ops in the ceph cluster can contribute to the issues. In the toolbox, make sure that no slow ops are present and the ceph cluster is healthy</p> <pre><code>ceph -s\n</code></pre>   <pre><code>cluster:\n  id:     ba41ac93-3b55-4f32-9e06-d3d8c6ff7334\n  health: HEALTH_WARN\n          30 slow ops, oldest one blocked for 10624 sec, mon.a has slow ops\n</code></pre>   <p>If Ceph is not healthy, check the following health for more clues:</p> <ul> <li>The Ceph monitor logs for errors</li> <li>The OSD logs for errors</li> <li>Disk Health</li> <li>Network Health</li> </ul>","title":"Slow Operations"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#ceph-troubleshooting","text":"","title":"Ceph Troubleshooting"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#check-if-the-rbd-pool-exists","text":"<p>Make sure the pool you have specified in the <code>storageclass.yaml</code> exists in the ceph cluster.</p> <p>Suppose the pool name mentioned in the <code>storageclass.yaml</code> is <code>replicapool</code>. It can be verified to exist in the toolbox:</p> <pre><code>ceph osd lspools\n</code></pre>   <pre><code>1 device_health_metrics\n2 replicapool\n</code></pre>   <p>If the pool is not in the list, create the <code>CephBlockPool</code> CR for the pool if you have not already. If you have already created the pool, check the Rook operator log for errors creating the pool.</p>","title":"Check if the RBD Pool exists"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#check-if-the-filesystem-exists","text":"<p>For the shared filesystem (CephFS), check that the filesystem and pools you have specified in the <code>storageclass.yaml</code> exist in the Ceph cluster.</p> <p>Suppose the <code>fsName</code> name mentioned in the <code>storageclass.yaml</code> is <code>myfs</code>. It can be verified in the toolbox:</p> <pre><code>ceph fs ls\n</code></pre>   <pre><code>name: myfs, metadata pool: myfs-metadata, data pools: [myfs-data0 ]\n</code></pre>   <p>Now verify the <code>pool</code> mentioned in the <code>storageclass.yaml</code> exists, such as the example <code>myfs-data0</code>.</p> <pre><code>ceph osd lspools\n</code></pre>   <pre><code>1 device_health_metrics\n2 replicapool\n3 myfs-metadata0\n4 myfs-data0\n</code></pre>   <p>The pool for the filesystem will have the suffix <code>-data0</code> compared the filesystem name that is created by the CephFilesystem CR.</p>","title":"Check if the Filesystem exists"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#subvolumegroups","text":"<p>If the subvolumegroup is not specified in the ceph-csi configmap (where you have passed the ceph monitor information), Ceph-CSI creates the default subvolumegroup with the name csi. Verify that the subvolumegroup exists:</p> <pre><code>ceph fs subvolumegroup ls myfs\n</code></pre>   <pre><code>[\n    {\n        \"name\": \"csi\"\n    }\n]\n</code></pre>   <p>If you don\u2019t see any issues with your Ceph cluster, the following sections will start debugging the issue from the CSI side.</p>","title":"subvolumegroups"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#provisioning-volumes","text":"<p>At times the issue can also exist in the Ceph-CSI or the sidecar containers used in Ceph-CSI.</p> <p>Ceph-CSI has included number of sidecar containers in the provisioner pods such as: <code>csi-attacher</code>, <code>csi-resizer</code>, <code>csi-provisioner</code>, <code>csi-cephfsplugin</code>, <code>csi-snapshotter</code>, and <code>liveness-prometheus</code>.</p> <p>The CephFS provisioner core CSI driver container name is <code>csi-cephfsplugin</code> as one of the container names. For the RBD (Block) provisioner you will see <code>csi-rbdplugin</code> as the container name.</p> <p>Here is a summary of the sidecar containers:</p>","title":"Provisioning Volumes"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#csi-provisioner","text":"<p>The external-provisioner is a sidecar container that dynamically provisions volumes by calling <code>ControllerCreateVolume()</code> and <code>ControllerDeleteVolume()</code> functions of CSI drivers. More details about external-provisioner can be found here.</p> <p>If there is an issue with PVC Create or Delete, check the logs of the <code>csi-provisioner</code> sidecar container.</p> <pre><code>kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-provisioner\n</code></pre>","title":"csi-provisioner"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#csi-resizer","text":"<p>The CSI <code>external-resizer</code> is a sidecar container that watches the Kubernetes API server for PersistentVolumeClaim updates and triggers <code>ControllerExpandVolume</code> operations against a CSI endpoint if the user requested more storage on the PersistentVolumeClaim object. More details about external-provisioner can be found here.</p> <p>If any issue exists in PVC expansion you can check the logs of the <code>csi-resizer</code> sidecar container.</p> <pre><code>kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-resizer\n</code></pre>","title":"csi-resizer"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#csi-snapshotter","text":"<p>The CSI external-snapshotter sidecar only watches for <code>VolumeSnapshotContent</code> create/update/delete events. It will talk to ceph-csi containers to create or delete snapshots. More details about external-snapshotter can be found here.</p> <p>In Kubernetes 1.17 the volume snapshot feature was promoted to beta. In Kubernetes 1.20, the feature gate is enabled by default on standard Kubernetes deployments and cannot be turned off.</p> <p>Make sure you have installed the correct snapshotter CRD version. If you have not installed the snapshotter controller, see the Snapshots guide.</p> <pre><code>kubectl get crd | grep snapshot\n</code></pre>   <pre><code>volumesnapshotclasses.snapshot.storage.k8s.io    2021-01-25T11:19:38Z\nvolumesnapshotcontents.snapshot.storage.k8s.io   2021-01-25T11:19:39Z\nvolumesnapshots.snapshot.storage.k8s.io          2021-01-25T11:19:40Z\n</code></pre>   <p>The above CRDs must have the matching version in your <code>snapshotclass.yaml</code> or <code>snapshot.yaml</code>. Otherwise, the <code>VolumeSnapshot</code> and <code>VolumesnapshotContent</code> will not be created.</p> <p>The snapshot controller is responsible for creating both <code>VolumeSnapshot</code> and <code>VolumesnapshotContent</code> object. If the objects are not getting created, you may need to check the logs of the snapshot-controller container.</p> <p>Rook only installs the snapshotter sidecar container, not the controller. It is recommended that Kubernetes distributors bundle and deploy the controller and CRDs as part of their Kubernetes cluster management process (independent of any CSI Driver).</p> <p>If your Kubernetes distribution does not bundle the snapshot controller, you may manually install these components.</p> <p>If any issue exists in the snapshot Create/Delete operation you can check the logs of the csi-snapshotter sidecar container.</p> <pre><code>kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-snapshotter\n</code></pre>  <p>If you see an error such as:</p>  <pre><code>GRPC error: rpc error: code = Aborted desc = an operation with the given Volume ID\n0001-0009-rook-ceph-0000000000000001-8d0ba728-0e17-11eb-a680-ce6eecc894de already &gt;exists.\n</code></pre>   <p>The issue typically is in the Ceph cluster or network connectivity. If the issue is in Provisioning the PVC Restarting the Provisioner pods help(for CephFS issue restart <code>csi-cephfsplugin-provisioner-xxxxxx</code> CephFS Provisioner. For RBD, restart the <code>csi-rbdplugin-provisioner-xxxxxx</code> pod. If the issue is in mounting the PVC, restart the <code>csi-rbdplugin-xxxxx</code> pod (for RBD) and the <code>csi-cephfsplugin-xxxxx</code> pod for CephFS issue.</p>","title":"csi-snapshotter"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#mounting-the-volume-to-application-pods","text":"<p>When a user requests to create the application pod with PVC, there is a three-step process</p> <ul> <li>CSI driver registration</li> <li>Create volume attachment object</li> <li>Stage and publish the volume</li> </ul>","title":"Mounting the volume to application pods"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#csi-driver-registration","text":"<p><code>csi-cephfsplugin-xxxx</code> or <code>csi-rbdplugin-xxxx</code> is a daemonset pod running on all the nodes  where your application gets scheduled. If the plugin pods are not running on the node where  your application is scheduled might cause the issue, make sure plugin pods are always running.</p> <p>Each plugin pod has two important containers: one is <code>driver-registrar</code> and <code>csi-rbdplugin</code> or <code>csi-cephfsplugin</code>. Sometimes there is also a <code>liveness-prometheus</code> container.</p>","title":"csi-driver registration"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#driver-registrar","text":"<p>The node-driver-registrar is a sidecar container that registers the CSI driver with Kubelet. More details can be found here.</p> <p>If any issue exists in attaching the PVC to the application pod check logs from driver-registrar sidecar container in plugin pod where your application pod is scheduled.</p> <pre><code>kubectl -n rook-ceph logs deploy/csi-rbdplugin -c driver-registrar\n</code></pre>   <pre><code>I0120 12:28:34.231761  124018 main.go:112] Version: v2.0.1\nI0120 12:28:34.233910  124018 connection.go:151] Connecting to unix:///csi/csi.sock\nI0120 12:28:35.242469  124018 node_register.go:55] Starting Registration Server at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock\nI0120 12:28:35.243364  124018 node_register.go:64] Registration Server started at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock\nI0120 12:28:35.243673  124018 node_register.go:86] Skipping healthz server because port set to: 0\nI0120 12:28:36.318482  124018 main.go:79] Received GetInfo call: &amp;InfoRequest{}\nI0120 12:28:37.455211  124018 main.go:89] Received NotifyRegistrationStatus call: &amp;RegistrationStatus{PluginRegistered:true,Error:,}\nE0121 05:19:28.658390  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.\nE0125 07:11:42.926133  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.\n</code></pre>   <p>You should see the response <code>RegistrationStatus{PluginRegistered:true,Error:,}</code> in the logs to confirm that plugin is registered with kubelet.</p> <p>If you see a driver not found an error in the application pod describe output. Restarting the <code>csi-xxxxplugin-xxx</code> pod on the node may help.</p>","title":"driver-registrar"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#volume-attachment","text":"<p>Each provisioner pod also has a sidecar container called <code>csi-attacher</code>.</p>","title":"Volume Attachment"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#csi-attacher","text":"<p>The external-attacher is a sidecar container that attaches volumes to nodes by calling <code>ControllerPublish</code> and <code>ControllerUnpublish</code> functions of CSI drivers. It is necessary because the internal Attach/Detach controller running in Kubernetes controller-manager does not have any direct interfaces to CSI drivers. More details can be found here.</p> <p>If any issue exists in attaching the PVC to the application pod first check the volumettachment object created and also log from csi-attacher sidecar container in provisioner pod.</p> <pre><code>kubectl get volumeattachment\n</code></pre>   <pre><code>NAME                                                                   ATTACHER                        PV                                         NODE       ATTACHED   AGE\ncsi-75903d8a902744853900d188f12137ea1cafb6c6f922ebc1c116fd58e950fc92   rook-ceph.cephfs.csi.ceph.com   pvc-5c547d2a-fdb8-4cb2-b7fe-e0f30b88d454   minikube   true       4m26s\n</code></pre>   <pre><code>kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-attacher\n</code></pre>","title":"csi-attacher"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#cephfs-stale-operations","text":"<p>Check for any stale mount commands on the <code>csi-cephfsplugin-xxxx</code> pod on the node where your application pod is scheduled.</p> <p>You need to exec in the <code>csi-cephfsplugin-xxxx</code> pod and grep for stale mount operators.</p> <p>Identify the <code>csi-cephfsplugin-xxxx</code> pod running on the node where your application is scheduled with <code>kubectl get po -o wide</code> and match the node names.</p> <pre><code>kubectl exec -it csi-cephfsplugin-tfk2g -c csi-cephfsplugin -- sh\nps -ef |grep mount\n\nroot          67      60  0 11:55 pts/0    00:00:00 grep mount\n</code></pre>  <pre><code>ps -ef |grep ceph\n\nroot           1       0  0 Jan20 ?        00:00:26 /usr/local/bin/cephcsi --nodeid=minikube --type=cephfs --endpoint=unix:///csi/csi.sock --v=0 --nodeserver=true --drivername=rook-ceph.cephfs.csi.ceph.com --pidlimit=-1 --metricsport=9091 --forcecephkernelclient=true --metricspath=/metrics --enablegrpcmetrics=true\nroot          69      60  0 11:55 pts/0    00:00:00 grep ceph\n</code></pre>  <p>If any commands are stuck check the dmesg logs from the node. Restarting the <code>csi-cephfsplugin</code> pod may also help sometimes.</p> <p>If you don\u2019t see any stuck messages, confirm the network connectivity, Ceph health, and slow ops.</p>","title":"CephFS Stale operations"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#rbd-stale-operations","text":"<p>Check for any stale <code>map/mkfs/mount</code> commands on the <code>csi-rbdplugin-xxxx</code> pod on the node where your application pod is scheduled.</p> <p>You need to exec in the <code>csi-rbdplugin-xxxx</code> pod and grep for stale operators like (<code>rbd map, rbd unmap, mkfs, mount</code> and <code>umount</code>).</p> <p>Identify the <code>csi-rbdplugin-xxxx</code> pod running on the node where your application is scheduled with <code>kubectl get po -o wide</code> and match the node names.</p> <p><pre><code>kubectl exec -it csi-rbdplugin-vh8d5 -c csi-rbdplugin -- sh\n</code></pre>  <pre><code>ps -ef |grep map\n</code></pre> </p>  <pre><code>root     1297024 1296907  0 12:00 pts/0    00:00:00 grep map\n</code></pre>   <pre><code>ps -ef |grep mount\n</code></pre>   <pre><code>root        1824       1  0 Jan19 ?        00:00:00 /usr/sbin/rpc.mountd\nceph     1041020 1040955  1 07:11 ?        00:03:43 ceph-mgr --fsid=ba41ac93-3b55-4f32-9e06-d3d8c6ff7334 --keyring=/etc/ceph/keyring-store/keyring --log-to-stderr=true --err-to-stderr=true --mon-cluster-log-to-stderr=true --log-stderr-prefix=debug  --default-log-to-file=false --default-mon-cluster-log-to-file=false --mon-host=[v2:10.111.136.166:3300,v1:10.111.136.166:6789] --mon-initial-members=a --id=a --setuser=ceph --setgroup=ceph --client-mount-uid=0 --client-mount-gid=0 --foreground --public-addr=172.17.0.6\nroot     1297115 1296907  0 12:00 pts/0    00:00:00 grep mount\n</code></pre>   <pre><code>ps -ef |grep mkfs\n</code></pre>   <pre><code>root     1297291 1296907  0 12:00 pts/0    00:00:00 grep mkfs\n</code></pre>   <pre><code>ps -ef |grep umount\n</code></pre>   <pre><code>root     1298500 1296907  0 12:01 pts/0    00:00:00 grep umount\n</code></pre>   <pre><code>ps -ef |grep unmap\n</code></pre>   <pre><code>root     1298578 1296907  0 12:01 pts/0    00:00:00 grep unmap\n</code></pre>   <p>If any commands are stuck check the dmesg logs from the node. Restarting the <code>csi-rbdplugin</code> pod also may help sometimes.</p> <p>If you don\u2019t see any stuck messages, confirm the network connectivity, Ceph health, and slow ops.</p>","title":"RBD Stale operations"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#dmesg-logs","text":"<p>Check the dmesg logs on the node where pvc mounting is failing or the <code>csi-rbdplugin</code> container of the <code>csi-rbdplugin-xxxx</code> pod on that node.</p> <pre><code>dmesg\n</code></pre>","title":"dmesg logs"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#rbd-commands","text":"<p>If nothing else helps, get the last executed command from the ceph-csi pod logs and run it manually inside the provisioner or plugin pod to see if there are errors returned even if they couldn't be seen in the logs.</p> <pre><code>$ rbd ls --id=csi-rbd-node -m=10.111.136.166:6789 --key=AQDpIQhg+v83EhAAgLboWIbl+FL/nThJzoI3Fg==\n</code></pre>  <p>Where <code>-m</code> is one of the mon endpoints and the <code>--key</code> is the key used by the CSI driver for accessing the Ceph cluster.</p>","title":"RBD Commands"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#node-loss","text":"<p>When a node is lost, you will see application pods on the node stuck in the <code>Terminating</code> state while another pod is rescheduled and is in the <code>ContainerCreating</code> state.</p> <p>To allow the application pod to start on another node, force delete the pod.</p>","title":"Node Loss"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#force-deleting-the-pod","text":"<p>To force delete the pod stuck in the <code>Terminating</code> state:</p> <pre><code>$ kubectl -n rook-ceph delete pod my-app-69cd495f9b-nl6hf --grace-period 0 --force\n</code></pre>  <p>After the force delete, wait for a timeout of about 8-10 minutes. If the pod still not in the running state, continue with the next section to blocklist the node.</p>","title":"Force deleting the pod"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#blocklisting-a-node","text":"<p>To shorten the timeout, you can mark the node as \"blocklisted\" from the Rook toolbox so Rook can safely failover the pod sooner.</p> <p>If the Ceph version is at least Pacific(v16.2.x), run the following command:</p> <pre><code>$ ceph osd blocklist add &lt;NODE_IP&gt; # get the node IP you want to blocklist\nblocklisting &lt;NODE_IP&gt;\n</code></pre>  <p>If the Ceph version is Octopus(v15.2.x) or older, run the following command:</p> <pre><code>$ ceph osd blacklist add &lt;NODE_IP&gt; # get the node IP you want to blacklist\nblacklisting &lt;NODE_IP&gt;\n</code></pre>  <p>After running the above command within a few minutes the pod will be running.</p>","title":"Blocklisting a node"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-troubleshooting/#removing-a-node-blocklist","text":"<p>After you are absolutely sure the node is permanently offline and that the node no longer needs to be blocklisted, remove the node from the blocklist.</p> <p>If the Ceph version is at least Pacific(v16.2.x), run:</p> <pre><code>$ ceph osd blocklist rm &lt;NODE_IP&gt;\nun-blocklisting &lt;NODE_IP&gt;\n</code></pre>  <p>If the Ceph version is Octopus(v15.2.x) or older, run:</p> <pre><code>$ ceph osd blacklist rm &lt;NODE_IP&gt; # get the node IP you want to blacklist\nun-blacklisting &lt;NODE_IP&gt;\n</code></pre>","title":"Removing a node blocklist"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/","text":"<p>The CSI Volume Cloning feature adds support for specifying existing PVCs in the <code>dataSource</code> field to indicate a user would like to clone a Volume.</p> <p>A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be. The only difference is that upon provisioning, rather than creating a \"new\" empty Volume, the back end device creates an exact duplicate of the specified Volume.</p> <p>Refer to clone-doc for more info.</p>","title":"Volume clone"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#rbd-volume-cloning","text":"","title":"RBD Volume Cloning"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#volume-clone-prerequisites","text":"<ol> <li>Requires Kubernetes v1.16+ which supports volume clone.</li> <li>Ceph-csi diver v3.0.0+ which supports volume clone.</li> </ol>","title":"Volume Clone Prerequisites"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#volume-cloning","text":"<p>In pvc-clone, <code>dataSource</code> should be the name of the <code>PVC</code> which is already created by RBD CSI driver. The <code>dataSource</code> kind should be the <code>PersistentVolumeClaim</code> and also storageclass should be same as the source <code>PVC</code>.</p> <p>Create a new PVC Clone from the PVC</p> <pre><code>kubectl create -f deploy/examples/csi/rbd/pvc-clone.yaml\n</code></pre>","title":"Volume Cloning"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#verify-rbd-volume-clone-pvc-creation","text":"<pre><code>kubectl get pvc\n</code></pre>   <pre><code>NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\nrbd-pvc           Bound    pvc-74734901-577a-11e9-b34f-525400581048   1Gi        &gt;RWO            rook-ceph-block       34m\nrbd-pvc-clone     Bound    pvc-70473135-577f-11e9-b34f-525400581048   1Gi        RWO            rook-ceph-block       8s\n</code></pre>","title":"Verify RBD volume Clone PVC Creation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#rbd-clone-resource-cleanup","text":"<p>To clean your cluster of the resources created by this example, run the following:</p> <pre><code>kubectl delete -f deploy/examples/csi/rbd/pvc-clone.yaml\n</code></pre>","title":"RBD clone resource Cleanup"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#cephfs-volume-cloning","text":"","title":"CephFS Volume Cloning"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#volume-clone-prerequisites_1","text":"<ol> <li>Requires Kubernetes v1.16+ which supports volume clone.</li> <li>Ceph-csi diver v3.1.0+ which supports volume clone.</li> </ol>","title":"Volume Clone Prerequisites"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#volume-cloning_1","text":"<p>In pvc-clone, <code>dataSource</code> should be the name of the <code>PVC</code> which is already created by CephFS CSI driver. The <code>dataSource</code> kind should be the <code>PersistentVolumeClaim</code> and also storageclass should be same as the source <code>PVC</code>.</p> <p>Create a new PVC Clone from the PVC</p> <pre><code>kubectl create -f deploy/examples/csi/cephfs/pvc-clone.yaml\n</code></pre>","title":"Volume Cloning"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#verify-cephfs-volume-clone-pvc-creation","text":"<pre><code>kubectl get pvc\n</code></pre>   <pre><code>NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc        Bound    pvc-1ea51547-a88b-4ab0-8b4a-812caeaf025d   1Gi        RWX            rook-cephfs    39m\ncephfs-pvc-clone  Bound    pvc-b575bc35-d521-4c41-b4f9-1d733cd28fdf   1Gi        RWX            rook-cephfs    8s\n</code></pre>","title":"Verify CephFS volume Clone PVC Creation"},{"location":"Usages-and-Examples/Ceph-CSI/ceph-csi-volume-clone/#cephfs-clone-resource-cleanup","text":"<p>To clean your cluster of the resources created by this example, run the following:</p> <pre><code>kubectl delete -f deploy/examples/csi/cephfs/pvc-clone.yaml\n</code></pre>","title":"CephFS clone resource Cleanup"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/","text":"<p>A shared filesystem can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared filesystem.</p> <p>This example runs a shared filesystem for the kube-registry.</p>","title":"Shared Filesystem"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#prerequisites","text":"<p>This guide assumes you have created a Rook cluster as explained in the main Kubernetes guide</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#multiple-filesystems-support","text":"<p>Multiple filesystems are supported as of the Ceph Pacific release.</p>","title":"Multiple Filesystems Support"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#create-the-filesystem","text":"<p>Create the filesystem by specifying the desired settings for the metadata pool, data pools, and metadata server in the <code>CephFilesystem</code> CRD. In this example we create the metadata pool with replication of three and a single data pool with replication of three. For more options, see the documentation on creating shared filesystems.</p> <p>Save this shared filesystem definition as <code>filesystem.yaml</code>:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephFilesystem\nmetadata:\n  name: myfs\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    replicated:\n      size: 3\n  dataPools:\n    - name: replicated\n      replicated:\n        size: 3\n  preserveFilesystemOnDelete: true\n  metadataServer:\n    activeCount: 1\n    activeStandby: true\n</code></pre>  <p>The Rook operator will create all the pools and other resources necessary to start the service. This may take a minute to complete.</p> <pre><code># Create the filesystem\nkubectl create -f filesystem.yaml\n[...]\n</code></pre>  <pre><code># To confirm the filesystem is configured, wait for the mds pods to start\nkubectl -n rook-ceph get pod -l app=rook-ceph-mds\n</code></pre>   <pre><code>NAME                                      READY     STATUS    RESTARTS   AGE\nrook-ceph-mds-myfs-7d59fdfcf4-h8kw9       1/1       Running   0          12s\nrook-ceph-mds-myfs-7d59fdfcf4-kgkjp       1/1       Running   0          12s\n</code></pre>   <p>To see detailed status of the filesystem, start and connect to the Rook toolbox. A new line will be shown with <code>ceph status</code> for the <code>mds</code> service. In this example, there is one active instance of MDS which is up, with one MDS instance in <code>standby-replay</code> mode in case of failover.</p> <pre><code>ceph status\n</code></pre>   <pre><code>  ...\n  services:\n    mds: myfs-1/1/1 up {[myfs:0]=mzw58b=up:active}, 1 up:standby-replay\n</code></pre>","title":"Create the Filesystem"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#provision-storage","text":"<p>Before Rook can start provisioning storage, a StorageClass needs to be created based on the filesystem. This is needed for Kubernetes to interoperate with the CSI driver to create persistent volumes.</p> <p>Save this storage class definition as <code>storageclass.yaml</code>:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-cephfs\n# Change \"rook-ceph\" provisioner prefix to match the operator namespace if needed\nprovisioner: rook-ceph.cephfs.csi.ceph.com\nparameters:\n  # clusterID is the namespace where the rook cluster is running\n  # If you change this namespace, also change the namespace below where the secret namespaces are defined\n  clusterID: rook-ceph\n\n  # CephFS filesystem name into which the volume shall be created\n  fsName: myfs\n\n  # Ceph pool into which the volume shall be created\n  # Required for provisionVolume: \"true\"\n  pool: myfs-replicated\n\n  # The secrets contain Ceph admin credentials. These are generated automatically by the operator\n  # in the same namespace as the cluster.\n  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner\n  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner\n  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph\n  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node\n  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\n\nreclaimPolicy: Delete\n</code></pre>  <p>If you've deployed the Rook operator in a namespace other than \"rook-ceph\" as is common change the prefix in the provisioner to match the namespace you used. For example, if the Rook operator is running in \"rook-op\" the provisioner value should be \"rook-op.rbd.csi.ceph.com\".</p> <p>Create the storage class.</p> <pre><code>kubectl create -f deploy/examples/csi/cephfs/storageclass.yaml\n</code></pre>","title":"Provision Storage"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#quotas","text":"<p>IMPORTANT: The CephFS CSI driver uses quotas to enforce the PVC size requested. Only newer kernels support CephFS quotas (kernel version of at least 4.17). If you require quotas to be enforced and the kernel driver does not support it, you can disable the kernel driver and use the FUSE client. This can be done by setting <code>CSI_FORCE_CEPHFS_KERNEL_CLIENT: false</code> in the operator deployment (<code>operator.yaml</code>). However, it is important to know that when the FUSE client is enabled, there is an issue that during upgrade the application pods will be disconnected from the mount and will need to be restarted. See the upgrade guide for more details.</p>","title":"Quotas"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#consume-the-shared-filesystem-k8s-registry-sample","text":"<p>As an example, we will start the kube-registry pod with the shared filesystem as the backing store. Save the following spec as <code>kube-registry.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc\n  namespace: kube-system\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: rook-cephfs\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-registry\n  namespace: kube-system\n  labels:\n    k8s-app: kube-registry\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: kube-registry\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-registry\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        # Configuration reference: https://docs.docker.com/registry/configuration/\n        - name: REGISTRY_HTTP_ADDR\n          value: :5000\n        - name: REGISTRY_HTTP_SECRET\n          value: \"Ple4seCh4ngeThisN0tAVerySecretV4lue\"\n        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY\n          value: /var/lib/registry\n        volumeMounts:\n        - name: image-store\n          mountPath: /var/lib/registry\n        ports:\n        - containerPort: 5000\n          name: registry\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: registry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: registry\n      volumes:\n      - name: image-store\n        persistentVolumeClaim:\n          claimName: cephfs-pvc\n          readOnly: false\n</code></pre>  <p>Create the Kube registry deployment:</p> <pre><code>kubectl create -f deploy/examples/csi/cephfs/kube-registry.yaml\n</code></pre>  <p>You now have a docker registry which is HA with persistent storage.</p>","title":"Consume the Shared Filesystem: K8s Registry Sample"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#kernel-version-requirement","text":"<p>If the Rook cluster has more than one filesystem and the application pod is scheduled to a node with kernel version older than 4.7, inconsistent results may arise since kernels older than 4.7 do not support specifying filesystem namespaces.</p>","title":"Kernel Version Requirement"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#consume-the-shared-filesystem-toolbox","text":"<p>Once you have pushed an image to the registry (see the instructions to expose and use the kube-registry), verify that kube-registry is using the filesystem that was configured above by mounting the shared filesystem in the toolbox pod. See the Direct Filesystem topic for more details.</p>","title":"Consume the Shared Filesystem: Toolbox"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#teardown","text":"<p>To clean up all the artifacts created by the filesystem demo:</p> <pre><code>kubectl delete -f kube-registry.yaml\n</code></pre>  <p>To delete the filesystem components and backing data, delete the Filesystem CRD.</p>  <p>WARNING: Data will be deleted if preserveFilesystemOnDelete=false.</p>  <pre><code>kubectl -n rook-ceph delete cephfilesystem myfs\n</code></pre>  <p>Note: If the \"preserveFilesystemOnDelete\" filesystem attribute is set to true, the above command won't delete the filesystem. Recreating the same CRD will reuse the existing filesystem.</p>","title":"Teardown"},{"location":"Usages-and-Examples/CephFS-Filesystem-Storage/ceph-filesystem/#advanced-example-erasure-coded-filesystem","text":"<p>The Ceph filesystem example can be found here: Ceph Shared Filesystem - Samples - Erasure Coded.</p>","title":"Advanced Example: Erasure Coded Filesystem"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-claim/","text":"<p>Rook supports the creation of new buckets and access to existing buckets via two custom resources:</p> <ul> <li>an <code>Object Bucket Claim (OBC)</code> is custom resource which requests a bucket (new or existing) and is described by a Custom Resource Definition (CRD) shown below.</li> <li>an <code>Object Bucket (OB)</code> is a custom resource automatically generated when a bucket is provisioned. It is a global resource, typically not visible to non-admin users, and contains information specific to the bucket. It is described by an OB CRD, also shown below.</li> </ul> <p>An OBC references a storage class which is created by an administrator. The storage class defines whether the bucket requested is a new bucket or an existing bucket. It also defines the bucket retention policy. Users request a new or existing bucket by creating an OBC which is shown below. The ceph provisioner detects the OBC and creates a new bucket or grants access to an existing bucket, depending the the storage class referenced in the OBC. It also generates a Secret which provides credentials to access the bucket, and a ConfigMap which contains the bucket's endpoint. Application pods consume the information in the Secret and ConfigMap to access the bucket. Please note that to make provisioner watch the cluster namespace only you need to set <code>ROOK_OBC_WATCH_OPERATOR_NAMESPACE</code> to <code>true</code> in the operator manifest, otherwise it watches all namespaces.</p>","title":"Ceph Object Bucket Claim"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-claim/#sample","text":"","title":"Sample"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-claim/#obc-custom-resource","text":"<p><pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: ceph-bucket [1]\n  namespace: rook-ceph [2]\nspec:\n  bucketName: [3]\n  generateBucketName: photo-booth [4]\n  storageClassName: rook-ceph-bucket [4]\n  additionalConfig: [5]\n    maxObjects: \"1000\"\n    maxSize: \"2G\"\n</code></pre>  1. <code>name</code> of the <code>ObjectBucketClaim</code>. This name becomes the name of the Secret and ConfigMap. 1. <code>namespace</code>(optional) of the <code>ObjectBucketClaim</code>, which is also the namespace of the ConfigMap and Secret. 1. <code>bucketName</code> name of the <code>bucket</code>. Not recommended for new buckets since names must be unique within an entire object store. 1. <code>generateBucketName</code> value becomes the prefix for a randomly generated name, if supplied then <code>bucketName</code> must be empty. If both <code>bucketName</code> and <code>generateBucketName</code> are supplied then <code>BucketName</code> has precedence and <code>GenerateBucketName</code> is ignored. If both <code>bucketName</code> and <code>generateBucketName</code> are blank or omitted then the storage class is expected to contain the name of an existing bucket. It's an error if all three bucket related names are blank or omitted. 1. <code>storageClassName</code> which defines the StorageClass which contains the names of the bucket provisioner, the object-store and specifies the bucket retention policy. 1. <code>additionalConfig</code> is an optional list of key-value pairs used to define attributes specific to the bucket being provisioned by this OBC. This information is typically tuned to a particular bucket provisioner and may limit application portability. Options supported:   - <code>maxObjects</code>: The maximum number of objects in the bucket   - <code>maxSize</code>: The maximum size of the bucket, please note minimum recommended value is 4K.</p>","title":"OBC Custom Resource"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-claim/#obc-custom-resource-after-bucket-provisioning","text":"<p><pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  creationTimestamp: \"2019-10-18T09:54:01Z\"\n  generation: 2\n  name: ceph-bucket\n  namespace: default [1]\n  resourceVersion: \"559491\"\nspec:\n  ObjectBucketName: obc-default-ceph-bucket [2]\n  additionalConfig: null\n  bucketName: photo-booth-c1178d61-1517-431f-8408-ec4c9fa50bee [3]\n  storageClassName: rook-ceph-bucket [4]\nstatus:\n  phase: Bound [5]\n</code></pre>  1. <code>namespace</code> where OBC got created. 1. <code>ObjectBucketName</code> generated OB name created using name space and OBC name. 1. the generated (in this case), unique <code>bucket name</code> for the new bucket. 1. name of the storage class from OBC got created. 1. phases of bucket creation:     - Pending: the operator is processing the request.     - Bound: the operator finished processing the request and linked the OBC and OB     - Released: the OB has been deleted, leaving the OBC unclaimed but unavailable.     - Failed: not currently set.</p>","title":"OBC Custom Resource after Bucket Provisioning"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-claim/#app-pod","text":"<p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\n  namespace: dev-user\nspec:\n  containers:\n  - name: mycontainer\n    image: redis\n    envFrom: [1]\n    - configMapRef:\n        name: ceph-bucket [2]\n    - secretRef:\n        name: ceph-bucket [3]\n</code></pre>  1. use <code>env:</code> if mapping of the defined key names to the env var names used by the app is needed. 1. makes available to the pod as env variables: <code>BUCKET_HOST</code>, <code>BUCKET_PORT</code>, <code>BUCKET_NAME</code> 1. makes available to the pod as env variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></p>","title":"App Pod"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-claim/#storageclass","text":"<p><pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-ceph-bucket\n  labels:\n    aws-s3/object [1]\nprovisioner: rook-ceph.ceph.rook.io/bucket [2]\nparameters: [3]\n  objectStoreName: my-store\n  objectStoreNamespace: rook-ceph\n  bucketName: ceph-bucket [4]\nreclaimPolicy: Delete [5]\n</code></pre>  1. <code>label</code>(optional) here associates this <code>StorageClass</code> to a specific provisioner. 1. <code>provisioner</code> responsible for handling <code>OBCs</code> referencing this <code>StorageClass</code>. 1. all <code>parameter</code> required. 1. <code>bucketName</code> is required for access to existing buckets but is omitted when provisioning new buckets. Unlike greenfield provisioning, the brownfield bucket name appears in the <code>StorageClass</code>, not the <code>OBC</code>. 1. rook-ceph provisioner decides how to treat the <code>reclaimPolicy</code> when an <code>OBC</code> is deleted for the bucket. See explanation as specified in Kubernetes + Delete = physically delete the bucket. + Retain = do not physically delete the bucket.</p>","title":"StorageClass"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/","text":"<p>This feature is experimental</p>  <p>Rook supports the creation of bucket notifications via two custom resources:</p> <ul> <li>a <code>CephBucketNotification</code> is a custom resource the defines: topic, events and filters of a bucket notification, and is described by a Custom Resource Definition (CRD) shown below. Bucket notifications are associated with a bucket by setting labels on the Object Bucket claim (OBC). See the Ceph documentation for detailed information: https://docs.ceph.com/en/latest/radosgw/notifications/.</li> <li>a <code>CephBucketTopic</code> is custom resource which represents a bucket notification topic and is described by a CRD shown below. A bucket notification topic represents an endpoint (or a \"topic\" inside this endpoint) to which bucket notifications could be sent.</li> </ul>","title":"Ceph Object Bucket Notifications"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#notifications","text":"<p>A CephBucketNotification defines what bucket actions trigger the notification and which topic to send notifications to. A CephBucketNotification may also define a filter, based on the object's name and other object attributes. Notifications can be associated with buckets created via ObjectBucketClaims by adding labels to an ObjectBucketClaim with the following format: <pre><code>bucket-notification-&lt;notification name&gt;: &lt;notification name&gt;\n</code></pre>  The CephBucketTopic, CephBucketNotification and ObjectBucketClaim must all belong to the same namespace. If a bucket was created manually (not via an ObjectBucketClaim), notifications on this bucket should also be created manually. However, topics in these notifications may reference topics that were created via CephBucketTopic resources.</p>","title":"Notifications"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#topics","text":"<p>A CephBucketTopic represents an endpoint (of types: Kafka, AMQP0.9.1 or HTTP), or a specific resource inside this endpoint (e.g a Kafka or an AMQP topic, or a specific URI in an HTTP server). The CephBucketTopic also holds any additional info needed for a CephObjectStore's RADOS Gateways (RGW) to connect to the endpoint. Topics don't belong to a specific bucket or notification. Notifications from multiple buckets may be sent to the same topic, and one bucket (via multiple CephBucketNotifications) may send notifications to multiple topics.</p>","title":"Topics"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#notification-reliability-and-delivery","text":"<p>Notifications may be sent synchronously, as part of the operation that triggered them. In this mode, the operation is acknowledged only after the notification is sent to the topic\u2019s configured endpoint, which means that the round trip time of the notification is added to the latency of the operation itself. The original triggering operation will still be considered as successful even if the notification fail with an error, cannot be delivered or times out.</p> <p>Notifications may also be sent asynchronously. They will be committed into persistent storage and then asynchronously sent to the topic\u2019s configured endpoint. In this case, the only latency added to the original operation is of committing the notification to persistent storage. If the notification fail with an error, cannot be delivered or times out, it will be retried until successfully acknowledged.</p>","title":"Notification Reliability and Delivery"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#sample","text":"","title":"Sample"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#cephbuckettopic-custom-resource","text":"<p><pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: CephBucketTopic\nmetadata:\n  name: my-topic [1]\n  namespace: my-app-space [2]\nspec:\n  objectStoreName: my-store [3]\n  objectStoreNamespace: rook-ceph [4]\n  opaqueData: my@email.com [5]\n  persistent: false [6]\n  endpoint: [7]\n    http: [8]\n      uri: http://my-notification-endpoint:8080\n#     uri: http://my-notification-endpoint:8080/my-topic\n#     uri: https://my-notification-endpoint:8443\n      disableVerifySSL: true [9]\n      sendCloudEvents: false [10]\n#   amqp: [11]\n#     uri: amqp://my-rabbitmq-service:5672\n#     uri: amqp://my-rabbitmq-service:5672/vhost1\n#     uri: amqps://user@password:my-rabbitmq-service:5672\n#     disableVerifySSL: true [12]\n#     ackLevel: broker [13]\n#     exchange: my-exchange [14]\n#   kafka: [15]\n#     uri: kafka://my-kafka-service:9092\n#     disableVerifySSL: true [16]\n#     ackLevel: broker [17]\n#     useSSL: false [18]\n</code></pre>  1. <code>name</code> of the <code>CephBucketTopic</code>    - In case of AMQP endpoint, the name is used for the AMQP topic (\u201crouting key\u201d for a topic exchange)    - In case of Kafka endpoint, the name is used as the Kafka topic 1. <code>namespace</code>(optional) of the <code>CephBucketTopic</code>. Should match the namespace of the CephBucketNotification associated with this CephBucketTopic, and the OBC with the label referencing the CephBucketNotification 1. <code>objectStoreName</code> is the name of the object store in which the topic should be created. This must be the same object store used for the buckets associated with the notifications referencing this topic. 1. <code>objectStoreNamespace</code> is the namespace of the object store in which the topic should be created 1. <code>opaqueData</code> (optional) is added to all notifications triggered by a notifications associated with the topic 1. <code>persistent</code> (optional) indicates whether notifications to this endpoint are persistent (=asynchronous) or sent synchronously (\u201cfalse\u201d by default) 1. <code>endpoint</code> to which to send the notifications to. Exactly one of the endpoints must be defined: <code>http</code>, <code>amqp</code>, <code>kafka</code> 1. <code>http</code> (optional) hold the spec for an HTTP endpoint. The format of the URI would be: <code>http[s]://&lt;fqdn&gt;[:&lt;port&gt;][/&lt;resource&gt;]</code>    - port defaults to: 80/443 for HTTP/S accordingly 1. <code>disableVerifySSL</code> indicates whether the RGW is going to verify the SSL certificate of the HTTP server in case HTTPS is used (\"false\" by default) 1. <code>amqp</code> (optional) hold the spec for an AMQP endpoint. The format of the URI would be: <code>amqp[s]://[&lt;user&gt;:&lt;password&gt;@]&lt;fqdn&gt;[:&lt;port&gt;][/&lt;vhost&gt;]</code>    - port defaults to: 5672/5671 for AMQP/S accordingly    - user/password defaults to: guest/guest    - user/password may only be provided if HTTPS is used with the RGW. If not, topic creation request will be rejected    - vhost defaults to: \u201c/\u201d 1. <code>disableVerifySSL</code> (optional) indicates whether the RGW is going to verify the SSL certificate of the AMQP server in case AMQPS is used (\"false\" by default) 1. <code>sendCloudEvents</code>: (optional) send the notifications with the CloudEvents header. Supported for Ceph Quincy (v17) or newer (\"false\" by default) 1. <code>ackLevel</code> (optional) indicates what kind of ack the RGW is waiting for after sending the notifications:    - \u201cnone\u201d: message is considered \u201cdelivered\u201d if sent to broker    - \u201cbroker\u201d: message is considered \u201cdelivered\u201d if acked by broker (default)    - \u201croutable\u201d: message is considered \u201cdelivered\u201d if broker can route to a consumer 1. <code>exchange</code> in the AMQP broker that would route the notifications. Different topics pointing to the same endpoint must use the same exchange 1. <code>kafka</code> (optional) hold the spec for a Kafka endpoint. The format of the URI would be: <code>kafka://[&lt;user&gt;:&lt;password&gt;@]&lt;fqdn&gt;[:&lt;port]</code>    - port defaults to: 9092    - user/password may only be provided if HTTPS is used with the RGW. If not, topic creation request will be rejected    - user/password may only be provided together with <code>useSSL</code>, if not, the connection to the broker would fail 1. <code>disableVerifySSL</code> (optional) indicates whether the RGW is going to verify the SSL certificate of the Kafka server in case <code>useSSL</code> flag is used (\"false\" by default) 1. <code>ackLevel</code> (optional) indicates what kind of ack the RGW is waiting for after sending the notifications:    - \u201cnone\u201d: message is considered \u201cdelivered\u201d if sent to broker    - \u201cbroker\u201d: message is considered \u201cdelivered\u201d if acked by broker (default) 1. <code>useSSL</code> (optional) indicates that secure connection will be used for connecting with the broker (\u201cfalse\u201d by default)</p>  <p>Note that n case of Kafka and AMQP, the consumer of the notifications is not required to ack the notifications, since the broker persists the messages before delivering them to their final destinations</p>","title":"CephBucketTopic Custom Resource"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#cephbucketnotification-custom-resource","text":"<p><pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBucketNotification\nmetadata:\n  name: my-notification [1]\n  namespace: my-app-space [2]\nspec:\n  topic: my-topic [3]\n  filter: [4]\n    keyFilters: [5]\n      # match objects with keys that start with \"hello\"\n      - name: prefix\n        value: hello\n      # match objects with keys that end with \".png\"\n      - name: suffix\n        value: .png\n      # match objects with keys with only lowercase characters\n      - name: regex\n        value: \"[a-z]*\\\\.*\"\n    metadataFilters: [6]\n      - name: x-amz-meta-color\n        value: blue\n      - name: x-amz-meta-user-type\n        value: free\n    tagFilters: [7]\n      - name: project\n        value: brown\n  # notification apply for any of the events\n  # full list of supported events is here:\n  # https://docs.ceph.com/en/latest/radosgw/s3-notification-compatibility/#event-types\n  events: [8]\n    - s3:ObjectCreated:Put\n    - s3:ObjectCreated:Copy\n</code></pre>  1. <code>name</code> of the <code>CephBucketNotification</code> 1. <code>namespace</code>(optional) of the <code>CephBucketNotification</code>. Should match the namespace of the CephBucketTopic referenced in [3], and the OBC with the label referencing the CephBucketNotification 1. <code>topic</code> to which the notifications should be sent 1. <code>filter</code> (optional) holds a list of filtering rules of different types. Only objects that match all the filters will trigger notification sending 1. <code>keyFilter</code> (optional) are filters based on the object key. There could be up to 3 key filters defined: <code>prefix</code>, <code>suffix</code> and <code>regex</code> 1. <code>metadataFilters</code> (optional) are filters based on the object metadata. All metadata fields defined as filters must exists in the object, with the values defined in the filter. Other metadata fields may exist in the object 1. <code>tagFilters</code> (optional) are filters based on object tags. All tags defined as filters must exists in the object, with the values defined in the filter. Other tags may exist in the object 1. <code>events</code> (optional) is a list of events that should trigger the notifications. By default all events should trigger notifications. Valid Events are:    - s3:ObjectCreated:    - s3:ObjectCreated:Put    - s3:ObjectCreated:Post    - s3:ObjectCreated:Copy    - s3:ObjectCreated:CompleteMultipartUpload    - s3:ObjectRemoved:    - s3:ObjectRemoved:Delete    - s3:ObjectRemoved:DeleteMarkerCreated</p>","title":"CephBucketNotification Custom Resource"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-bucket-notifications/#obc-custom-resource","text":"<p>For a notifications to be associated with a bucket, a labels must be added to the OBC, indicating the name of the notification. To delete a notification from a bucket the matching label must be removed. When an OBC is deleted, all of the notifications associated with the bucket will be deleted as well. <pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: ceph-notification-bucket\n  labels:\n    # labels that don't have this structure: bucket-notification-&lt;name&gt; : &lt;name&gt;\n    # are ignored by the operator's bucket notifications provisioning mechanism\n    some-label: some-value\n    # the following label adds notifications to this bucket\n    bucket-notification-my-notification: my-notification\n    bucket-notification-another-notification: another-notification\nspec:\n  generateBucketName: ceph-bkt\n  storageClassName: rook-ceph-delete-bucket\n</code></pre> </p>","title":"OBC Custom Resource"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/","text":"<p>Multisite is a feature of Ceph that allows object stores to replicate their data over multiple Ceph clusters.</p> <p>Multisite also allows object stores to be independent and isolated from other object stores in a cluster.</p> <p>When a ceph-object-store is created without the <code>zone</code> section; a realm, zone group, and zone is created with the same name as the ceph-object-store.</p> <p>Since it is the only ceph-object-store in the realm, the data in the ceph-object-store remain independent and isolated from others on the same cluster.</p> <p>When a ceph-object-store is created with the <code>zone</code> section, the ceph-object-store will join a custom created zone, zone group, and realm each with a different names than its own.</p> <p>This allows the ceph-object-store to replicate its data over multiple Ceph clusters.</p> <p>To review core multisite concepts please read the ceph-multisite design overview.</p>","title":"Object Multisite"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#prerequisites","text":"<p>This guide assumes a Rook cluster as explained in the Quickstart.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#creating-object-multisite","text":"<p>If an admin wants to set up multisite on a Rook Ceph cluster, the admin should create:</p> <ol> <li>A realm</li> <li>A zonegroup</li> <li>A zone</li> <li>A ceph object store with the <code>zone</code> section</li> </ol> <p>object-multisite.yaml in the examples directory can be used to create the multisite CRDs.</p> <pre><code>kubectl create -f object-multisite.yaml\n</code></pre>  <p>The first zone group created in a realm is the master zone group. The first zone created in a zone group is the master zone.</p> <p>When a non-master zone or non-master zone group is created, the zone group or zone is not in the Ceph Radosgw Multisite Period until an object-store is created in that zone (and zone group).</p> <p>The zone will create the pools for the object-store(s) that are in the zone to use.</p> <p>When one of the multisite CRs (realm, zone group, zone) is deleted the underlying ceph realm/zone group/zone is not deleted, neither are the pools created by the zone. See the \"Multisite Cleanup\" section for more information.</p> <p>For more information on the multisite CRDs please read ceph-object-multisite-crd.</p>","title":"Creating Object Multisite"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#pulling-a-realm","text":"<p>If an admin wants to sync data from another cluster, the admin needs to pull a realm on a Rook Ceph cluster from another Rook Ceph (or Ceph) cluster.</p> <p>To begin doing this, the admin needs 2 pieces of information:</p> <ol> <li>An endpoint from the realm being pulled from</li> <li>The access key and the system key of the system user from the realm being pulled from.</li> </ol>","title":"Pulling a Realm"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#getting-the-pull-endpoint","text":"<p>To pull a Ceph realm from a remote Ceph cluster, an <code>endpoint</code> must be added to the CephObjectRealm's <code>pull</code> section in the <code>spec</code>. This endpoint must be from the master zone in the master zone group of that realm.</p> <p>If an admin does not know of an endpoint that fits this criteria, the admin can find such an endpoint on the remote Ceph cluster (via the tool box if it is a Rook Ceph Cluster) by running:</p> <pre><code>$ radosgw-admin zonegroup get --rgw-realm=$REALM_NAME --rgw-zonegroup=$MASTER_ZONEGROUP_NAME\n</code></pre>   <pre><code>{\n    ...\n    \"endpoints\": [http://10.17.159.77:80],\n    ...\n}\n</code></pre>   <p>A list of endpoints in the master zone group in the master zone is in the <code>endpoints</code> section of the JSON output of the <code>zonegoup get</code> command.</p> <p>This endpoint must also be resolvable from the new Rook Ceph cluster. To test this run the <code>curl</code> command on the endpoint:</p> <pre><code>$ curl -L http://10.17.159.77:80\n</code></pre>   <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n</code></pre>   <p>Finally add the endpoint to the <code>pull</code> section of the CephObjectRealm's spec. The CephObjectRealm should have the same name as the CephObjectRealm/Ceph realm it is pulling from.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectRealm\nmetadata:\n  name: realm-a\n  namespace: rook-ceph\nspec:\n  pull:\n    endpoint: http://10.17.159.77:80\n</code></pre>","title":"Getting the Pull Endpoint"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#getting-realm-access-key-and-secret-key","text":"<p>The access key and secret key of the system user are keys that allow other Ceph clusters to pull the realm of the system user.</p>","title":"Getting Realm Access Key and Secret Key"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#getting-the-realm-access-key-and-secret-key-from-the-rook-ceph-cluster","text":"<p>When an admin creates a ceph-object-realm a system user automatically gets created for the realm with an access key and a secret key.</p> <p>This system user has the name \"$REALM_NAME-system-user\". For the example realm, the uid for the system user is \"realm-a-system-user\".</p> <p>These keys for the user are exported as a kubernetes secret called \"$REALM_NAME-keys\" (ex: realm-a-keys).</p> <p>To get these keys from the cluster the realm was originally created on, run: <pre><code>$ kubectl -n $ORIGINAL_CLUSTER_NAMESPACE get secrets realm-a-keys -o yaml &gt; realm-a-keys.yaml\n</code></pre>  Edit the <code>realm-a-keys.yaml</code> file, and change the <code>namespace</code> with the namespace that the new Rook Ceph cluster exists in.</p> <p>Then create a kubernetes secret on the pulling Rook Ceph cluster with the same secrets yaml file. <pre><code>kubectl create -f realm-a-keys.yaml\n</code></pre> </p>","title":"Getting the Realm Access Key and Secret Key from the Rook Ceph Cluster"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#getting-the-realm-access-key-and-secret-key-from-a-non-rook-ceph-cluster","text":"<p>The access key and the secret key of the system user can be found in the output of running the following command on a non-rook ceph cluster: <pre><code>radosgw-admin user info --uid=\"realm-a-system-user\"\n</code></pre> </p>  <p><code>{    ...    \"keys\": [        {            \"user\": \"realm-a-system-user\"            \"access_key\": \"aSw4blZIKV9nKEU5VC0=\"            \"secret_key\": \"JSlDXFt5TlgjSV9QOE9XUndrLiI5JEo9YDBsJg==\",        }    ],    ... }</code></p>  <p>Then base64 encode the each of the keys and create a <code>.yaml</code> file for the Kubernetes secret from the following template.</p> <p>Only the <code>access-key</code>, <code>secret-key</code>, and <code>namespace</code> sections need to be replaced. <pre><code>apiVersion: v1\ndata:\n  access-key: YVN3NGJsWklLVjluS0VVNVZDMD0=\n  secret-key: SlNsRFhGdDVUbGdqU1Y5UU9FOVhVbmRyTGlJNUpFbzlZREJzSmc9PQ==\nkind: Secret\nmetadata:\n  name: realm-a-keys\n  namespace: $NEW_ROOK_CLUSTER_NAMESPACE\ntype: kubernetes.io/rook\n</code></pre> </p> <p>Finally, create a kubernetes secret on the pulling Rook Ceph cluster with the new secrets yaml file. <pre><code>kubectl create -f realm-a-keys.yaml\n</code></pre> </p>","title":"Getting the Realm Access Key and Secret Key from a Non Rook Ceph Cluster"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#pulling-a-realm-on-a-new-rook-ceph-cluster","text":"<p>Once the admin knows the endpoint and the secret for the keys has been created, the admin should create:</p> <ol> <li>A CephObjectRealm matching to the realm on the other Ceph cluster, with an endpoint as described above.</li> <li>A CephObjectZoneGroup matching the master zone group name or the master CephObjectZoneGroup from the cluster the the realm was pulled from.</li> <li>A CephObjectZone referring to the CephObjectZoneGroup created above.</li> <li>A CephObjectStore referring to the new CephObjectZone resource.</li> </ol> <p>object-multisite-pull-realm.yaml (with changes) in the examples directory can be used to create the multisite CRDs.</p> <pre><code>kubectl create -f object-multisite-pull-realm.yaml\n</code></pre>","title":"Pulling a Realm on a New Rook Ceph Cluster"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#multisite-cleanup","text":"<p>Multisite configuration must be cleaned up by hand. Deleting a realm/zone group/zone CR will not delete the underlying Ceph realm, zone group, zone, or the pools associated with a zone.</p>","title":"Multisite Cleanup"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#realm-deletion","text":"<p>Changes made to the resource's configuration or deletion of the resource are not reflected on the Ceph cluster.</p> <p>When the ceph-object-realm resource is deleted or modified, the realm is not deleted from the Ceph cluster. Realm deletion must be done via the toolbox.</p>","title":"Realm Deletion"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#deleting-a-realm","text":"<p>The Rook toolbox can modify the Ceph Multisite state via the radosgw-admin command.</p> <p>The following command, run via the toolbox, deletes the realm.</p> <pre><code>radosgw-admin realm delete --rgw-realm=realm-a\n</code></pre>","title":"Deleting a Realm"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#zone-group-deletion","text":"<p>Changes made to the resource's configuration or deletion of the resource are not reflected on the Ceph cluster.</p> <p>When the ceph-object-zone group resource is deleted or modified, the zone group is not deleted from the Ceph cluster. Zone Group deletion must be done through the toolbox.</p>","title":"Zone Group Deletion"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#deleting-a-zone-group","text":"<p>The Rook toolbox can modify the Ceph Multisite state via the radosgw-admin command.</p> <p>The following command, run via the toolbox, deletes the zone group.</p> <pre><code>radosgw-admin zonegroup delete --rgw-realm=realm-a --rgw-zonegroup=zone-group-a\nradosgw-admin period update --commit --rgw-realm=realm-a --rgw-zonegroup=zone-group-a\n</code></pre>","title":"Deleting a Zone Group"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#deleting-and-reconfiguring-the-ceph-object-zone","text":"<p>Changes made to the resource's configuration or deletion of the resource are not reflected on the Ceph cluster.</p> <p>When the ceph-object-zone resource is deleted or modified, the zone is not deleted from the Ceph cluster. Zone deletion must be done through the toolbox.</p>","title":"Deleting and Reconfiguring the Ceph Object Zone"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#changing-the-master-zone","text":"<p>The Rook toolbox can change the master zone in a zone group.</p> <pre><code>radosgw-admin zone modify --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a --master\nradosgw-admin zonegroup modify --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --master\nradosgw-admin period update --commit --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\n</code></pre>","title":"Changing the Master Zone"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#deleting-zone","text":"<p>The Rook toolbox can modify the Ceph Multisite state via the radosgw-admin command.</p> <p>There are two scenarios possible when deleting a zone. The following commands, run via the toolbox, deletes the zone if there is only one zone in the zone group.</p> <pre><code>radosgw-admin zone delete --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\nradosgw-admin period update --commit --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\n</code></pre>  <p>In the other scenario, there are more than one zones in a zone group.</p> <p>Care must be taken when changing which zone is the master zone.</p> <p>Please read the following documentation before running the below commands:</p> <p>The following commands, run via toolboxes, remove the zone from the zone group first, then delete the zone.</p> <pre><code>radosgw-admin zonegroup rm --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\nradosgw-admin period update --commit --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\nradosgw-admin zone delete --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\nradosgw-admin period update --commit --rgw-realm=realm-a --rgw-zonegroup=zone-group-a --rgw-zone=zone-a\n</code></pre>  <p>When a zone is deleted, the pools for that zone are not deleted.</p>","title":"Deleting Zone"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#deleting-pools-for-a-zone","text":"<p>The Rook toolbox can delete pools. Deleting pools should be done with caution.</p> <p>The following documentation on pools should be read before deleting any pools.</p> <p>When a zone is created the following pools are created for each zone: <pre><code>$ZONE_NAME.rgw.control\n$ZONE_NAME.rgw.meta\n$ZONE_NAME.rgw.log\n$ZONE_NAME.rgw.buckets.index\n$ZONE_NAME.rgw.buckets.non-ec\n$ZONE_NAME.rgw.buckets.data\n</code></pre>  Here is an example command to delete the .rgw.buckets.data pool for zone-a.</p> <pre><code>ceph osd pool rm zone-a.rgw.buckets.data zone-a.rgw.buckets.data --yes-i-really-really-mean-it\n</code></pre>  <p>In this command the pool name must be mentioned twice for the pool to be removed.</p>","title":"Deleting Pools for a Zone"},{"location":"Usages-and-Examples/Object-Storage/ceph-object-multisite/#removing-an-object-store-from-a-zone","text":"<p>When an object-store (created in a zone) is deleted, the endpoint for that object store is removed from that zone, via <pre><code>kubectl delete -f object-store.yaml\n</code></pre> </p> <p>Removing object store(s) from the master zone of the master zone group should be done with caution. When all of these object-stores are deleted the period cannot be updated and that realm cannot be pulled.</p>","title":"Removing an Object Store from a Zone"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/","text":"<p>Object storage exposes an S3 API to the storage cluster for applications to put and get data.</p>","title":"Object Storage"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#prerequisites","text":"<p>This guide assumes a Rook cluster as explained in the Quickstart.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#configure-an-object-store","text":"<p>Rook has the ability to either deploy an object store in Kubernetes or to connect to an external RGW service. Most commonly, the object store will be configured locally by Rook. Alternatively, if you have an existing Ceph cluster with Rados Gateways, see the external section to consume it from Rook.</p>","title":"Configure an Object Store"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#create-a-local-object-store","text":"<p>The below sample will create a <code>CephObjectStore</code> that starts the RGW service in the cluster with an S3 API.</p>  <p>NOTE: This sample requires at least 3 bluestore OSDs, with each OSD located on a different node.</p>  <p>The OSDs must be located on different nodes, because the <code>failureDomain</code> is set to <code>host</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p> <p>See the Object Store CRD, for more detail on the settings available for a <code>CephObjectStore</code>.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStore\nmetadata:\n  name: my-store\n  namespace: rook-ceph\nspec:\n  metadataPool:\n    failureDomain: host\n    replicated:\n      size: 3\n  dataPool:\n    failureDomain: host\n    erasureCoded:\n      dataChunks: 2\n      codingChunks: 1\n  preservePoolsOnDelete: true\n  gateway:\n    sslCertificateRef:\n    port: 80\n    # securePort: 443\n    instances: 1\n  healthCheck:\n    bucket:\n      disabled: false\n      interval: 60s\n</code></pre>  <p>After the <code>CephObjectStore</code> is created, the Rook operator will then create all the pools and other resources necessary to start the service. This may take a minute to complete.</p> <pre><code># Create the object store\nkubectl create -f object.yaml\n\n# To confirm the object store is configured, wait for the rgw pod to start\nkubectl -n rook-ceph get pod -l app=rook-ceph-rgw\n</code></pre>","title":"Create a Local Object Store"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#connect-to-an-external-object-store","text":"<p>Rook can connect to existing RGW gateways to work in conjunction with the external mode of the <code>CephCluster</code> CRD. If you have an external <code>CephCluster</code> CR, you can instruct Rook to consume external gateways with the following:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStore\nmetadata:\n  name: external-store\n  namespace: rook-ceph\nspec:\n  gateway:\n    port: 8080\n    externalRgwEndpoints:\n      - ip: 192.168.39.182\n  healthCheck:\n    bucket:\n      enabled: true\n      interval: 60s\n</code></pre>  <p>You can use the existing <code>object-external.yaml</code> file. When ready the ceph-object-controller will output a message in the Operator log similar to this one:</p>  <pre><code>ceph-object-controller: ceph object store gateway service &gt;running at 10.100.28.138:8080\n</code></pre>   <p>You can now get and access the store via:</p> <pre><code>kubectl -n rook-ceph get svc -l app=rook-ceph-rgw\n</code></pre>   <pre><code>NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nrook-ceph-rgw-my-store   ClusterIP   10.100.28.138   &lt;none&gt;        8080/TCP   6h59m\n</code></pre>   <p>Any pod from your cluster can now access this endpoint:</p> <pre><code>$ curl 10.100.28.138:8080\n</code></pre>   <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n</code></pre>   <p>It is also possible to use the internally registered DNS name:</p> <pre><code>curl rook-ceph-rgw-my-store.rook-ceph:8080\n</code></pre>  <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n</code></pre>  <p>The DNS name is created  with the following schema <code>rook-ceph-rgw-$STORE_NAME.$NAMESPACE</code>.</p>","title":"Connect to an External Object Store"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#create-a-bucket","text":"<p>Now that the object store is configured, next we need to create a bucket where a client can read and write objects. A bucket can be created by defining a storage class, similar to the pattern used by block and file storage. First, define the storage class that will allow object clients to create a bucket. The storage class defines the object storage system, the bucket retention policy, and other properties required by the administrator. Save the following as <code>storageclass-bucket-delete.yaml</code> (the example is named as such due to the <code>Delete</code> reclaim policy).</p> <p><pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: rook-ceph-bucket\n# Change \"rook-ceph\" provisioner prefix to match the operator namespace if needed\nprovisioner: rook-ceph.ceph.rook.io/bucket\nreclaimPolicy: Delete\nparameters:\n  objectStoreName: my-store\n  objectStoreNamespace: rook-ceph\n</code></pre>  If you\u2019ve deployed the Rook operator in a namespace other than <code>rook-ceph</code>, change the prefix in the provisioner to match the namespace you used. For example, if the Rook operator is running in the namespace <code>my-namespace</code> the provisioner value should be <code>my-namespace.ceph.rook.io/bucket</code>. <pre><code>kubectl create -f storageclass-bucket-delete.yaml\n</code></pre> </p> <p>Based on this storage class, an object client can now request a bucket by creating an Object Bucket Claim (OBC). When the OBC is created, the Rook-Ceph bucket provisioner will create a new bucket. Notice that the OBC references the storage class that was created above. Save the following as <code>object-bucket-claim-delete.yaml</code> (the example is named as such due to the <code>Delete</code> reclaim policy):</p> <pre><code>apiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: ceph-bucket\nspec:\n  generateBucketName: ceph-bkt\n  storageClassName: rook-ceph-bucket\n</code></pre>  <pre><code>kubectl create -f object-bucket-claim-delete.yaml\n</code></pre>  <p>Now that the claim is created, the operator will create the bucket as well as generate other artifacts to enable access to the bucket. A secret and ConfigMap are created with the same name as the OBC and in the same namespace. The secret contains credentials used by the application pod to access the bucket. The ConfigMap contains bucket endpoint information and is also consumed by the pod. See the Object Bucket Claim Documentation for more details on the <code>CephObjectBucketClaims</code>.</p>","title":"Create a Bucket"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#client-connections","text":"<p>The following commands extract key pieces of information from the secret and configmap:\"</p> <pre><code>#config-map, secret, OBC will part of default if no specific name space mentioned\nexport AWS_HOST=$(kubectl -n default get cm ceph-bucket -o jsonpath='{.data.BUCKET_HOST}')\nexport PORT=$(kubectl -n default get cm ceph-bucket -o jsonpath='{.data.BUCKET_PORT}')\nexport BUCKET_NAME=$(kubectl -n default get cm ceph-bucket -o jsonpath='{.data.BUCKET_NAME}')\nexport AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-bucket -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 --decode)\nexport AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-bucket -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode)\n</code></pre>","title":"Client Connections"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#consume-the-object-storage","text":"<p>Now that you have the object store configured and a bucket created, you can consume the object storage from an S3 client.</p> <p>This section will guide you through testing the connection to the <code>CephObjectStore</code> and uploading and downloading from it. Run the following commands after you have connected to the Rook toolbox.</p>","title":"Consume the Object Storage"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#connection-environment-variables","text":"<p>To simplify the s3 client commands, you will want to set the four environment variables for use by your client (ie. inside the toolbox). See above for retrieving the variables for a bucket created by an <code>ObjectBucketClaim</code>.</p> <pre><code>export AWS_HOST=&lt;host&gt;\nexport PORT=&lt;port&gt;\nexport AWS_ACCESS_KEY_ID=&lt;accessKey&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;secretKey&gt;\n</code></pre>  <ul> <li><code>Host</code>: The DNS host name where the rgw service is found in the cluster. Assuming you are using the default <code>rook-ceph</code> cluster, it will be <code>rook-ceph-rgw-my-store.rook-ceph.svc</code>.</li> <li><code>Port</code>: The endpoint where the rgw service is listening. Run <code>kubectl -n rook-ceph get svc rook-ceph-rgw-my-store</code>, to get the port.</li> <li><code>Access key</code>: The user's <code>access_key</code> as printed above</li> <li><code>Secret key</code>: The user's <code>secret_key</code> as printed above</li> </ul> <p>The variables for the user generated in this example might be:</p> <pre><code>export AWS_HOST=rook-ceph-rgw-my-store.rook-ceph.svc\nexport PORT=80\nexport AWS_ACCESS_KEY_ID=XEZDB3UJ6X7HVBE7X7MA\nexport AWS_SECRET_ACCESS_KEY=7yGIZON7EhFORz0I40BFniML36D2rl8CQQ5kXU6l\n</code></pre>  <p>The access key and secret key can be retrieved as described in the section above on client connections or below in the section creating a user if you are not creating the buckets with an <code>ObjectBucketClaim</code>.</p>","title":"Connection Environment Variables"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#configure-s5cmd","text":"<p>To test the <code>CephObjectStore</code>, set the object store credentials in the toolbox pod for the <code>s5cmd</code> tool.</p> <pre><code>mkdir ~/.aws\ncat &gt; ~/.aws/credentials &lt;&lt; EOF\n[default]\naws_access_key_id = ${AWS_ACCESS_KEY_ID}\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\nEOF\n</code></pre>","title":"Configure s5cmd"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#put-or-get-an-object","text":"<p>Upload a file to the newly created bucket</p> <pre><code>echo \"Hello Rook\" &gt; /tmp/rookObj\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp /tmp/rookObj s3://$BUCKET_NAME\n</code></pre>  <p>Download and verify the file from the bucket</p> <pre><code>s5cmd --endpoint-url http://$AWS_HOST:$PORT cp s3://$BUCKET_NAME/rookObj /tmp/rookObj-download\ncat /tmp/rookObj-download\n</code></pre>","title":"PUT or GET an object"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#access-external-to-the-cluster","text":"<p>Rook sets up the object storage so pods will have access internal to the cluster. If your applications are running outside the cluster, you will need to setup an external service through a <code>NodePort</code>.</p> <p>First, note the service that exposes RGW internal to the cluster. We will leave this service intact and create a new service for external access.</p> <pre><code>kubectl -n rook-ceph get service rook-ceph-rgw-my-store\n</code></pre>   <pre><code>NAME                     CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE\nrook-ceph-rgw-my-store   10.3.0.177   &lt;none&gt;        80/TCP      2m\n</code></pre>   <p>Save the external service as <code>rgw-external.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: rook-ceph-rgw-my-store-external\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-rgw\n    rook_cluster: rook-ceph\n    rook_object_store: my-store\nspec:\n  ports:\n  - name: rgw\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: rook-ceph-rgw\n    rook_cluster: rook-ceph\n    rook_object_store: my-store\n  sessionAffinity: None\n  type: NodePort\n</code></pre>  <p>Now create the external service.</p> <pre><code>kubectl create -f rgw-external.yaml\n</code></pre>  <p>See both rgw services running and notice what port the external service is running on:</p> <pre><code>kubectl -n rook-ceph get service rook-ceph-rgw-my-store rook-ceph-rgw-my-store-external\n</code></pre>   <pre><code>NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nrook-ceph-rgw-my-store            ClusterIP   10.104.82.228    &lt;none&gt;        80/TCP         4m\nrook-ceph-rgw-my-store-external   NodePort    10.111.113.237   &lt;none&gt;        80:31536/TCP   39s\n</code></pre>   <p>Internally the rgw service is running on port <code>80</code>. The external port in this case is <code>31536</code>. Now you can access the <code>CephObjectStore</code> from anywhere! All you need is the hostname for any machine in the cluster, the external port, and the user credentials.</p>","title":"Access External to the Cluster"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#create-a-user","text":"<p>If you need to create an independent set of user credentials to access the S3 endpoint, create a <code>CephObjectStoreUser</code>. The user will be used to connect to the RGW service in the cluster using the S3 API. The user will be independent of any object bucket claims that you might have created in the earlier instructions in this document.</p> <p>See the Object Store User CRD for more detail on the settings available for a <code>CephObjectStoreUser</code>.</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephObjectStoreUser\nmetadata:\n  name: my-user\n  namespace: rook-ceph\nspec:\n  store: my-store\n  displayName: \"my display name\"\n</code></pre>  <p>When the <code>CephObjectStoreUser</code> is created, the Rook operator will then create the RGW user on the specified <code>CephObjectStore</code> and store the Access Key and Secret Key in a kubernetes secret in the same namespace as the <code>CephObjectStoreUser</code>.</p> <pre><code># Create the object store user\nkubectl create -f object-user.yaml\n</code></pre>  <pre><code># To confirm the object store user is configured, describe the secret\nkubectl -n rook-ceph describe secret rook-ceph-object-user-my-store-my-user\n</code></pre>   <pre><code>Name:      rook-ceph-object-user-my-store-my-user\nNamespace: rook-ceph\nLabels:            app=rook-ceph-rgw\n                 rook_cluster=rook-ceph\n                 rook_object_store=my-store\nAnnotations:   &lt;none&gt;\n\nType:  kubernetes.io/rook\n\nData\n====\nAccessKey: 20 bytes\nSecretKey: 40 bytes\n</code></pre>   <p>The AccessKey and SecretKey data fields can be mounted in a pod as an environment variable. More information on consuming kubernetes secrets can be found in the K8s secret documentation</p> <p>To directly retrieve the secrets:</p> <pre><code>kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o jsonpath='{.data.AccessKey}' | base64 --decode\nkubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o jsonpath='{.data.SecretKey}' | base64 --decode\n</code></pre>","title":"Create a User"},{"location":"Usages-and-Examples/Object-Storage/ceph-object/#object-multisite","text":"<p>Multisite is a feature of Ceph that allows object stores to replicate its data over multiple Ceph clusters.</p> <p>Multisite also allows object stores to be independent and isloated from other object stores in a cluster.</p> <p>For more information on multisite please read the ceph multisite overview for how to run it.</p>","title":"Object Multisite"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/","text":"","title":"RBD Asynchronous DR Failover and Failback"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#table-of-contents","text":"<ul> <li>Planned Migration and Disaster Recovery</li> <li>Planned Migration</li> <li>Relocation</li> <li>Disaster Recovery</li> <li>Failover</li> <li>Failback</li> </ul>","title":"Table of Contents"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#planned-migration-and-disaster-recovery","text":"<p>Rook comes with the volume replication support, which allows users to perform disaster recovery and planned migration of clusters.</p> <p>The following document will help to track the procedure for failover and failback in case of a Disaster recovery or Planned migration use cases.</p>  <p>Note: The document assumes that RBD Mirroring is set up between the peer clusters. For information on rbd mirroring and how to set it up using rook, please refer to the rbd-mirroring guide.</p>","title":"Planned Migration and Disaster Recovery"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#planned-migration","text":"<p>Use cases: Datacenter maintenance, technology refresh, disaster avoidance, etc.</p>","title":"Planned Migration"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#relocation","text":"<p>The Relocation operation is the process of switching production to a  backup facility(normally your recovery site) or vice versa. For relocation,  access to the image on the primary site should be stopped. The image should now be made primary on the secondary cluster so that  the access can be resumed there.</p>  <p> Periodic or one-time backup of the application should be available for restore on the secondary site (cluster-2).</p>  <p>Follow the below steps for planned migration of workload from the primary  cluster to the secondary cluster:</p> <ul> <li>Scale down all the application pods which are using the  mirrored PVC on the Primary Cluster.</li> <li>Take a backup of PVC and PV object from the primary cluster.  This can be done using some backup tools like velero.</li> <li>Update VolumeReplication CR to set <code>replicationState</code> to <code>secondary</code> at the Primary Site.  When the operator sees this change, it will pass the information down to the   driver via GRPC request to mark the dataSource as <code>secondary</code>.</li> <li>If you are manually recreating the PVC and PV on the secondary cluster,  remove the <code>claimRef</code> section in the PV objects. (See this for details)</li> <li>Recreate the storageclass, PVC, and PV objects on the secondary site.</li> <li>As you are creating the static binding between PVC and PV, a new PV won\u2019t  be created here, the PVC will get bind to the existing PV.</li> <li>Create the VolumeReplicationClass on the secondary site.</li> <li>Create VolumeReplications for all the PVC\u2019s for which mirroring  is enabled</li> <li><code>replicationState</code> should be <code>primary</code> for all the PVC\u2019s on    the secondary site.</li> <li>Check VolumeReplication CR status to verify if the image is marked <code>primary</code> on the secondary site.</li> <li>Once the Image is marked as <code>primary</code>, the PVC is now ready  to be used. Now, we can scale up the applications to use the PVC.</li> </ul>  <p> WARNING: In Async Disaster recovery use case, we don't get the complete data. We will only get the crash-consistent data based on the snapshot interval time.</p>","title":"Relocation"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#disaster-recovery","text":"<p>Use cases: Natural disasters, Power failures, System failures, and crashes, etc.</p> <p>NOTE: To effectively resume operations after a failover/relocation, backup of the kubernetes artifacts like deployment, PVC, PV, etc need to be created beforehand by the admin; so that the application can be restored on the peer cluster. For more information, see backup and restore.</p>","title":"Disaster Recovery"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#failover-abrupt-shutdown","text":"<p>In case of Disaster recovery, create VolumeReplication CR at the Secondary Site.  Since the connection to the Primary Site is lost, the operator automatically  sends a GRPC request down to the driver to forcefully mark the dataSource as <code>primary</code>  on the Secondary Site.</p> <ul> <li>If you are manually creating the PVC and PV on the secondary cluster, remove  the claimRef section in the PV objects. (See this for details)</li> <li>Create the storageclass, PVC, and PV objects on the secondary site.</li> <li>As you are creating the static binding between PVC and PV, a new PV won\u2019t be  created here, the PVC will get bind to the existing PV.</li> <li>Create the VolumeReplicationClass and VolumeReplication CR on the secondary site.</li> <li>Check VolumeReplication CR status to verify if the image is marked <code>primary</code> on the secondary site.</li> <li>Once the Image is marked as <code>primary</code>, the PVC is now ready to be used. Now,  we can scale up the applications to use the PVC.</li> </ul>","title":"Failover (abrupt shutdown)"},{"location":"Usages-and-Examples/RBD-Block-Storage/async-disaster-recovery/#failback-post-disaster-recovery","text":"<p>Once the failed cluster is recovered on the primary site and you want to failback  from secondary site, follow the below steps:</p> <ul> <li>Scale down the running applications (if any) on the primary site.  Ensure that all persistent volumes in use by the workload are no  longer in use on the primary cluster.</li> <li>Update VolumeReplication CR replicationState  from <code>primary</code> to <code>secondary</code> on the primary site.</li> <li>Scale down the applications on the secondary site.</li> <li>Update VolumeReplication CR replicationState state from <code>primary</code> to <code>secondary</code> in secondary site.</li> <li>On the primary site, verify the VolumeReplication status is marked as  volume ready to use.</li> <li>Once the volume is marked to ready to use, change the replicationState state  from <code>secondary</code> to <code>primary</code> in primary site.</li> <li>Scale up the applications again on the primary site.</li> </ul>","title":"Failback (post-disaster recovery)"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/","text":"<p>Block storage allows a single pod to mount storage. This guide shows how to create a simple, multi-tier web application on Kubernetes using persistent volumes enabled by Rook.</p>","title":"Block Storage"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#prerequisites","text":"<p>This guide assumes a Rook cluster as explained in the Quickstart.</p>","title":"Prerequisites"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#provision-storage","text":"<p>Before Rook can provision storage, a <code>StorageClass</code> and <code>CephBlockPool</code> need to be created. This will allow Kubernetes to interoperate with Rook when provisioning persistent volumes.</p>  <p>NOTE: This sample requires at least 1 OSD per node, with each OSD located on 3 different nodes.</p>  <p>Each OSD must be located on a different node, because the <code>failureDomain</code> is set to <code>host</code> and the <code>replicated.size</code> is set to <code>3</code>.</p> <p>Save this <code>StorageClass</code> definition as <code>storageclass.yaml</code>:</p> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  failureDomain: host\n  replicated:\n    size: 3\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: rook-ceph-block\n# Change \"rook-ceph\" provisioner prefix to match the operator namespace if needed\nprovisioner: rook-ceph.rbd.csi.ceph.com\nparameters:\n    # clusterID is the namespace where the rook cluster is running\n    clusterID: rook-ceph\n    # Ceph pool into which the RBD image shall be created\n    pool: replicapool\n\n    # (optional) mapOptions is a comma-separated list of map options.\n    # For krbd options refer\n    # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options\n    # For nbd options refer\n    # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options\n    # mapOptions: lock_on_read,queue_depth=1024\n\n    # (optional) unmapOptions is a comma-separated list of unmap options.\n    # For krbd options refer\n    # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options\n    # For nbd options refer\n    # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options\n    # unmapOptions: force\n\n    # RBD image format. Defaults to \"2\".\n    imageFormat: \"2\"\n\n    # RBD image features. Available for imageFormat: \"2\". CSI RBD currently supports only `layering` feature.\n    imageFeatures: layering\n\n    # The secrets contain Ceph admin credentials.\n    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\n    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph\n    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\n    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\n\n    # Specify the filesystem type of the volume. If not specified, csi-provisioner\n    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock\n    # in hyperconverged settings where the volume is mounted on the same node as the osds.\n    csi.storage.k8s.io/fstype: ext4\n\n# Delete the rbd volume when a PVC is deleted\nreclaimPolicy: Delete\n\n# Optional, if you want to add dynamic resize for PVC.\n# For now only ext3, ext4, xfs resize support provided, like in Kubernetes itself.\nallowVolumeExpansion: true\n</code></pre>  <p>If you've deployed the Rook operator in a namespace other than \"rook-ceph\", change the prefix in the provisioner to match the namespace you used. For example, if the Rook operator is running in the namespace \"my-namespace\" the provisioner value should be \"my-namespace.rbd.csi.ceph.com\".</p> <p>Create the storage class.</p> <pre><code>kubectl create -f deploy/examples/csi/rbd/storageclass.yaml\n</code></pre>   <p>NOTE: As specified by Kubernetes, when using the <code>Retain</code> reclaim policy, any Ceph RBD image that is backed by a <code>PersistentVolume</code> will continue to exist even after the <code>PersistentVolume</code> has been deleted. These Ceph RBD images will need to be cleaned up manually using <code>rbd rm</code>.</p>","title":"Provision Storage"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#consume-the-storage-wordpress-sample","text":"<p>We create a sample app to consume the block storage provisioned by Rook with the classic wordpress and mysql apps. Both of these apps will make use of block volumes provisioned by Rook.</p> <p>Start mysql and wordpress from the <code>deploy/examples</code> folder:</p> <pre><code>kubectl create -f mysql.yaml\nkubectl create -f wordpress.yaml\n</code></pre>  <p>Both of these apps create a block volume and mount it to their respective pod. You can see the Kubernetes volume claims by running the following:</p> <pre><code>kubectl get pvc\n</code></pre>   <p><code>NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m</code></p>  <p>Once the wordpress and mysql pods are in the <code>Running</code> state, get the cluster IP of the wordpress app and enter it in your browser:</p> <pre><code>kubectl get svc wordpress\n</code></pre>   <pre><code>NAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE\nwordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m\n</code></pre>   <p>You should see the wordpress app running.</p> <p>If you are using Minikube, the Wordpress URL can be retrieved with this one-line command:</p> <pre><code>echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath='{.spec.ports[0].nodePort}')\n</code></pre>   <p>NOTE: When running in a vagrant environment, there will be no external IP address to reach wordpress with.  You will only be able to reach wordpress via the <code>CLUSTER-IP</code> from inside the Kubernetes cluster.</p>","title":"Consume the storage: Wordpress sample"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#consume-the-storage-toolbox","text":"<p>With the pool that was created above, we can also create a block image and mount it directly in a pod. See the Direct Block Tools topic for more details.</p>","title":"Consume the storage: Toolbox"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#teardown","text":"<p>To clean up all the artifacts created by the block demo:</p> <pre><code>kubectl delete -f wordpress.yaml\nkubectl delete -f mysql.yaml\nkubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool\nkubectl delete storageclass rook-ceph-block\n</code></pre>","title":"Teardown"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#advanced-example-erasure-coded-block-storage","text":"<p>If you want to use erasure coded pool with RBD, your OSDs must use <code>bluestore</code> as their <code>storeType</code>. Additionally the nodes that are going to mount the erasure coded RBD block storage must have Linux kernel &gt;= <code>4.11</code>.</p> <p>NOTE: This example requires at least 3 bluestore OSDs, with each OSD located on a different node.</p> <p>The OSDs must be located on different nodes, because the <code>failureDomain</code> is set to <code>host</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p> <p>To be able to use an erasure coded pool you need to create two pools (as seen below in the definitions): one erasure coded and one replicated.</p>  <p>NOTE: This example requires at least 3 bluestore OSDs, with each OSD located on a different node.</p>  <p>The OSDs must be located on different nodes, because the <code>failureDomain</code> is set to <code>host</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p>","title":"Advanced Example: Erasure Coded Block Storage"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#erasure-coded-csi-driver","text":"<p>The erasure coded pool must be set as the <code>dataPool</code> parameter in <code>storageclass-ec.yaml</code> It is used for the data of the RBD images.</p>","title":"Erasure Coded CSI Driver"},{"location":"Usages-and-Examples/RBD-Block-Storage/ceph-block/#erasure-coded-flex-driver","text":"<p>The erasure coded pool must be set as the <code>dataBlockPool</code> parameter in <code>storageclass-ec.yaml</code>. It is used for the data of the RBD images.</p>","title":"Erasure Coded Flex Driver"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/","text":"","title":"RBD Mirroring"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#disaster-recovery","text":"<p>Disaster recovery (DR) is an organization's ability to react to and recover from an incident that negatively affects business operations. This plan comprises strategies for minimizing the consequences of a disaster, so an organization can continue to operate \u2013 or quickly resume the key operations. Thus, disaster recovery is one of the aspects of business continuity. One of the solutions, to achieve the same, is RBD mirroring.</p>","title":"Disaster Recovery"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#rbd-mirroring_1","text":"<p>RBD mirroring  is an asynchronous replication of RBD images between multiple Ceph clusters.  This capability is available in two modes:</p> <ul> <li>Journal-based: Every write to the RBD image is first recorded  to the associated journal before modifying the actual image.  The remote cluster will read from this associated journal and  replay the updates to its local image.</li> <li>Snapshot-based: This mode uses periodically scheduled or  manually created RBD image mirror-snapshots to replicate  crash-consistent RBD images between clusters.</li> </ul>  <p>Note: This document sheds light on rbd mirroring and how to set it up using rook. For steps on failover or failback scenarios</p>","title":"RBD Mirroring"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#table-of-contents","text":"<ul> <li>Create RBD Pools</li> <li>Bootstrap Peers</li> <li>Configure the RBDMirror Daemon</li> <li>Add mirroring peer information to RBD pools</li> <li>Enable CSI Replication Sidecars</li> <li>Volume Replication Custom Resources</li> <li>Enable mirroring on a PVC</li> <li>Creating a VolumeReplicationClass CR</li> <li>Creating a VolumeReplications CR</li> <li>Check VolumeReplication CR status</li> <li>Backup and Restore</li> </ul>","title":"Table of Contents"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#create-rbd-pools","text":"<p>In this section, we create specific RBD pools that are RBD mirroring  enabled for use with the DR use case.</p> <p>Execute the following steps on each peer cluster to create mirror  enabled pools:</p> <ul> <li>Create a RBD pool that is enabled for mirroring by adding the section <code>spec.mirroring</code> in the CephBlockPool CR:</li> </ul> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: mirroredpool\n  namespace: rook-ceph\nspec:\n  replicated:\n    size: 1\n  mirroring:\n    enabled: true\n    mode: image\n</code></pre>  <pre><code>kubectl create -f pool-mirrored.yaml\n</code></pre>  <ul> <li>Repeat the steps on the peer cluster.</li> </ul>  <p>NOTE: Pool name across the cluster peers must be the same for RBD replication to function.</p>  <p>See the CephBlockPool documentation for more details.</p>  <p>Note: It is also feasible to edit existing pools and enable them for replication.</p>","title":"Create RBD Pools"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#bootstrap-peers","text":"<p>In order for the rbd-mirror daemon to discover its peer cluster, the  peer must be registered and a user account must be created.</p> <p>The following steps enable bootstrapping peers to discover and  authenticate to each other:</p> <ul> <li>For Bootstrapping a peer cluster its bootstrap secret is required. To determine the name of the secret that contains the bootstrap secret execute the following command on the remote cluster (cluster-2)</li> </ul> <pre><code>[cluster-2]$ kubectl get cephblockpool.ceph.rook.io/mirroredpool -n rook-ceph -ojsonpath='{.status.info.rbdMirrorBootstrapPeerSecretName}'\n</code></pre>  <p>Here, <code>pool-peer-token-mirroredpool</code> is the desired bootstrap secret name.</p> <ul> <li>The secret pool-peer-token-mirroredpool contains all the information related to the token and needs to be injected to the peer, to fetch the decoded secret:</li> </ul> <pre><code>[cluster-2]$ kubectl get secret -n rook-ceph pool-peer-token-mirroredpool -o jsonpath='{.data.token}'|base64 -d\n</code></pre>   <p><code>bash eyJmc2lkIjoiNGQ1YmNiNDAtNDY3YS00OWVkLThjMGEtOWVhOGJkNDY2OTE3IiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFDZ3hmZGdxN013R0JBQWZzcUtCaGpZVjJUZDRxVzJYQm5kemc9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMzY6MzMwMCx2MToxOTIuMTY4LjM5LjM2OjY3ODldIn0=</code></p>  <ul> <li>With this Decoded value, create a secret on the primary site (cluster-1):</li> </ul> <pre><code>[cluster-1]$ kubectl -n rook-ceph create secret generic rbd-primary-site-secret --from-literal=token=eyJmc2lkIjoiNGQ1YmNiNDAtNDY3YS00OWVkLThjMGEtOWVhOGJkNDY2OTE3IiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFDZ3hmZGdxN013R0JBQWZzcUtCaGpZVjJUZDRxVzJYQm5kemc9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMzY6MzMwMCx2MToxOTIuMTY4LjM5LjM2OjY3ODldIn0= --from-literal=pool=mirroredpool\n</code></pre>  <ul> <li>This completes the bootstrap process for cluster-1 to be peered with cluster-2.</li> <li>Repeat the process switching cluster-2 in place of cluster-1, to complete the bootstrap process across both peer clusters.</li> </ul> <p>For more details, refer to the official rbd mirror documentation on how to create a bootstrap peer.</p>","title":"Bootstrap Peers"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#configure-the-rbdmirror-daemon","text":"<p>Replication is handled by the rbd-mirror daemon. The rbd-mirror daemon  is responsible for pulling image updates from the remote, peer cluster,  and applying them to image within the local cluster.</p> <p>Creation of the rbd-mirror daemon(s) is done through the custom resource  definitions (CRDs), as follows:</p> <ul> <li>Create mirror.yaml, to deploy the rbd-mirror daemon</li> </ul> <pre><code>apiVersion: ceph.rook.io/v1\nkind: CephRBDMirror\nmetadata:\n  name: my-rbd-mirror\n  namespace: openshift-storage\nspec:\n  # the number of rbd-mirror daemons to deploy\n  count: 1\n</code></pre>  <ul> <li>Create the RBD mirror daemon</li> </ul> <pre><code>[cluster-1]$ kubectl create -f mirror.yaml -n rook-ceph\n</code></pre>  <ul> <li>Validate if <code>rbd-mirror</code> daemon pod is now up</li> </ul> <pre><code>[cluster-1]$ kubectl get pods -n rook-ceph\n</code></pre>   <pre><code>rook-ceph-rbd-mirror-a-6985b47c8c-dpv4k  1/1  Running  0  10s\n</code></pre>   <ul> <li>Verify that daemon health is OK</li> </ul> <pre><code>kubectl get cephblockpools.ceph.rook.io mirroredpool -n rook-ceph -o jsonpath='{.status.mirroringStatus.summary}'\n</code></pre>   <pre><code>{\"daemon_health\":\"OK\",\"health\":\"OK\",\"image_health\":\"OK\",\"states\":{\"replaying\":1}}\n</code></pre>   <ul> <li>Repeat the above steps on the peer cluster.</li> </ul> <p>See the CephRBDMirror CRD for more details on the mirroring settings.</p>","title":"Configure the RBDMirror Daemon"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#add-mirroring-peer-information-to-rbd-pools","text":"<p>Each pool can have its own peer. To add the peer information, patch the already created mirroring enabled pool to update the CephBlockPool CRD.</p> <pre><code>[cluster-1]$ kubectl -n rook-ceph patch cephblockpool mirroredpool --type merge -p '{\"spec\":{\"mirroring\":{\"peers\": {\"secretNames\": [\"rbd-primary-site-secret\"]}}}}'\n</code></pre>","title":"Add mirroring peer information to RBD pools"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#create-volumereplication-crds","text":"<p>Volume Replication Operator follows controller pattern and provides extended APIs for storage disaster recovery. The extended APIs are provided via Custom Resource Definition(CRD). Create the VolumeReplication CRDs on all the peer clusters.</p> <pre><code>$ kubectl create -f https://raw.githubusercontent.com/csi-addons/volume-replication-operator/v0.3.0/config/crd/bases/replication.storage.openshift.io_volumereplications.yaml\n\n$ kubectl create -f https://raw.githubusercontent.com/csi-addons/volume-replication-operator/v0.3.0/config/crd/bases/replication.storage.openshift.io_volumereplicationclasses.yaml\n</code></pre>","title":"Create VolumeReplication CRDs"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#enable-csi-replication-sidecars","text":"<p>To achieve RBD Mirroring, <code>csi-omap-generator</code> and <code>volume-replication</code>  containers need to be deployed in the RBD provisioner pods, which are not enabled by default.</p> <ul> <li> <p>Omap Generator: Omap generator is a sidecar container that when  deployed with the CSI provisioner pod, generates the internal CSI  omaps between the PV and the RBD image. This is required as static PVs are  transferred across peer clusters in the DR use case, and hence  is needed to preserve PVC to storage mappings.</p> </li> <li> <p>Volume Replication Operator: Volume Replication Operator is a  kubernetes operator that provides common and reusable APIs for  storage disaster recovery.  It is based on csi-addons/spec  specification and can be used by any storage provider.  For more details, refer to volume replication operator.</p> </li> </ul> <p>Execute the following steps on each peer cluster to enable the  OMap generator and Volume Replication sidecars:</p> <ul> <li>Edit the <code>rook-ceph-operator-config</code> configmap and add the  following configurations</li> </ul> <pre><code>kubectl edit cm rook-ceph-operator-config -n rook-ceph\n</code></pre>  <p>Add the following properties if not present:</p> <pre><code>data:\n  CSI_ENABLE_OMAP_GENERATOR: \"true\"\n  CSI_ENABLE_VOLUME_REPLICATION: \"true\"\n</code></pre>  <ul> <li>After updating the configmap with those settings, two new sidecars  should now start automatically in the CSI provisioner pod.</li> <li>Repeat the steps on the peer cluster.</li> </ul>","title":"Enable CSI Replication Sidecars"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#volume-replication-custom-resources","text":"<p>VolumeReplication CRDs provide support for two custom resources:</p> <ul> <li> <p>VolumeReplicationClass: VolumeReplicationClass is a cluster scoped resource that contains driver related configuration parameters. It holds the storage admin information required for the volume replication operator.</p> </li> <li> <p>VolumeReplication: VolumeReplication is a namespaced resource that contains references to storage object to be replicated and VolumeReplicationClass corresponding to the driver providing replication.</p> </li> </ul>  <p>For more information, please refer to the volume-replication-operator.</p>","title":"Volume Replication Custom Resources"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#enable-mirroring-on-a-pvc","text":"<p>Below guide assumes that we have a PVC (rbd-pvc) in BOUND state; created using StorageClass with <code>Retain</code> reclaimPolicy.</p> <pre><code>[cluster-1]$ kubectl get pvc\n</code></pre>   <pre><code>NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nrbd-pvc   Bound    pvc-65dc0aac-5e15-4474-90f4-7a3532c621ec   1Gi        RWO            csi-rbd-sc   44s\n</code></pre>","title":"Enable mirroring on a PVC"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#create-a-volume-replication-class-cr","text":"<p>In this case, we create a Volume Replication Class on cluster-1</p> <pre><code>[cluster-1]$ kubectl apply -f deploy/examples/volume-replication-class.yaml\n</code></pre>   <p>Note: The <code>schedulingInterval</code> can be specified in formats of minutes, hours or days using suffix <code>m</code>,<code>h</code> and <code>d</code> respectively. The optional schedulingStartTime can be specified using the ISO 8601 time format.</p>","title":"Create a Volume Replication Class CR"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#create-a-volumereplication-cr","text":"<ul> <li>Once VolumeReplicationClass is created, create a Volume Replication for  the PVC which we intend to replicate to secondary cluster.</li> </ul> <pre><code>[cluster-1]$ kubectl apply -f deploy/examples/volume-replication.yaml\n</code></pre>   <p> VolumeReplication is a namespace scoped object. Thus, it should be created in the same namespace as of PVC.</p>","title":"Create a VolumeReplication CR"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#checking-replication-status","text":"<p><code>replicationState</code> is the state of the volume being referenced.  Possible values are primary, secondary, and resync.</p> <ul> <li><code>primary</code> denotes that the volume is primary.</li> <li><code>secondary</code> denotes that the volume is secondary.</li> <li><code>resync</code> denotes that the volume needs to be resynced.</li> </ul> <p>To check VolumeReplication CR status:</p> <pre><code>[cluster-1]$kubectl get volumereplication pvc-volumereplication -oyaml\n</code></pre>   <pre><code>...\nspec:\n  dataSource:\n    apiGroup: \"\"\n    kind: PersistentVolumeClaim\n    name: rbd-pvc\n  replicationState: primary\n  volumeReplicationClass: rbd-volumereplicationclass\nstatus:\n  conditions:\n  - lastTransitionTime: \"2021-05-04T07:39:00Z\"\n    message: \"\"\n    observedGeneration: 1\n    reason: Promoted\n    status: \"True\"\n    type: Completed\n  - lastTransitionTime: \"2021-05-04T07:39:00Z\"\n    message: \"\"\n    observedGeneration: 1\n    reason: Healthy\n    status: \"False\"\n    type: Degraded\n  - lastTransitionTime: \"2021-05-04T07:39:00Z\"\n    message: \"\"\n    observedGeneration: 1\n    reason: NotResyncing\n    status: \"False\"\n    type: Resyncing\n  lastCompletionTime: \"2021-05-04T07:39:00Z\"\n  lastStartTime: \"2021-05-04T07:38:59Z\"\n  message: volume is marked primary\n  observedGeneration: 1\n  state: Primary\n</code></pre>","title":"Checking Replication Status"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#backup-restore","text":"<p>NOTE: To effectively resume operations after a failover/relocation, backup of the kubernetes artifacts like deployment, PVC, PV, etc need to be created beforehand by the admin; so that the application can be restored on the peer cluster.</p>  <p>Here, we take a backup of PVC and PV object on one site, so that they can be restored later to the peer cluster.</p>","title":"Backup &amp; Restore"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#take-backup-on-cluster-1","text":"<ul> <li>Take backup of the PVC <code>rbd-pvc</code></li> </ul> <pre><code>[cluster-1]$ kubectl  get pvc rbd-pvc -oyaml &gt; pvc-backup.yaml\n</code></pre>  <ul> <li>Take a backup of the PV, corresponding to the PVC</li> </ul> <pre><code>[cluster-1]$ kubectl get pv/pvc-65dc0aac-5e15-4474-90f4-7a3532c621ec -oyaml &gt; pv_backup.yaml\n</code></pre>   <p>Note: We can also take backup using external tools like Velero. See velero documentation for more information.</p>","title":"Take backup on cluster-1"},{"location":"Usages-and-Examples/RBD-Block-Storage/rbd-mirroring/#restore-the-backup-on-cluster-2","text":"<ul> <li>Create storageclass on the secondary cluster</li> </ul> <pre><code>[cluster-2]$ kubectl create -f examples/rbd/storageclass.yaml\n</code></pre>  <ul> <li>Create VolumeReplicationClass on the secondary cluster</li> </ul> <pre><code>[cluster-1]$ kubectl apply -f deploy/examples/volume-replication-class.yaml\n ```\n\n&gt; ```bash\n&gt; volumereplicationclass.replication.storage.openshift.io/rbd-volumereplicationclass created\n&gt; ```\n\n* If Persistent Volumes and Claims are created manually on the secondary cluster,\n     remove the `claimRef` on the backed up PV objects in yaml files; so that the\n     PV can get bound to the new claim on the secondary cluster.\n\n```yaml\n...\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 1Gi\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: rbd-pvc\n    namespace: default\n    resourceVersion: \"64252\"\n    uid: 65dc0aac-5e15-4474-90f4-7a3532c621ec\n  csi:\n...\n</code></pre>  <ul> <li>Apply the Persistent Volume backup from the primary cluster</li> </ul> <pre><code>[cluster-2]$ kubectl create -f pv-backup.yaml\n</code></pre>  <ul> <li>Apply the Persistent Volume claim from the restored backup</li> </ul> <pre><code>[cluster-2]$ kubectl create -f pvc-backup.yaml\n</code></pre>  <pre><code>[cluster-2]$ kubectl get pvc\n</code></pre>   <pre><code>NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nrbd-pvc   Bound    pvc-65dc0aac-5e15-4474-90f4-7a3532c621ec   1Gi        RWO            rook-ceph-block   44s\n</code></pre>","title":"Restore the backup on cluster-2"}]}