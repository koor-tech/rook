<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Rook Ceph Documentation"><meta name=author content="Rook Authors"><link href=https://rook.io/1.8/ceph-pool-crd/ rel=canonical><link rel=icon href=https://rook.io/images/favicon_192x192.png><meta name=generator content="mkdocs-1.3.0, mkdocs-material-8.2.5+insiders-4.11.0"><title>Block Pool CRD - Rook Ceph Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.589a02ac.min.css><link rel=stylesheet href=../assets/stylesheets/palette.e6a45f82.min.css><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#ceph-block-pool-crd class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-component=outdated hidden> <aside class="md-banner md-banner--warning"> <div class="md-banner__inner md-grid md-typeset"> You're not viewing the latest version. <a href=../..> <strong>Click here to go to latest.</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=outdated]"),outdated=__md_get("__outdated",sessionStorage);!0===outdated&&el&&(el.hidden=!1)</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Rook Ceph Documentation" class="md-header__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Rook Ceph Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Block Pool CRD </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=rook-blue data-md-color-accent=red aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Rook Ceph Documentation" class="md-nav__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> Rook Ceph Documentation </label> <div class=md-nav__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Rook </span> </a> </li> <li class=md-nav__item> <a href=../async-disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Failover and Failback </span> </a> </li> <li class=md-nav__item> <a href=../authenticated-registry/ class=md-nav__link> <span class=md-ellipsis> Authenticated Registries </span> </a> </li> <li class=md-nav__item> <a href=../ceph-advanced-configuration/ class=md-nav__link> <span class=md-ellipsis> Advanced Configuration </span> </a> </li> <li class=md-nav__item> <a href=../ceph-block/ class=md-nav__link> <span class=md-ellipsis> Block Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-client-crd/ class=md-nav__link> <span class=md-ellipsis> Client CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-cluster-crd/ class=md-nav__link> <span class=md-ellipsis> Cluster CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-drivers/ class=md-nav__link> <span class=md-ellipsis> Ceph CSI </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-snapshot/ class=md-nav__link> <span class=md-ellipsis> Snapshots </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-troubleshooting/ class=md-nav__link> <span class=md-ellipsis> CSI Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-volume-clone/ class=md-nav__link> <span class=md-ellipsis> Volume clone </span> </a> </li> <li class=md-nav__item> <a href=../ceph-dashboard/ class=md-nav__link> <span class=md-ellipsis> Ceph Dashboard </span> </a> </li> <li class=md-nav__item> <a href=../ceph-disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Disaster Recovery </span> </a> </li> <li class=md-nav__item> <a href=../ceph-examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> <li class=md-nav__item> <a href=../ceph-filesystem-crd/ class=md-nav__link> <span class=md-ellipsis> Shared Filesystem CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-filesystem/ class=md-nav__link> <span class=md-ellipsis> Shared Filesystem </span> </a> </li> <li class=md-nav__item> <a href=../ceph-fs-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> Filesystem Mirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-fs-subvolumegroup/ class=md-nav__link> <span class=md-ellipsis> SubVolume Group CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-kms/ class=md-nav__link> <span class=md-ellipsis> Key Management System </span> </a> </li> <li class=md-nav__item> <a href=../ceph-mon-health/ class=md-nav__link> <span class=md-ellipsis> Monitor Health </span> </a> </li> <li class=md-nav__item> <a href=../ceph-monitoring/ class=md-nav__link> <span class=md-ellipsis> Prometheus Monitoring </span> </a> </li> <li class=md-nav__item> <a href=../ceph-nfs-crd/ class=md-nav__link> <span class=md-ellipsis> NFS CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-bucket-claim/ class=md-nav__link> <span class=md-ellipsis> Object Bucket Claim </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-bucket-notifications/ class=md-nav__link> <span class=md-ellipsis> Bucket Notifications </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-multisite-crd/ class=md-nav__link> <span class=md-ellipsis> Object Multisite CRDs </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-multisite/ class=md-nav__link> <span class=md-ellipsis> Object Multisite </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-store-crd/ class=md-nav__link> <span class=md-ellipsis> Object Store CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-store-user-crd/ class=md-nav__link> <span class=md-ellipsis> Object Store User CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object/ class=md-nav__link> <span class=md-ellipsis> Object Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-openshift-issues/ class=md-nav__link> <span class=md-ellipsis> OpenShift Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-openshift/ class=md-nav__link> <span class=md-ellipsis> OpenShift </span> </a> </li> <li class=md-nav__item> <a href=../ceph-osd-mgmt/ class=md-nav__link> <span class=md-ellipsis> OSD Management </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Block Pool CRD </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Block Pool CRD </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#samples class=md-nav__link> Samples </a> <nav class=md-nav aria-label=Samples> <ul class=md-nav__list> <li class=md-nav__item> <a href=#replicated class=md-nav__link> Replicated </a> <nav class=md-nav aria-label=Replicated> <ul class=md-nav__list> <li class=md-nav__item> <a href=#hybrid-storage-pools class=md-nav__link> Hybrid Storage Pools </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#erasure-coded class=md-nav__link> Erasure Coded </a> </li> <li class=md-nav__item> <a href=#mirroring class=md-nav__link> Mirroring </a> </li> <li class=md-nav__item> <a href=#data-spread-across-subdomains class=md-nav__link> Data spread across subdomains </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#pool-settings class=md-nav__link> Pool Settings </a> <nav class=md-nav aria-label="Pool Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#metadata class=md-nav__link> Metadata </a> </li> <li class=md-nav__item> <a href=#spec class=md-nav__link> Spec </a> </li> <li class=md-nav__item> <a href=#add-specific-pool-properties class=md-nav__link> Add specific pool properties </a> </li> <li class=md-nav__item> <a href=#erasure-coding class=md-nav__link> Erasure Coding </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../ceph-pool-radosnamespace/ class=md-nav__link> <span class=md-ellipsis> RADOS Namespace CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-rbd-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> RBD Mirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-storage/ class=md-nav__link> <span class=md-ellipsis> Ceph Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-teardown/ class=md-nav__link> <span class=md-ellipsis> Cleanup </span> </a> </li> <li class=md-nav__item> <a href=../ceph-toolbox/ class=md-nav__link> <span class=md-ellipsis> Toolbox </span> </a> </li> <li class=md-nav__item> <a href=../ceph-tools/ class=md-nav__link> <span class=md-ellipsis> Ceph Tools </span> </a> </li> <li class=md-nav__item> <a href=../ceph-upgrade/ class=md-nav__link> <span class=md-ellipsis> Upgrades </span> </a> </li> <li class=md-nav__item> <a href=../common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../development-environment/ class=md-nav__link> <span class=md-ellipsis> Developer Environment </span> </a> </li> <li class=md-nav__item> <a href=../development-flow/ class=md-nav__link> <span class=md-ellipsis> Contributing </span> </a> </li> <li class=md-nav__item> <a href=../direct-tools/ class=md-nav__link> <span class=md-ellipsis> Direct Tools </span> </a> </li> <li class=md-nav__item> <a href=../helm-ceph-cluster/ class=md-nav__link> <span class=md-ellipsis> Ceph Cluster </span> </a> </li> <li class=md-nav__item> <a href=../helm-operator/ class=md-nav__link> <span class=md-ellipsis> Ceph Operator </span> </a> </li> <li class=md-nav__item> <a href=../helm/ class=md-nav__link> <span class=md-ellipsis> Helm Charts </span> </a> </li> <li class=md-nav__item> <a href=../pod-security-policies/ class=md-nav__link> <span class=md-ellipsis> Pod Security Policies </span> </a> </li> <li class=md-nav__item> <a href=../pre-reqs/ class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=../quickstart/ class=md-nav__link> <span class=md-ellipsis> Quickstart </span> </a> </li> <li class=md-nav__item> <a href=../rbd-mirroring/ class=md-nav__link> <span class=md-ellipsis> RBD Mirroring </span> </a> </li> <li class=md-nav__item> <a href=../storage-providers/ class=md-nav__link> <span class=md-ellipsis> Storage Providers </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#samples class=md-nav__link> Samples </a> <nav class=md-nav aria-label=Samples> <ul class=md-nav__list> <li class=md-nav__item> <a href=#replicated class=md-nav__link> Replicated </a> <nav class=md-nav aria-label=Replicated> <ul class=md-nav__list> <li class=md-nav__item> <a href=#hybrid-storage-pools class=md-nav__link> Hybrid Storage Pools </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#erasure-coded class=md-nav__link> Erasure Coded </a> </li> <li class=md-nav__item> <a href=#mirroring class=md-nav__link> Mirroring </a> </li> <li class=md-nav__item> <a href=#data-spread-across-subdomains class=md-nav__link> Data spread across subdomains </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#pool-settings class=md-nav__link> Pool Settings </a> <nav class=md-nav aria-label="Pool Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#metadata class=md-nav__link> Metadata </a> </li> <li class=md-nav__item> <a href=#spec class=md-nav__link> Spec </a> </li> <li class=md-nav__item> <a href=#add-specific-pool-properties class=md-nav__link> Add specific pool properties </a> </li> <li class=md-nav__item> <a href=#erasure-coding class=md-nav__link> Erasure Coding </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/rook/rook/edit/master/Documentation/ceph-pool-crd.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <p>{% include_relative branch.liquid %}</p> <h1 id=ceph-block-pool-crd>Ceph Block Pool CRD<a class=headerlink href=#ceph-block-pool-crd title="Permanent link">&para;</a></h1> <p>Rook allows creation and customization of storage pools through the custom resource definitions (CRDs). The following settings are available for pools.</p> <h2 id=samples>Samples<a class=headerlink href=#samples title="Permanent link">&para;</a></h2> <h3 id=replicated>Replicated<a class=headerlink href=#replicated title="Permanent link">&para;</a></h3> <p>For optimal performance, while also adding redundancy, this sample will configure Ceph to make three full copies of the data on multiple nodes.</p> <blockquote> <p><strong>NOTE</strong>: This sample requires <em>at least 1 OSD per node</em>, with each OSD located on <em>3 different nodes</em>.</p> </blockquote> <p>Each OSD must be located on a different node, because the <a href=./#spec><code>failureDomain</code></a> is set to <code>host</code> and the <code>replicated.size</code> is set to <code>3</code>.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ceph.rook.io/v1</span><span class=w></span>
<span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">CephBlockPool</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">replicapool</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>failureDomain</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">host</span><span class=w></span>
<span class=w>  </span><span class=nt>replicated</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class=w></span>
<span class=w>  </span><span class=nt>deviceClass</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">hdd</span><span class=w></span>
</code></pre></div> </td></tr></table> <h4 id=hybrid-storage-pools>Hybrid Storage Pools<a class=headerlink href=#hybrid-storage-pools title="Permanent link">&para;</a></h4> <p>Hybrid storage is a combination of two different storage tiers. For example, SSD and HDD.<br> This helps to improve the read performance of cluster by placing, say, 1st copy of data on the higher performance tier (SSD or NVME) and remaining replicated copies on lower cost tier (HDDs).</p> <p><strong>WARNING</strong> Hybrid storage pools are likely to suffer from lower availability if a node goes down. The data across the two<br> tiers may actually end up on the same node, instead of being spread across unique nodes (or failure domains) as expected.<br> Instead of using hybrid pools, consider configuring <a href=https://docs.ceph.com/en/latest/rados/operations/crush-map/#primary-affinity>primary affinity</a> from the toolbox.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ceph.rook.io/v1</span><span class=w></span>
<span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">CephBlockPool</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">replicapool</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>failureDomain</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">host</span><span class=w></span>
<span class=w>  </span><span class=nt>replicated</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class=w></span>
<span class=w>    </span><span class=nt>hybridStorage</span><span class=p>:</span><span class=w></span>
<span class=w>      </span><span class=nt>primaryDeviceClass</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ssd</span><span class=w></span>
<span class=w>      </span><span class=nt>secondaryDeviceClass</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">hdd</span><span class=w></span>
</code></pre></div> </td></tr></table> <blockquote> <p><strong>IMPORTANT</strong>: The device classes <code>primaryDeviceClass</code> and <code>secondaryDeviceClass</code> must have at least one OSD associated with them or else the pool creation will fail.</p> </blockquote> <h3 id=erasure-coded>Erasure Coded<a class=headerlink href=#erasure-coded title="Permanent link">&para;</a></h3> <p>This sample will lower the overall storage capacity requirement, while also adding redundancy by using <a href=#erasure-coding>erasure coding</a>.</p> <blockquote> <p><strong>NOTE</strong>: This sample requires <em>at least 3 bluestore OSDs</em>.</p> </blockquote> <p>The OSDs can be located on a single Ceph node or spread across multiple nodes, because the <a href=./#spec><code>failureDomain</code></a> is set to <code>osd</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ceph.rook.io/v1</span><span class=w></span>
<span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">CephBlockPool</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ecpool</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>failureDomain</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">osd</span><span class=w></span>
<span class=w>  </span><span class=nt>erasureCoded</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>dataChunks</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class=w></span>
<span class=w>    </span><span class=nt>codingChunks</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class=w></span>
<span class=w>  </span><span class=nt>deviceClass</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">hdd</span><span class=w></span>
</code></pre></div> </td></tr></table> <p>High performance applications typically will not use erasure coding due to the performance overhead of creating and distributing the chunks in the cluster.</p> <p>When creating an erasure-coded pool, it is highly recommended to create the pool when you have <strong>bluestore OSDs</strong> in your cluster<br> (see the <a href=../ceph-cluster-crd/#osd-configuration-settings>OSD configuration settings</a>. Filestore OSDs have<br> <a href=http://docs.ceph.com/docs/master/rados/operations/erasure-code/#erasure-coding-with-overwrites>limitations</a> that are unsafe and lower performance.</p> <h3 id=mirroring>Mirroring<a class=headerlink href=#mirroring title="Permanent link">&para;</a></h3> <p>RADOS Block Device (RBD) mirroring is a process of asynchronous replication of Ceph block device images between two or more Ceph clusters.<br> Mirroring ensures point-in-time consistent replicas of all changes to an image, including reads and writes, block device resizing, snapshots, clones and flattening.<br> It is generally useful when planning for Disaster Recovery.<br> Mirroring is for clusters that are geographically distributed and stretching a single cluster is not possible due to high latencies.</p> <p>The following will enable mirroring of the pool at the image level:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ceph.rook.io/v1</span><span class=w></span>
<span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">CephBlockPool</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">replicapool</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>replicated</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class=w></span>
<span class=w>  </span><span class=nt>mirroring</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w></span>
<span class=w>    </span><span class=nt>mode</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">image</span><span class=w></span>
<span class=w>    </span><span class=c1># schedule(s) of snapshot</span><span class=w></span>
<span class=w>    </span><span class=nt>snapshotSchedules</span><span class=p>:</span><span class=w></span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>interval</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">24h</span><span class=w> </span><span class=c1># daily snapshots</span><span class=w></span>
<span class=w>        </span><span class=nt>startTime</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">14:00:00-05:00</span><span class=w></span>
</code></pre></div> </td></tr></table> <p>Once mirroring is enabled, Rook will by default create its own <a href=https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#bootstrap-peers>bootstrap peer token</a> so that it can be used by another cluster.<br> The bootstrap peer token can be found in a Kubernetes Secret. The name of the Secret is present in the Status field of the CephBlockPool CR:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>status</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>info</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>rbdMirrorBootstrapPeerSecretName</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">pool-peer-token-replicapool</span><span class=w></span>
</code></pre></div> </td></tr></table> <p>This secret can then be fetched like so:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl get secret -n rook-ceph pool-peer-token-replicapool -o jsonpath=&#39;{.data.token}&#39;|base64 -d</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>eyJmc2lkIjoiOTFlYWUwZGQtMDZiMS00ZDJjLTkxZjMtMTMxMWM5ZGYzODJiIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFEN1psOWZ3V1VGRHhBQWdmY0gyZi8xeUhYeGZDUTU5L1N0NEE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjEwMS4xOC4yMjM6MzMwMCx2MToxMC4xMDEuMTguMjIzOjY3ODldIn0=
</code></pre></div> </td></tr></table> </blockquote> <p>The secret must be decoded. The result will be another base64 encoded blob that you will import in the destination cluster:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>external-cluster-console # rbd mirror pool peer bootstrap import &lt;token file path&gt;</span>
</code></pre></div> </td></tr></table> <p>See the official rbd mirror documentation on <a href=https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#bootstrap-peers>how to add a bootstrap peer</a>.</p> <h3 id=data-spread-across-subdomains>Data spread across subdomains<a class=headerlink href=#data-spread-across-subdomains title="Permanent link">&para;</a></h3> <p>Imagine the following topology with datacenters containing racks and then hosts:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>.
├── datacenter-1
│   ├── rack-1
│   │   ├── host-1
│   │   ├── host-2
│   └── rack-2
│       ├── host-3
│       ├── host-4
└── datacenter-2
    ├── rack-3
    │   ├── host-5
    │   ├── host-6
    └── rack-4
        ├── host-7
        └── host-8
</code></pre></div> </td></tr></table> <p>As an administrator I would like to place 4 copies across both datacenter where each copy inside a datacenter is across a rack.<br> This can be achieved by the following:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ceph.rook.io/v1</span><span class=w></span>
<span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">CephBlockPool</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">replicapool</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>replicated</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class=w></span>
<span class=w>    </span><span class=nt>replicasPerFailureDomain</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class=w></span>
<span class=w>    </span><span class=nt>subFailureDomain</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rack</span><span class=w></span>
</code></pre></div> </td></tr></table> <h2 id=pool-settings>Pool Settings<a class=headerlink href=#pool-settings title="Permanent link">&para;</a></h2> <h3 id=metadata>Metadata<a class=headerlink href=#metadata title="Permanent link">&para;</a></h3> <ul> <li><code>name</code>: The name of the pool to create.</li> <li><code>namespace</code>: The namespace of the Rook cluster where the pool is created.</li> </ul> <h3 id=spec>Spec<a class=headerlink href=#spec title="Permanent link">&para;</a></h3> <ul> <li><code>replicated</code>: Settings for a replicated pool. If specified, <code>erasureCoded</code> settings must not be specified.</li> <li><code>size</code>: The desired number of copies to make of the data in the pool.</li> <li><code>requireSafeReplicaSize</code>: set to false if you want to create a pool with size 1, setting pool size 1 could lead to data loss without recovery. Make sure you are <em>ABSOLUTELY CERTAIN</em> that is what you want.</li> <li><code>replicasPerFailureDomain</code>: Sets up the number of replicas to place in a given failure domain. For instance, if the failure domain is a datacenter (cluster is<br> stretched) then you will have 2 replicas per datacenter where each replica ends up on a different host. This gives you a total of 4 replicas and for this, the <code>size</code> must be set to 4. The default is 1.</li> <li><code>subFailureDomain</code>: Name of the CRUSH bucket representing a sub-failure domain. In a stretched configuration this option represent the "last" bucket where replicas will end up being written. Imagine the cluster is stretched across two datacenters, you can then have 2 copies per datacenter and each copy on a different CRUSH bucket. The default is "host".</li> <li><code>erasureCoded</code>: Settings for an erasure-coded pool. If specified, <code>replicated</code> settings must not be specified. See below for more details on <a href=#erasure-coding>erasure coding</a>.</li> <li><code>dataChunks</code>: Number of chunks to divide the original object into</li> <li><code>codingChunks</code>: Number of coding chunks to generate</li> <li> <p><code>failureDomain</code>: The failure domain across which the data will be spread. This can be set to a value of either <code>osd</code> or <code>host</code>, with <code>host</code> being the default setting. A failure domain can also be set to a different type (e.g. <code>rack</code>), if the OSDs are created on nodes with the supported <a href=../ceph-cluster-crd/#osd-topology>topology labels</a>. If the <code>failureDomain</code> is changed on the pool, the operator will create a new CRUSH rule and update the pool.<br> If a <code>replicated</code> pool of size <code>3</code> is configured and the <code>failureDomain</code> is set to <code>host</code>, all three copies of the replicated data will be placed on OSDs located on <code>3</code> different Ceph hosts. This case is guaranteed to tolerate a failure of two hosts without a loss of data. Similarly, a failure domain set to <code>osd</code>, can tolerate a loss of two OSD devices.</p> <p>If erasure coding is used, the data and coding chunks are spread across the configured failure domain.</p> <blockquote> <p><strong>NOTE</strong>: Neither Rook, nor Ceph, prevent the creation of a cluster where the replicated data (or Erasure Coded chunks) can be written safely. By design, Ceph will delay checking for suitable OSDs until a write request is made and this write can hang if there are not sufficient OSDs to satisfy the request. * <code>deviceClass</code>: Sets up the CRUSH rule for the pool to distribute data only on the specified device class. If left empty or unspecified, the pool will use the cluster's default CRUSH root, which usually distributes data over all OSDs, regardless of their class. * <code>crushRoot</code>: The root in the crush map to be used by the pool. If left empty or unspecified, the default root will be used. Creating a crush hierarchy for the OSDs currently requires the Rook toolbox to run the Ceph tools described <a href=http://docs.ceph.com/docs/master/rados/operations/crush-map/#modifying-the-crush-map>here</a>. * <code>enableRBDStats</code>: Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false. For more info see the <a href=https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics>ceph documentation</a>. * <code>name</code>: The name of Ceph pools is based on the <code>metadata.name</code> of the CephBlockPool CR. Some built-in Ceph pools<br> require names that are incompatible with K8s resource names. These special pools can be configured<br> by setting this <code>name</code> to override the name of the Ceph pool that is created instead of using the <code>metadata.name</code> for the pool.<br> Only the following pool names are supported: <code>device_health_metrics</code>, <code>.nfs</code>, and <code>.mgr</code>. See the example<br> <a href="https://github.com/rook/rook/blob/{{ branchName }}/deploy/examples/pool-builtin-mgr.yaml">builtin mgr pool</a>.</p> </blockquote> </li> <li> <p><code>parameters</code>: Sets any <a href=https://docs.ceph.com/docs/master/rados/operations/pools/#set-pool-values>parameters</a> listed to the given pool</p> </li> <li><code>target_size_ratio:</code> gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool, for more info see the <a href=https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size>ceph documentation</a></li> <li> <p><code>compression_mode</code>: Sets up the pool for inline compression when using a Bluestore OSD. If left unspecified does not setup any compression mode for the pool. Values supported are the same as Bluestore inline compression <a href=https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression>modes</a>, such as <code>none</code>, <code>passive</code>, <code>aggressive</code>, and <code>force</code>.</p> </li> <li> <p><code>mirroring</code>: Sets up mirroring of the pool</p> </li> <li><code>enabled</code>: whether mirroring is enabled on that pool (default: false)</li> <li><code>mode</code>: mirroring mode to run, possible values are "pool" or "image" (required). Refer to the <a href=https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#enable-mirroring>mirroring modes Ceph documentation</a> for more details.</li> <li><code>snapshotSchedules</code>: schedule(s) snapshot at the <strong>pool</strong> level. <strong>Only</strong> supported as of Ceph Octopus (v15) release. One or more schedules are supported.<ul> <li><code>interval</code>: frequency of the snapshots. The interval can be specified in days, hours, or minutes using d, h, m suffix respectively.</li> <li><code>startTime</code>: optional, determines at what time the snapshot process starts, specified using the ISO 8601 time format.</li> </ul> </li> <li> <p><code>peers</code>: to configure mirroring peers. See the prerequisite <a href=../ceph-rbd-mirror-crd/ >RBD Mirror documentation</a> first.</p> <ul> <li><code>secretNames</code>: a list of peers to connect to. Currently <strong>only a single</strong> peer is supported where a peer represents a Ceph cluster.</li> </ul> </li> <li> <p><code>statusCheck</code>: Sets up pool mirroring status</p> </li> <li> <p><code>mirror</code>: displays the mirroring status</p> <ul> <li><code>disabled</code>: whether to enable or disable pool mirroring status</li> <li><code>interval</code>: time interval to refresh the mirroring status (default 60s)</li> </ul> </li> <li> <p><code>quotas</code>: Set byte and object quotas. See the <a href=https://docs.ceph.com/en/latest/rados/operations/pools/#set-pool-quotas>ceph documentation</a> for more info.</p> </li> <li><code>maxSize</code>: quota in bytes as a string with quantity suffixes (e.g. "10Gi")</li> <li><code>maxObjects</code>: quota in objects as an integer<br> &gt; <strong>NOTE</strong>: A value of 0 disables the quota.</li> </ul> <h3 id=add-specific-pool-properties>Add specific pool properties<a class=headerlink href=#add-specific-pool-properties title="Permanent link">&para;</a></h3> <p>With <code>poolProperties</code> you can set any pool property:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>parameters</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>&lt;name of the parameter&gt;</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">&lt;parameter value&gt;</span><span class=w></span>
</code></pre></div> </td></tr></table> <p>For instance:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>spec</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>parameters</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>min_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class=w></span>
</code></pre></div> </td></tr></table> <h3 id=erasure-coding>Erasure Coding<a class=headerlink href=#erasure-coding title="Permanent link">&para;</a></h3> <p><a href=http://docs.ceph.com/docs/master/rados/operations/erasure-code/ >Erasure coding</a> allows you to keep your data safe while reducing the storage overhead. Instead of creating multiple replicas of the data,<br> erasure coding divides the original data into chunks of equal size, then generates extra chunks of that same size for redundancy.</p> <p>For example, if you have an object of size 2MB, the simplest erasure coding with two data chunks would divide the object into two chunks of size 1MB each (data chunks). One more chunk (coding chunk) of size 1MB will be generated. In total, 3MB will be stored in the cluster. The object will be able to suffer the loss of any one of the chunks and still be able to reconstruct the original object.</p> <p>The number of data and coding chunks you choose will depend on your resiliency to loss and how much storage overhead is acceptable in your storage cluster.<br> Here are some examples to illustrate how the number of chunks affects the storage and loss toleration.</p> <table> <thead> <tr> <th>Data chunks (k)</th> <th>Coding chunks (m)</th> <th>Total storage</th> <th>Losses Tolerated</th> <th>OSDs required</th> </tr> </thead> <tbody> <tr> <td>2</td> <td>1</td> <td>1.5x</td> <td>1</td> <td>3</td> </tr> <tr> <td>2</td> <td>2</td> <td>2x</td> <td>2</td> <td>4</td> </tr> <tr> <td>4</td> <td>2</td> <td>1.5x</td> <td>2</td> <td>6</td> </tr> <tr> <td>16</td> <td>4</td> <td>1.25x</td> <td>4</td> <td>20</td> </tr> </tbody> </table> <p>The <code>failureDomain</code> must be also be taken into account when determining the number of chunks. The failure domain determines the level in the Ceph CRUSH hierarchy where the chunks must be uniquely distributed. This decision will impact whether node losses or disk losses are tolerated. There could also be performance differences of placing the data across nodes or osds.</p> <ul> <li><code>host</code>: All chunks will be placed on unique hosts</li> <li><code>osd</code>: All chunks will be placed on unique OSDs</li> </ul> <p>If you do not have a sufficient number of hosts or OSDs for unique placement the pool can be created, writing to the pool will hang.</p> <p>Rook currently only configures two levels in the CRUSH map. It is also possible to configure other levels such as <code>rack</code> with by adding <a href=../ceph-cluster-crd/#osd-topology>topology labels</a> to the nodes.</p> </article> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> </div> </div> <a href=# class="md-top md-top--hidden md-icon" data-md-component=top> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../ceph-osd-mgmt/ class="md-footer__link md-footer__link--prev" aria-label="Previous: OSD Management" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> OSD Management </div> </div> </a> <a href=../ceph-pool-radosnamespace/ class="md-footer__link md-footer__link--next" aria-label="Next: RADOS Namespace CRD" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> RADOS Namespace CRD </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> <a href=/ class=logo> <img src=/images/rook-logo-small.svg alt=rook.io> </a> <p> &#169; Rook Authors {{ site.time | date: '%Y' }}. Documentation distributed under <a href=https://creativecommons.org/licenses/by/4.0>CC-BY-4.0</a>. </p> <p> &#169; {{ site.time | date: '%Y' }} The Linux Foundation. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage/ >Trademark Usage</a> page. </p> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs Insiders </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.tabs.link", "instant", "navigation.expand", "navigation.top", "navigation.tracking", "privacy", "search.highlight", "search.share", "search.suggest", "tabs"], "search": "../assets/javascripts/workers/search.5c9dbbf3.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../assets/javascripts/bundle.10bf1588.min.js></script> </body> </html>