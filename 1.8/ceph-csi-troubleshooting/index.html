<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Rook Ceph Documentation"><meta name=author content="Rook Authors"><link href=https://rook.io/1.8/ceph-csi-troubleshooting/ rel=canonical><link rel=icon href=https://rook.io/images/favicon_192x192.png><meta name=generator content="mkdocs-1.3.0, mkdocs-material-8.2.5+insiders-4.11.0"><title>CSI Common Issues - Rook Ceph Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.589a02ac.min.css><link rel=stylesheet href=../assets/stylesheets/palette.e6a45f82.min.css><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#csi-common-issues class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-component=outdated hidden> <aside class="md-banner md-banner--warning"> <div class="md-banner__inner md-grid md-typeset"> You're not viewing the latest version. <a href=../..> <strong>Click here to go to latest.</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=outdated]"),outdated=__md_get("__outdated",sessionStorage);!0===outdated&&el&&(el.hidden=!1)</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Rook Ceph Documentation" class="md-header__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Rook Ceph Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> CSI Common Issues </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=rook-blue data-md-color-accent=red aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Rook Ceph Documentation" class="md-nav__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> Rook Ceph Documentation </label> <div class=md-nav__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Rook </span> </a> </li> <li class=md-nav__item> <a href=../async-disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Failover and Failback </span> </a> </li> <li class=md-nav__item> <a href=../authenticated-registry/ class=md-nav__link> <span class=md-ellipsis> Authenticated Registries </span> </a> </li> <li class=md-nav__item> <a href=../ceph-advanced-configuration/ class=md-nav__link> <span class=md-ellipsis> Advanced Configuration </span> </a> </li> <li class=md-nav__item> <a href=../ceph-block/ class=md-nav__link> <span class=md-ellipsis> Block Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-client-crd/ class=md-nav__link> <span class=md-ellipsis> Client CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-cluster-crd/ class=md-nav__link> <span class=md-ellipsis> Cluster CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-drivers/ class=md-nav__link> <span class=md-ellipsis> Ceph CSI </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-snapshot/ class=md-nav__link> <span class=md-ellipsis> Snapshots </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> CSI Common Issues </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> CSI Common Issues </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#block-rbd class=md-nav__link> Block (RBD) </a> </li> <li class=md-nav__item> <a href=#shared-filesystem-cephfs class=md-nav__link> Shared Filesystem (CephFS) </a> </li> <li class=md-nav__item> <a href=#network-connectivity class=md-nav__link> Network Connectivity </a> </li> <li class=md-nav__item> <a href=#ceph-health class=md-nav__link> Ceph Health </a> </li> <li class=md-nav__item> <a href=#slow-operations class=md-nav__link> Slow Operations </a> </li> <li class=md-nav__item> <a href=#ceph-troubleshooting class=md-nav__link> Ceph Troubleshooting </a> <nav class=md-nav aria-label="Ceph Troubleshooting"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#check-if-the-rbd-pool-exists class=md-nav__link> Check if the RBD Pool exists </a> </li> <li class=md-nav__item> <a href=#check-if-the-filesystem-exists class=md-nav__link> Check if the Filesystem exists </a> </li> <li class=md-nav__item> <a href=#subvolumegroups class=md-nav__link> subvolumegroups </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#provisioning-volumes class=md-nav__link> Provisioning Volumes </a> <nav class=md-nav aria-label="Provisioning Volumes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#csi-provisioner class=md-nav__link> csi-provisioner </a> </li> <li class=md-nav__item> <a href=#csi-resizer class=md-nav__link> csi-resizer </a> </li> <li class=md-nav__item> <a href=#csi-snapshotter class=md-nav__link> csi-snapshotter </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#mounting-the-volume-to-application-pods class=md-nav__link> Mounting the volume to application pods </a> <nav class=md-nav aria-label="Mounting the volume to application pods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#csi-driver-registration class=md-nav__link> csi-driver registration </a> </li> <li class=md-nav__item> <a href=#driver-registrar class=md-nav__link> driver-registrar </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#volume-attachment class=md-nav__link> Volume Attachment </a> <nav class=md-nav aria-label="Volume Attachment"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#csi-attacher class=md-nav__link> csi-attacher </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cephfs-stale-operations class=md-nav__link> CephFS Stale operations </a> </li> <li class=md-nav__item> <a href=#rbd-stale-operations class=md-nav__link> RBD Stale operations </a> </li> <li class=md-nav__item> <a href=#dmesg-logs class=md-nav__link> dmesg logs </a> </li> <li class=md-nav__item> <a href=#rbd-commands class=md-nav__link> RBD Commands </a> </li> <li class=md-nav__item> <a href=#node-loss class=md-nav__link> Node Loss </a> <nav class=md-nav aria-label="Node Loss"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#force-deleting-the-pod class=md-nav__link> Force deleting the pod </a> </li> <li class=md-nav__item> <a href=#blocklisting-a-node class=md-nav__link> Blocklisting a node </a> </li> <li class=md-nav__item> <a href=#removing-a-node-blocklist class=md-nav__link> Removing a node blocklist </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../ceph-csi-volume-clone/ class=md-nav__link> <span class=md-ellipsis> Volume clone </span> </a> </li> <li class=md-nav__item> <a href=../ceph-dashboard/ class=md-nav__link> <span class=md-ellipsis> Ceph Dashboard </span> </a> </li> <li class=md-nav__item> <a href=../ceph-disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Disaster Recovery </span> </a> </li> <li class=md-nav__item> <a href=../ceph-examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> <li class=md-nav__item> <a href=../ceph-filesystem-crd/ class=md-nav__link> <span class=md-ellipsis> Shared Filesystem CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-filesystem/ class=md-nav__link> <span class=md-ellipsis> Shared Filesystem </span> </a> </li> <li class=md-nav__item> <a href=../ceph-fs-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> Filesystem Mirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-fs-subvolumegroup/ class=md-nav__link> <span class=md-ellipsis> SubVolume Group CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-kms/ class=md-nav__link> <span class=md-ellipsis> Key Management System </span> </a> </li> <li class=md-nav__item> <a href=../ceph-mon-health/ class=md-nav__link> <span class=md-ellipsis> Monitor Health </span> </a> </li> <li class=md-nav__item> <a href=../ceph-monitoring/ class=md-nav__link> <span class=md-ellipsis> Prometheus Monitoring </span> </a> </li> <li class=md-nav__item> <a href=../ceph-nfs-crd/ class=md-nav__link> <span class=md-ellipsis> NFS CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-bucket-claim/ class=md-nav__link> <span class=md-ellipsis> Object Bucket Claim </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-bucket-notifications/ class=md-nav__link> <span class=md-ellipsis> Bucket Notifications </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-multisite-crd/ class=md-nav__link> <span class=md-ellipsis> Object Multisite CRDs </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-multisite/ class=md-nav__link> <span class=md-ellipsis> Object Multisite </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-store-crd/ class=md-nav__link> <span class=md-ellipsis> Object Store CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-store-user-crd/ class=md-nav__link> <span class=md-ellipsis> Object Store User CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object/ class=md-nav__link> <span class=md-ellipsis> Object Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-openshift-issues/ class=md-nav__link> <span class=md-ellipsis> OpenShift Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-openshift/ class=md-nav__link> <span class=md-ellipsis> OpenShift </span> </a> </li> <li class=md-nav__item> <a href=../ceph-osd-mgmt/ class=md-nav__link> <span class=md-ellipsis> OSD Management </span> </a> </li> <li class=md-nav__item> <a href=../ceph-pool-crd/ class=md-nav__link> <span class=md-ellipsis> Block Pool CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-pool-radosnamespace/ class=md-nav__link> <span class=md-ellipsis> RADOS Namespace CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-rbd-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> RBD Mirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-storage/ class=md-nav__link> <span class=md-ellipsis> Ceph Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-teardown/ class=md-nav__link> <span class=md-ellipsis> Cleanup </span> </a> </li> <li class=md-nav__item> <a href=../ceph-toolbox/ class=md-nav__link> <span class=md-ellipsis> Toolbox </span> </a> </li> <li class=md-nav__item> <a href=../ceph-tools/ class=md-nav__link> <span class=md-ellipsis> Ceph Tools </span> </a> </li> <li class=md-nav__item> <a href=../ceph-upgrade/ class=md-nav__link> <span class=md-ellipsis> Upgrades </span> </a> </li> <li class=md-nav__item> <a href=../common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../development-environment/ class=md-nav__link> <span class=md-ellipsis> Developer Environment </span> </a> </li> <li class=md-nav__item> <a href=../development-flow/ class=md-nav__link> <span class=md-ellipsis> Contributing </span> </a> </li> <li class=md-nav__item> <a href=../direct-tools/ class=md-nav__link> <span class=md-ellipsis> Direct Tools </span> </a> </li> <li class=md-nav__item> <a href=../helm-ceph-cluster/ class=md-nav__link> <span class=md-ellipsis> Ceph Cluster </span> </a> </li> <li class=md-nav__item> <a href=../helm-operator/ class=md-nav__link> <span class=md-ellipsis> Ceph Operator </span> </a> </li> <li class=md-nav__item> <a href=../helm/ class=md-nav__link> <span class=md-ellipsis> Helm Charts </span> </a> </li> <li class=md-nav__item> <a href=../pod-security-policies/ class=md-nav__link> <span class=md-ellipsis> Pod Security Policies </span> </a> </li> <li class=md-nav__item> <a href=../pre-reqs/ class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=../quickstart/ class=md-nav__link> <span class=md-ellipsis> Quickstart </span> </a> </li> <li class=md-nav__item> <a href=../rbd-mirroring/ class=md-nav__link> <span class=md-ellipsis> RBD Mirroring </span> </a> </li> <li class=md-nav__item> <a href=../storage-providers/ class=md-nav__link> <span class=md-ellipsis> Storage Providers </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#block-rbd class=md-nav__link> Block (RBD) </a> </li> <li class=md-nav__item> <a href=#shared-filesystem-cephfs class=md-nav__link> Shared Filesystem (CephFS) </a> </li> <li class=md-nav__item> <a href=#network-connectivity class=md-nav__link> Network Connectivity </a> </li> <li class=md-nav__item> <a href=#ceph-health class=md-nav__link> Ceph Health </a> </li> <li class=md-nav__item> <a href=#slow-operations class=md-nav__link> Slow Operations </a> </li> <li class=md-nav__item> <a href=#ceph-troubleshooting class=md-nav__link> Ceph Troubleshooting </a> <nav class=md-nav aria-label="Ceph Troubleshooting"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#check-if-the-rbd-pool-exists class=md-nav__link> Check if the RBD Pool exists </a> </li> <li class=md-nav__item> <a href=#check-if-the-filesystem-exists class=md-nav__link> Check if the Filesystem exists </a> </li> <li class=md-nav__item> <a href=#subvolumegroups class=md-nav__link> subvolumegroups </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#provisioning-volumes class=md-nav__link> Provisioning Volumes </a> <nav class=md-nav aria-label="Provisioning Volumes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#csi-provisioner class=md-nav__link> csi-provisioner </a> </li> <li class=md-nav__item> <a href=#csi-resizer class=md-nav__link> csi-resizer </a> </li> <li class=md-nav__item> <a href=#csi-snapshotter class=md-nav__link> csi-snapshotter </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#mounting-the-volume-to-application-pods class=md-nav__link> Mounting the volume to application pods </a> <nav class=md-nav aria-label="Mounting the volume to application pods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#csi-driver-registration class=md-nav__link> csi-driver registration </a> </li> <li class=md-nav__item> <a href=#driver-registrar class=md-nav__link> driver-registrar </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#volume-attachment class=md-nav__link> Volume Attachment </a> <nav class=md-nav aria-label="Volume Attachment"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#csi-attacher class=md-nav__link> csi-attacher </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cephfs-stale-operations class=md-nav__link> CephFS Stale operations </a> </li> <li class=md-nav__item> <a href=#rbd-stale-operations class=md-nav__link> RBD Stale operations </a> </li> <li class=md-nav__item> <a href=#dmesg-logs class=md-nav__link> dmesg logs </a> </li> <li class=md-nav__item> <a href=#rbd-commands class=md-nav__link> RBD Commands </a> </li> <li class=md-nav__item> <a href=#node-loss class=md-nav__link> Node Loss </a> <nav class=md-nav aria-label="Node Loss"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#force-deleting-the-pod class=md-nav__link> Force deleting the pod </a> </li> <li class=md-nav__item> <a href=#blocklisting-a-node class=md-nav__link> Blocklisting a node </a> </li> <li class=md-nav__item> <a href=#removing-a-node-blocklist class=md-nav__link> Removing a node blocklist </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/rook/rook/edit/master/Documentation/ceph-csi-troubleshooting.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=csi-common-issues>CSI Common Issues<a class=headerlink href=#csi-common-issues title="Permanent link">&para;</a></h1> <p>Issues when provisioning volumes with the Ceph CSI driver can happen for many reasons such as:</p> <ul> <li>Network connectivity between CSI pods and ceph</li> <li>Cluster health issues</li> <li>Slow operations</li> <li>Kubernetes issues</li> <li>Ceph-CSI configuration or bugs</li> </ul> <p>The following troubleshooting steps can help identify a number of issues.</p> <h3 id=block-rbd>Block (RBD)<a class=headerlink href=#block-rbd title="Permanent link">&para;</a></h3> <p>If you are mounting block volumes (usually RWO), these are referred to as <code>RBD</code> volumes in Ceph.<br> See the sections below for RBD if you are having block volume issues.</p> <h3 id=shared-filesystem-cephfs>Shared Filesystem (CephFS)<a class=headerlink href=#shared-filesystem-cephfs title="Permanent link">&para;</a></h3> <p>If you are mounting shared filesystem volumes (usually RWX), these are referred to as <code>CephFS</code> volumes in Ceph.<br> See the sections below for CephFS if you are having filesystem volume issues.</p> <h2 id=network-connectivity>Network Connectivity<a class=headerlink href=#network-connectivity title="Permanent link">&para;</a></h2> <p>The Ceph monitors are the most critical component of the cluster to check first.<br> Retrieve the mon endpoints from the services:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph get svc -l app=rook-ceph-mon</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
rook-ceph-mon-a   ClusterIP   10.104.165.31   &lt;none&gt;        6789/TCP,3300/TCP   18h
rook-ceph-mon-b   ClusterIP   10.97.244.93    &lt;none&gt;        6789/TCP,3300/TCP   21s
rook-ceph-mon-c   ClusterIP   10.99.248.163   &lt;none&gt;        6789/TCP,3300/TCP   8s
</code></pre></div> </td></tr></table> </blockquote> <p>If host networking is enabled in the CephCluster CR, you will instead need to find the<br> node IPs for the hosts where the mons are running.</p> <p>The <code>clusterIP</code> is the mon IP and <code>3300</code> is the port that will be used by Ceph-CSI to connect to the ceph cluster.<br> These endpoints must be accessible by all clients in the cluster, including the CSI driver.</p> <p>If you are seeing issues provisioning the PVC then you need to check the network connectivity from the provisioner pods.</p> <ul> <li>For CephFS PVCs, check network connectivity from the <code>csi-cephfsplugin</code> container of the <code>csi-cephfsplugin-provisioner</code> pods</li> <li>For Block PVCs, check network connectivity from the <code>csi-rbdplugin</code> container of the <code>csi-rbdplugin-provisioner</code> pods</li> </ul> <p>For redundancy, there are two provisioner pods for each type. Make sure to test connectivity from all provisioner pods.</p> <p>Connect to the provisioner pods and verify the connection to the mon endpoints such as the following:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span>
<span class=normal>6</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp># </span>Connect to the csi-cephfsplugin container <span class=k>in</span> the provisioner pod
<span class=go>kubectl -n rook-ceph exec -ti deploy/csi-cephfsplugin-provisioner -c csi-cephfsplugin -- bash</span>

<span class=gp># </span>Test the network connection to the mon endpoint
<span class=go>curl 10.104.165.31:3300 2&gt;/dev/null</span>
<span class=go>ceph v2</span>
</code></pre></div> </td></tr></table> <p>If you see the response "ceph v2", the connection succeeded.<br> If there is no response then there is a network issue connecting to the ceph cluster.</p> <p>Check network connectivity for all monitor IP’s and ports which are passed to ceph-csi.</p> <h2 id=ceph-health>Ceph Health<a class=headerlink href=#ceph-health title="Permanent link">&para;</a></h2> <p>Sometimes an unhealthy Ceph cluster can contribute to the issues in creating or mounting the PVC.<br> Check that your Ceph cluster is healthy by connecting to the <a href=../ceph-toolbox/ >Toolbox</a> and<br> running the <code>ceph</code> commands:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph health detail</span>
</code></pre></div> </td></tr></table> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>HEALTH_OK</span>
</code></pre></div> </td></tr></table> <h2 id=slow-operations>Slow Operations<a class=headerlink href=#slow-operations title="Permanent link">&para;</a></h2> <p>Even slow ops in the ceph cluster can contribute to the issues. In the toolbox,<br> make sure that no slow ops are present and the ceph cluster is healthy</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph -s</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>cluster:
  id:     ba41ac93-3b55-4f32-9e06-d3d8c6ff7334
  health: HEALTH_WARN
          30 slow ops, oldest one blocked for 10624 sec, mon.a has slow ops
</code></pre></div> </td></tr></table> </blockquote> <p>If Ceph is not healthy, check the following health for more clues:</p> <ul> <li>The Ceph monitor logs for errors</li> <li>The OSD logs for errors</li> <li>Disk Health</li> <li>Network Health</li> </ul> <h2 id=ceph-troubleshooting>Ceph Troubleshooting<a class=headerlink href=#ceph-troubleshooting title="Permanent link">&para;</a></h2> <h3 id=check-if-the-rbd-pool-exists>Check if the RBD Pool exists<a class=headerlink href=#check-if-the-rbd-pool-exists title="Permanent link">&para;</a></h3> <p>Make sure the pool you have specified in the <code>storageclass.yaml</code> exists in the ceph cluster.</p> <p>Suppose the pool name mentioned in the <code>storageclass.yaml</code> is <code>replicapool</code>. It can be verified<br> to exist in the toolbox:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd lspools</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>1 device_health_metrics
2 replicapool
</code></pre></div> </td></tr></table> </blockquote> <p>If the pool is not in the list, create the <code>CephBlockPool</code> CR for the pool if you have not already.<br> If you have already created the pool, check the Rook operator log for errors creating the pool.</p> <h3 id=check-if-the-filesystem-exists>Check if the Filesystem exists<a class=headerlink href=#check-if-the-filesystem-exists title="Permanent link">&para;</a></h3> <p>For the shared filesystem (CephFS), check that the filesystem and pools you have specified in the <code>storageclass.yaml</code> exist in the Ceph cluster.</p> <p>Suppose the <code>fsName</code> name mentioned in the <code>storageclass.yaml</code> is <code>myfs</code>. It can be verified in the toolbox:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph fs ls</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>name: myfs, metadata pool: myfs-metadata, data pools: [myfs-data0 ]
</code></pre></div> </td></tr></table> </blockquote> <p>Now verify the <code>pool</code> mentioned in the <code>storageclass.yaml</code> exists, such as the example <code>myfs-data0</code>.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd lspools</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>1 device_health_metrics
2 replicapool
3 myfs-metadata0
4 myfs-data0
</code></pre></div> </td></tr></table> </blockquote> <p>The pool for the filesystem will have the suffix <code>-data0</code> compared the filesystem name that is created<br> by the CephFilesystem CR.</p> <h3 id=subvolumegroups>subvolumegroups<a class=headerlink href=#subvolumegroups title="Permanent link">&para;</a></h3> <p>If the subvolumegroup is not specified in the ceph-csi configmap (where you have passed the ceph monitor information),<br> Ceph-CSI creates the default subvolumegroup with the name csi. Verify that the subvolumegroup<br> exists:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph fs subvolumegroup ls myfs</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>[
    {
        &quot;name&quot;: &quot;csi&quot;
    }
]
</code></pre></div> </td></tr></table> </blockquote> <p>If you don’t see any issues with your Ceph cluster, the following sections will start debugging the issue from the CSI side.</p> <h2 id=provisioning-volumes>Provisioning Volumes<a class=headerlink href=#provisioning-volumes title="Permanent link">&para;</a></h2> <p>At times the issue can also exist in the Ceph-CSI or the sidecar containers used in Ceph-CSI.</p> <p>Ceph-CSI has included number of sidecar containers in the provisioner pods such as:<br> <code>csi-attacher</code>, <code>csi-resizer</code>, <code>csi-provisioner</code>, <code>csi-cephfsplugin</code>, <code>csi-snapshotter</code>, and <code>liveness-prometheus</code>.</p> <p>The CephFS provisioner core CSI driver container name is <code>csi-cephfsplugin</code> as one of the container names.<br> For the RBD (Block) provisioner you will see <code>csi-rbdplugin</code> as the container name.</p> <p>Here is a summary of the sidecar containers:</p> <h3 id=csi-provisioner>csi-provisioner<a class=headerlink href=#csi-provisioner title="Permanent link">&para;</a></h3> <p>The external-provisioner is a sidecar container that dynamically provisions volumes by calling <code>ControllerCreateVolume()</code><br> and <code>ControllerDeleteVolume()</code> functions of CSI drivers. More details about external-provisioner can be found here.</p> <p>If there is an issue with PVC Create or Delete, check the logs of the <code>csi-provisioner</code> sidecar container.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-provisioner</span>
</code></pre></div> </td></tr></table> <h3 id=csi-resizer>csi-resizer<a class=headerlink href=#csi-resizer title="Permanent link">&para;</a></h3> <p>The CSI <code>external-resizer</code> is a sidecar container that watches the Kubernetes API server for PersistentVolumeClaim<br> updates and triggers <code>ControllerExpandVolume</code> operations against a CSI endpoint if the user requested more storage<br> on the PersistentVolumeClaim object. More details about external-provisioner can be found here.</p> <p>If any issue exists in PVC expansion you can check the logs of the <code>csi-resizer</code> sidecar container.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-resizer</span>
</code></pre></div> </td></tr></table> <h3 id=csi-snapshotter>csi-snapshotter<a class=headerlink href=#csi-snapshotter title="Permanent link">&para;</a></h3> <p>The CSI external-snapshotter sidecar only watches for <code>VolumeSnapshotContent</code> create/update/delete events.<br> It will talk to ceph-csi containers to create or delete snapshots. More details about external-snapshotter can<br> be found <a href=https://github.com/kubernetes-csi/external-snapshotter>here</a>.</p> <p><strong>In Kubernetes 1.17 the volume snapshot feature was promoted to beta. In Kubernetes 1.20, the feature gate is enabled by default on standard Kubernetes deployments and cannot be turned off.</strong></p> <p>Make sure you have installed the correct snapshotter CRD version. If you have not installed the snapshotter<br> controller, see the <a href=../ceph-csi-snapshot/ >Snapshots guide</a>.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl get crd | grep snapshot</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>volumesnapshotclasses.snapshot.storage.k8s.io    2021-01-25T11:19:38Z
volumesnapshotcontents.snapshot.storage.k8s.io   2021-01-25T11:19:39Z
volumesnapshots.snapshot.storage.k8s.io          2021-01-25T11:19:40Z
</code></pre></div> </td></tr></table> </blockquote> <p>The above CRDs must have the matching version in your <code>snapshotclass.yaml</code> or <code>snapshot.yaml</code>.<br> Otherwise, the <code>VolumeSnapshot</code> and <code>VolumesnapshotContent</code> will not be created.</p> <p>The snapshot controller is responsible for creating both <code>VolumeSnapshot</code> and<br> <code>VolumesnapshotContent</code> object. If the objects are not getting created, you may need to<br> check the logs of the snapshot-controller container.</p> <p>Rook only installs the snapshotter sidecar container, not the controller. It is recommended<br> that Kubernetes distributors bundle and deploy the controller and CRDs as part of their Kubernetes cluster<br> management process (independent of any CSI Driver).</p> <p>If your Kubernetes distribution does not bundle the snapshot controller, you may manually install these components.</p> <p>If any issue exists in the snapshot Create/Delete operation you can check the logs of the csi-snapshotter sidecar container.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph logs deploy/csi-rbdplugin-provisioner -c csi-snapshotter</span>
</code></pre></div> </td></tr></table> <p>If you see an error such as:</p> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>GRPC error: rpc error: code = Aborted desc = an operation with the given Volume ID
0001-0009-rook-ceph-0000000000000001-8d0ba728-0e17-11eb-a680-ce6eecc894de already &gt;exists.
</code></pre></div> </td></tr></table> </blockquote> <p>The issue typically is in the Ceph cluster or network connectivity. If the issue is<br> in Provisioning the PVC Restarting the Provisioner pods help(for CephFS issue<br> restart <code>csi-cephfsplugin-provisioner-xxxxxx</code> CephFS Provisioner. For RBD, restart<br> the <code>csi-rbdplugin-provisioner-xxxxxx</code> pod. If the issue is in mounting the PVC,<br> restart the <code>csi-rbdplugin-xxxxx</code> pod (for RBD) and the <code>csi-cephfsplugin-xxxxx</code> pod<br> for CephFS issue.</p> <h2 id=mounting-the-volume-to-application-pods>Mounting the volume to application pods<a class=headerlink href=#mounting-the-volume-to-application-pods title="Permanent link">&para;</a></h2> <p>When a user requests to create the application pod with PVC, there is a three-step process</p> <ul> <li>CSI driver registration</li> <li>Create volume attachment object</li> <li>Stage and publish the volume</li> </ul> <h3 id=csi-driver-registration>csi-driver registration<a class=headerlink href=#csi-driver-registration title="Permanent link">&para;</a></h3> <p><code>csi-cephfsplugin-xxxx</code> or <code>csi-rbdplugin-xxxx</code> is a daemonset pod running on all the nodes<br> where your application gets scheduled. If the plugin pods are not running on the node where<br> your application is scheduled might cause the issue, make sure plugin pods are always running.</p> <p>Each plugin pod has two important containers: one is <code>driver-registrar</code> and <code>csi-rbdplugin</code> or<br> <code>csi-cephfsplugin</code>. Sometimes there is also a <code>liveness-prometheus</code> container.</p> <h3 id=driver-registrar>driver-registrar<a class=headerlink href=#driver-registrar title="Permanent link">&para;</a></h3> <p>The node-driver-registrar is a sidecar container that registers the CSI driver with Kubelet.<br> More details can be found <a href=https://github.com/kubernetes-csi/node-driver-registrar>here</a>.</p> <p>If any issue exists in attaching the PVC to the application pod check logs from driver-registrar<br> sidecar container in plugin pod where your application pod is scheduled.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph logs deploy/csi-rbdplugin -c driver-registrar</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span>
<span class=normal>6</span>
<span class=normal>7</span>
<span class=normal>8</span>
<span class=normal>9</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>I0120 12:28:34.231761  124018 main.go:112] Version: v2.0.1
I0120 12:28:34.233910  124018 connection.go:151] Connecting to unix:///csi/csi.sock
I0120 12:28:35.242469  124018 node_register.go:55] Starting Registration Server at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock
I0120 12:28:35.243364  124018 node_register.go:64] Registration Server started at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock
I0120 12:28:35.243673  124018 node_register.go:86] Skipping healthz server because port set to: 0
I0120 12:28:36.318482  124018 main.go:79] Received GetInfo call: &amp;InfoRequest{}
I0120 12:28:37.455211  124018 main.go:89] Received NotifyRegistrationStatus call: &amp;RegistrationStatus{PluginRegistered:true,Error:,}
E0121 05:19:28.658390  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.
E0125 07:11:42.926133  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.
</code></pre></div> </td></tr></table> </blockquote> <p>You should see the response <code>RegistrationStatus{PluginRegistered:true,Error:,}</code> in the logs to<br> confirm that plugin is registered with kubelet.</p> <p>If you see a driver not found an error in the application pod describe output.<br> Restarting the <code>csi-xxxxplugin-xxx</code> pod on the node may help.</p> <h2 id=volume-attachment>Volume Attachment<a class=headerlink href=#volume-attachment title="Permanent link">&para;</a></h2> <p>Each provisioner pod also has a sidecar container called <code>csi-attacher</code>.</p> <h3 id=csi-attacher>csi-attacher<a class=headerlink href=#csi-attacher title="Permanent link">&para;</a></h3> <p>The external-attacher is a sidecar container that attaches volumes to nodes by calling <code>ControllerPublish</code> and<br> <code>ControllerUnpublish</code> functions of CSI drivers. It is necessary because the internal Attach/Detach controller<br> running in Kubernetes controller-manager does not have any direct interfaces to CSI drivers. More details can<br> be found <a href=https://github.com/kubernetes-csi/external-attacher>here</a>.</p> <p>If any issue exists in attaching the PVC to the application pod first check the volumettachment object created<br> and also log from csi-attacher sidecar container in provisioner pod.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl get volumeattachment</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>NAME                                                                   ATTACHER                        PV                                         NODE       ATTACHED   AGE
csi-75903d8a902744853900d188f12137ea1cafb6c6f922ebc1c116fd58e950fc92   rook-ceph.cephfs.csi.ceph.com   pvc-5c547d2a-fdb8-4cb2-b7fe-e0f30b88d454   minikube   true       4m26s
</code></pre></div> </td></tr></table> </blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-attacher</span>
</code></pre></div> </td></tr></table> <h2 id=cephfs-stale-operations>CephFS Stale operations<a class=headerlink href=#cephfs-stale-operations title="Permanent link">&para;</a></h2> <p>Check for any stale mount commands on the <code>csi-cephfsplugin-xxxx</code> pod on the node where your application pod is scheduled.</p> <p>You need to exec in the <code>csi-cephfsplugin-xxxx</code> pod and grep for stale mount operators.</p> <p>Identify the <code>csi-cephfsplugin-xxxx</code> pod running on the node where your application is scheduled with<br> <code>kubectl get po -o wide</code> and match the node names.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl exec -it csi-cephfsplugin-tfk2g -c csi-cephfsplugin -- sh</span>
<span class=go>ps -ef |grep mount</span>

<span class=go>root          67      60  0 11:55 pts/0    00:00:00 grep mount</span>
</code></pre></div> </td></tr></table> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ps -ef |grep ceph</span>

<span class=go>root           1       0  0 Jan20 ?        00:00:26 /usr/local/bin/cephcsi --nodeid=minikube --type=cephfs --endpoint=unix:///csi/csi.sock --v=0 --nodeserver=true --drivername=rook-ceph.cephfs.csi.ceph.com --pidlimit=-1 --metricsport=9091 --forcecephkernelclient=true --metricspath=/metrics --enablegrpcmetrics=true</span>
<span class=go>root          69      60  0 11:55 pts/0    00:00:00 grep ceph</span>
</code></pre></div> </td></tr></table> <p>If any commands are stuck check the <strong>dmesg</strong> logs from the node.<br> Restarting the <code>csi-cephfsplugin</code> pod may also help sometimes.</p> <p>If you don’t see any stuck messages, confirm the network connectivity, Ceph health, and slow ops.</p> <h2 id=rbd-stale-operations>RBD Stale operations<a class=headerlink href=#rbd-stale-operations title="Permanent link">&para;</a></h2> <p>Check for any stale <code>map/mkfs/mount</code> commands on the <code>csi-rbdplugin-xxxx</code> pod on the node where your application pod is scheduled.</p> <p>You need to exec in the <code>csi-rbdplugin-xxxx</code> pod and grep for stale operators like (<code>rbd map, rbd unmap, mkfs, mount</code> and <code>umount</code>).</p> <p>Identify the <code>csi-rbdplugin-xxxx</code> pod running on the node where your application is scheduled with<br> <code>kubectl get po -o wide</code> and match the node names.</p> <p><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl exec -it csi-rbdplugin-vh8d5 -c csi-rbdplugin -- sh</span>
</code></pre></div> </td></tr></table><br> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ps -ef |grep map</span>
</code></pre></div> </td></tr></table></p> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>root     1297024 1296907  0 12:00 pts/0    00:00:00 grep map
</code></pre></div> </td></tr></table> </blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ps -ef |grep mount</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>root        1824       1  0 Jan19 ?        00:00:00 /usr/sbin/rpc.mountd
ceph     1041020 1040955  1 07:11 ?        00:03:43 ceph-mgr --fsid=ba41ac93-3b55-4f32-9e06-d3d8c6ff7334 --keyring=/etc/ceph/keyring-store/keyring --log-to-stderr=true --err-to-stderr=true --mon-cluster-log-to-stderr=true --log-stderr-prefix=debug  --default-log-to-file=false --default-mon-cluster-log-to-file=false --mon-host=[v2:10.111.136.166:3300,v1:10.111.136.166:6789] --mon-initial-members=a --id=a --setuser=ceph --setgroup=ceph --client-mount-uid=0 --client-mount-gid=0 --foreground --public-addr=172.17.0.6
root     1297115 1296907  0 12:00 pts/0    00:00:00 grep mount
</code></pre></div> </td></tr></table> </blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ps -ef |grep mkfs</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>root     1297291 1296907  0 12:00 pts/0    00:00:00 grep mkfs
</code></pre></div> </td></tr></table> </blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ps -ef |grep umount</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>root     1298500 1296907  0 12:01 pts/0    00:00:00 grep umount
</code></pre></div> </td></tr></table> </blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ps -ef |grep unmap</span>
</code></pre></div> </td></tr></table> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>root     1298578 1296907  0 12:01 pts/0    00:00:00 grep unmap
</code></pre></div> </td></tr></table> </blockquote> <p>If any commands are stuck check the <strong>dmesg</strong> logs from the node.<br> Restarting the <code>csi-rbdplugin</code> pod also may help sometimes.</p> <p>If you don’t see any stuck messages, confirm the network connectivity, Ceph health, and slow ops.</p> <h2 id=dmesg-logs>dmesg logs<a class=headerlink href=#dmesg-logs title="Permanent link">&para;</a></h2> <p>Check the dmesg logs on the node where pvc mounting is failing or the <code>csi-rbdplugin</code> container of the<br> <code>csi-rbdplugin-xxxx</code> pod on that node.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>dmesg</span>
</code></pre></div> </td></tr></table> <h2 id=rbd-commands>RBD Commands<a class=headerlink href=#rbd-commands title="Permanent link">&para;</a></h2> <p>If nothing else helps, get the last executed command from the ceph-csi pod logs and run it manually inside<br> the provisioner or plugin pod to see if there are errors returned even if they couldn't be seen in the logs.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>rbd ls --id<span class=o>=</span>csi-rbd-node -m<span class=o>=</span><span class=m>10</span>.111.136.166:6789 --key<span class=o>=</span>AQDpIQhg+v83EhAAgLboWIbl+FL/nThJzoI3Fg<span class=o>==</span>
</code></pre></div> </td></tr></table> <p>Where <code>-m</code> is one of the mon endpoints and the <code>--key</code> is the key used by the CSI driver for accessing the Ceph cluster.</p> <h2 id=node-loss>Node Loss<a class=headerlink href=#node-loss title="Permanent link">&para;</a></h2> <p>When a node is lost, you will see application pods on the node stuck in the <code>Terminating</code> state while another pod is rescheduled and is in the <code>ContainerCreating</code> state.</p> <p>To allow the application pod to start on another node, force delete the pod.</p> <h3 id=force-deleting-the-pod>Force deleting the pod<a class=headerlink href=#force-deleting-the-pod title="Permanent link">&para;</a></h3> <p>To force delete the pod stuck in the <code>Terminating</code> state:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>kubectl -n rook-ceph delete pod my-app-69cd495f9b-nl6hf --grace-period <span class=m>0</span> --force
</code></pre></div> </td></tr></table> <p>After the force delete, wait for a timeout of about 8-10 minutes. If the pod still not in the running state, continue with the next section to blocklist the node.</p> <h3 id=blocklisting-a-node>Blocklisting a node<a class=headerlink href=#blocklisting-a-node title="Permanent link">&para;</a></h3> <p>To shorten the timeout, you can mark the node as "blocklisted" from the <a href=../ceph-toolbox/ >Rook toolbox</a> so Rook can safely failover the pod sooner.</p> <p>If the Ceph version is at least Pacific(v16.2.x), run the following command:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>ceph osd blocklist add &lt;NODE_IP&gt; <span class=c1># get the node IP you want to blocklist</span>
<span class=go>blocklisting &lt;NODE_IP&gt;</span>
</code></pre></div> </td></tr></table> <p>If the Ceph version is Octopus(v15.2.x) or older, run the following command:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>ceph osd blacklist add &lt;NODE_IP&gt; <span class=c1># get the node IP you want to blacklist</span>
<span class=go>blacklisting &lt;NODE_IP&gt;</span>
</code></pre></div> </td></tr></table> <p>After running the above command within a few minutes the pod will be running.</p> <h3 id=removing-a-node-blocklist>Removing a node blocklist<a class=headerlink href=#removing-a-node-blocklist title="Permanent link">&para;</a></h3> <p>After you are absolutely sure the node is permanently offline and that the node no longer needs to be blocklisted, remove the node from the blocklist.</p> <p>If the Ceph version is at least Pacific(v16.2.x), run:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>ceph osd blocklist rm &lt;NODE_IP&gt;
<span class=go>un-blocklisting &lt;NODE_IP&gt;</span>
</code></pre></div> </td></tr></table> <p>If the Ceph version is Octopus(v15.2.x) or older, run:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>ceph osd blacklist rm &lt;NODE_IP&gt; <span class=c1># get the node IP you want to blacklist</span>
<span class=go>un-blacklisting &lt;NODE_IP&gt;</span>
</code></pre></div> </td></tr></table> </article> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> </div> </div> <a href=# class="md-top md-top--hidden md-icon" data-md-component=top> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../ceph-csi-snapshot/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Snapshots" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Snapshots </div> </div> </a> <a href=../ceph-csi-volume-clone/ class="md-footer__link md-footer__link--next" aria-label="Next: Volume clone" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> Volume clone </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> <a href=/ class=logo> <img src=/images/rook-logo-small.svg alt=rook.io> </a> <p> &#169; Rook Authors {{ site.time | date: '%Y' }}. Documentation distributed under <a href=https://creativecommons.org/licenses/by/4.0>CC-BY-4.0</a>. </p> <p> &#169; {{ site.time | date: '%Y' }} The Linux Foundation. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage/ >Trademark Usage</a> page. </p> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs Insiders </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.tabs.link", "instant", "navigation.expand", "navigation.top", "navigation.tracking", "privacy", "search.highlight", "search.share", "search.suggest", "tabs"], "search": "../assets/javascripts/workers/search.5c9dbbf3.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../assets/javascripts/bundle.10bf1588.min.js></script> </body> </html>