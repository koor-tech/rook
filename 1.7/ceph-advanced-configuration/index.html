<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Rook Ceph Documentation"><meta name=author content="Rook Authors"><link href=https://rook.io/1.7/ceph-advanced-configuration/ rel=canonical><link rel=icon href=https://rook.io/images/favicon_192x192.png><meta name=generator content="mkdocs-1.3.0, mkdocs-material-8.2.5+insiders-4.11.0"><title>Advanced Configuration - Rook Ceph Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.589a02ac.min.css><link rel=stylesheet href=../assets/stylesheets/palette.e6a45f82.min.css><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#advanced-configuration class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-component=outdated hidden> <aside class="md-banner md-banner--warning"> <div class="md-banner__inner md-grid md-typeset"> You're not viewing the latest version. <a href=../..> <strong>Click here to go to latest.</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=outdated]"),outdated=__md_get("__outdated",sessionStorage);!0===outdated&&el&&(el.hidden=!1)</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Rook Ceph Documentation" class="md-header__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Rook Ceph Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Advanced Configuration </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=rook-blue data-md-color-accent=red aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Rook Ceph Documentation" class="md-nav__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> Rook Ceph Documentation </label> <div class=md-nav__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Rook </span> </a> </li> <li class=md-nav__item> <a href=../async-disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Failover and Failback </span> </a> </li> <li class=md-nav__item> <a href=../authenticated-registry/ class=md-nav__link> <span class=md-ellipsis> Authenticated Registries </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Advanced Configuration </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Advanced Configuration </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> Prerequisites </a> </li> <li class=md-nav__item> <a href=#using-alternate-namespaces class=md-nav__link> Using alternate namespaces </a> </li> <li class=md-nav__item> <a href=#deploying-a-second-cluster class=md-nav__link> Deploying a second cluster </a> </li> <li class=md-nav__item> <a href=#log-collection class=md-nav__link> Log Collection </a> </li> <li class=md-nav__item> <a href=#osd-information class=md-nav__link> OSD Information </a> <nav class=md-nav aria-label="OSD Information"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#kubernetes class=md-nav__link> Kubernetes </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#separate-storage-groups class=md-nav__link> Separate Storage Groups </a> </li> <li class=md-nav__item> <a href=#configuring-pools class=md-nav__link> Configuring Pools </a> <nav class=md-nav aria-label="Configuring Pools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#placement-group-sizing class=md-nav__link> Placement Group Sizing </a> </li> <li class=md-nav__item> <a href=#setting-pg-count class=md-nav__link> Setting PG Count </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#custom-cephconf-settings class=md-nav__link> Custom ceph.conf Settings </a> <nav class=md-nav aria-label="Custom ceph.conf Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example class=md-nav__link> Example </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#custom-csi-cephconf-settings class=md-nav__link> Custom CSI ceph.conf Settings </a> <nav class=md-nav aria-label="Custom CSI ceph.conf Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example_1 class=md-nav__link> Example </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-crush-settings class=md-nav__link> OSD CRUSH Settings </a> <nav class=md-nav aria-label="OSD CRUSH Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#osd-weight class=md-nav__link> OSD Weight </a> </li> <li class=md-nav__item> <a href=#osd-primary-affinity class=md-nav__link> OSD Primary Affinity </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-dedicated-network class=md-nav__link> OSD Dedicated Network </a> <nav class=md-nav aria-label="OSD Dedicated Network"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#use-hostnetwork-in-the-rook-ceph-cluster-configuration class=md-nav__link> Use hostNetwork in the rook ceph cluster configuration </a> </li> <li class=md-nav__item> <a href=#define-the-subnets-to-use-for-public-and-private-osd-networks class=md-nav__link> Define the subnets to use for public and private OSD networks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#phantom-osd-removal class=md-nav__link> Phantom OSD Removal </a> </li> <li class=md-nav__item> <a href=#auto-expansion-of-osds class=md-nav__link> Auto Expansion of OSDs </a> <nav class=md-nav aria-label="Auto Expansion of OSDs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#prerequisites_1 class=md-nav__link> Prerequisites </a> </li> <li class=md-nav__item> <a href=#to-scale-osds-vertically class=md-nav__link> To scale OSDs Vertically </a> </li> <li class=md-nav__item> <a href=#to-scale-osds-horizontally class=md-nav__link> To scale OSDs Horizontally </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../ceph-block/ class=md-nav__link> <span class=md-ellipsis> Block Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-client-crd/ class=md-nav__link> <span class=md-ellipsis> Client CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-cluster-crd/ class=md-nav__link> <span class=md-ellipsis> Cluster CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-drivers/ class=md-nav__link> <span class=md-ellipsis> Ceph CSI </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-snapshot/ class=md-nav__link> <span class=md-ellipsis> Snapshots </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-troubleshooting/ class=md-nav__link> <span class=md-ellipsis> CSI Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-csi-volume-clone/ class=md-nav__link> <span class=md-ellipsis> Volume clone </span> </a> </li> <li class=md-nav__item> <a href=../ceph-dashboard/ class=md-nav__link> <span class=md-ellipsis> Ceph Dashboard </span> </a> </li> <li class=md-nav__item> <a href=../ceph-disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Disaster Recovery </span> </a> </li> <li class=md-nav__item> <a href=../ceph-examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> <li class=md-nav__item> <a href=../ceph-filesystem-crd/ class=md-nav__link> <span class=md-ellipsis> Shared Filesystem CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-filesystem/ class=md-nav__link> <span class=md-ellipsis> Shared Filesystem </span> </a> </li> <li class=md-nav__item> <a href=../ceph-fs-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> Filesystem Mirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-fs-subvolumegroup/ class=md-nav__link> <span class=md-ellipsis> SubVolume Group CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-kms/ class=md-nav__link> <span class=md-ellipsis> Key Management System </span> </a> </li> <li class=md-nav__item> <a href=../ceph-mon-health/ class=md-nav__link> <span class=md-ellipsis> Monitor Health </span> </a> </li> <li class=md-nav__item> <a href=../ceph-monitoring/ class=md-nav__link> <span class=md-ellipsis> Prometheus Monitoring </span> </a> </li> <li class=md-nav__item> <a href=../ceph-nfs-crd/ class=md-nav__link> <span class=md-ellipsis> NFS CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-bucket-claim/ class=md-nav__link> <span class=md-ellipsis> Object Bucket Claim </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-bucket-notifications/ class=md-nav__link> <span class=md-ellipsis> Bucket Notifications </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-multisite-crd/ class=md-nav__link> <span class=md-ellipsis> Object Multisite CRDs </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-multisite/ class=md-nav__link> <span class=md-ellipsis> Object Multisite </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-store-crd/ class=md-nav__link> <span class=md-ellipsis> Object Store CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object-store-user-crd/ class=md-nav__link> <span class=md-ellipsis> Object Store User CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-object/ class=md-nav__link> <span class=md-ellipsis> Object Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-openshift-issues/ class=md-nav__link> <span class=md-ellipsis> OpenShift Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../ceph-openshift/ class=md-nav__link> <span class=md-ellipsis> OpenShift </span> </a> </li> <li class=md-nav__item> <a href=../ceph-osd-mgmt/ class=md-nav__link> <span class=md-ellipsis> OSD Management </span> </a> </li> <li class=md-nav__item> <a href=../ceph-pool-crd/ class=md-nav__link> <span class=md-ellipsis> Block Pool CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-pool-radosnamespace/ class=md-nav__link> <span class=md-ellipsis> RADOS Namespace CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-rbd-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> RBD Mirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../ceph-storage/ class=md-nav__link> <span class=md-ellipsis> Ceph Storage </span> </a> </li> <li class=md-nav__item> <a href=../ceph-teardown/ class=md-nav__link> <span class=md-ellipsis> Cleanup </span> </a> </li> <li class=md-nav__item> <a href=../ceph-toolbox/ class=md-nav__link> <span class=md-ellipsis> Toolbox </span> </a> </li> <li class=md-nav__item> <a href=../ceph-tools/ class=md-nav__link> <span class=md-ellipsis> Ceph Tools </span> </a> </li> <li class=md-nav__item> <a href=../ceph-upgrade/ class=md-nav__link> <span class=md-ellipsis> Upgrades </span> </a> </li> <li class=md-nav__item> <a href=../common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../development-environment/ class=md-nav__link> <span class=md-ellipsis> Developer Environment </span> </a> </li> <li class=md-nav__item> <a href=../development-flow/ class=md-nav__link> <span class=md-ellipsis> Contributing </span> </a> </li> <li class=md-nav__item> <a href=../direct-tools/ class=md-nav__link> <span class=md-ellipsis> Direct Tools </span> </a> </li> <li class=md-nav__item> <a href=../helm-ceph-cluster/ class=md-nav__link> <span class=md-ellipsis> Ceph Cluster </span> </a> </li> <li class=md-nav__item> <a href=../helm-operator/ class=md-nav__link> <span class=md-ellipsis> Ceph Operator </span> </a> </li> <li class=md-nav__item> <a href=../helm/ class=md-nav__link> <span class=md-ellipsis> Helm Charts </span> </a> </li> <li class=md-nav__item> <a href=../pod-security-policies/ class=md-nav__link> <span class=md-ellipsis> Pod Security Policies </span> </a> </li> <li class=md-nav__item> <a href=../pre-reqs/ class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=../quickstart/ class=md-nav__link> <span class=md-ellipsis> Quickstart </span> </a> </li> <li class=md-nav__item> <a href=../rbd-mirroring/ class=md-nav__link> <span class=md-ellipsis> RBD Mirroring </span> </a> </li> <li class=md-nav__item> <a href=../storage-providers/ class=md-nav__link> <span class=md-ellipsis> Storage Providers </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> Prerequisites </a> </li> <li class=md-nav__item> <a href=#using-alternate-namespaces class=md-nav__link> Using alternate namespaces </a> </li> <li class=md-nav__item> <a href=#deploying-a-second-cluster class=md-nav__link> Deploying a second cluster </a> </li> <li class=md-nav__item> <a href=#log-collection class=md-nav__link> Log Collection </a> </li> <li class=md-nav__item> <a href=#osd-information class=md-nav__link> OSD Information </a> <nav class=md-nav aria-label="OSD Information"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#kubernetes class=md-nav__link> Kubernetes </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#separate-storage-groups class=md-nav__link> Separate Storage Groups </a> </li> <li class=md-nav__item> <a href=#configuring-pools class=md-nav__link> Configuring Pools </a> <nav class=md-nav aria-label="Configuring Pools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#placement-group-sizing class=md-nav__link> Placement Group Sizing </a> </li> <li class=md-nav__item> <a href=#setting-pg-count class=md-nav__link> Setting PG Count </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#custom-cephconf-settings class=md-nav__link> Custom ceph.conf Settings </a> <nav class=md-nav aria-label="Custom ceph.conf Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example class=md-nav__link> Example </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#custom-csi-cephconf-settings class=md-nav__link> Custom CSI ceph.conf Settings </a> <nav class=md-nav aria-label="Custom CSI ceph.conf Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example_1 class=md-nav__link> Example </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-crush-settings class=md-nav__link> OSD CRUSH Settings </a> <nav class=md-nav aria-label="OSD CRUSH Settings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#osd-weight class=md-nav__link> OSD Weight </a> </li> <li class=md-nav__item> <a href=#osd-primary-affinity class=md-nav__link> OSD Primary Affinity </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-dedicated-network class=md-nav__link> OSD Dedicated Network </a> <nav class=md-nav aria-label="OSD Dedicated Network"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#use-hostnetwork-in-the-rook-ceph-cluster-configuration class=md-nav__link> Use hostNetwork in the rook ceph cluster configuration </a> </li> <li class=md-nav__item> <a href=#define-the-subnets-to-use-for-public-and-private-osd-networks class=md-nav__link> Define the subnets to use for public and private OSD networks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#phantom-osd-removal class=md-nav__link> Phantom OSD Removal </a> </li> <li class=md-nav__item> <a href=#auto-expansion-of-osds class=md-nav__link> Auto Expansion of OSDs </a> <nav class=md-nav aria-label="Auto Expansion of OSDs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#prerequisites_1 class=md-nav__link> Prerequisites </a> </li> <li class=md-nav__item> <a href=#to-scale-osds-vertically class=md-nav__link> To scale OSDs Vertically </a> </li> <li class=md-nav__item> <a href=#to-scale-osds-horizontally class=md-nav__link> To scale OSDs Horizontally </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/rook/rook/edit/master/Documentation/ceph-advanced-configuration.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <p>{% include_relative branch.liquid %}</p> <h1 id=advanced-configuration>Advanced Configuration<a class=headerlink href=#advanced-configuration title="Permanent link">&para;</a></h1> <p>These examples show how to perform advanced configuration tasks on your Rook<br> storage cluster.</p> <ul> <li><a href=#prerequisites>Prerequisites</a></li> <li><a href=#using-alternate-namespaces>Using alternate namespaces</a></li> <li><a href=#deploying-a-second-cluster>Deploying a second cluster</a></li> <li><a href=#log-collection>Log Collection</a></li> <li><a href=#osd-information>OSD Information</a></li> <li><a href=#separate-storage-groups>Separate Storage Groups</a></li> <li><a href=#configuring-pools>Configuring Pools</a></li> <li><a href=#custom-cephconf-settings>Custom ceph.conf Settings</a></li> <li><a href=#custom-csi-cephconf-settings>Custom CSI ceph.conf Settings</a></li> <li><a href=#osd-crush-settings>OSD CRUSH Settings</a></li> <li><a href=#osd-dedicated-network>OSD Dedicated Network</a></li> <li><a href=#phantom-osd-removal>Phantom OSD Removal</a></li> <li><a href=#auto-expansion-of-osds>Auto Expansion of OSDs</a></li> </ul> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h2> <p>Most of the examples make use of the <code>ceph</code> client command. A quick way to use<br> the Ceph client suite is from a <a href=../ceph-toolbox/ >Rook Toolbox container</a>.</p> <p>The Kubernetes based examples assume Rook OSD pods are in the <code>rook-ceph</code> namespace.<br> If you run them in a different namespace, modify <code>kubectl -n rook-ceph [...]</code> to fit<br> your situation.</p> <h2 id=using-alternate-namespaces>Using alternate namespaces<a class=headerlink href=#using-alternate-namespaces title="Permanent link">&para;</a></h2> <p>If you wish to deploy the Rook Operator and/or Ceph clusters to namespaces other than the default<br> <code>rook-ceph</code>, the manifests are commented to allow for easy <code>sed</code> replacements. Change<br> <code>ROOK_CLUSTER_NAMESPACE</code> to tailor the manifests for additional Ceph clusters. You can choose<br> to also change <code>ROOK_OPERATOR_NAMESPACE</code> to create a new Rook Operator for each Ceph cluster (don't<br> forget to set <code>ROOK_CURRENT_NAMESPACE_ONLY</code>), or you can leave it at the same value for every<br> Ceph cluster if you only wish to have one Operator manage all Ceph clusters.</p> <p>This will help you manage namespaces more easily, but you should still make sure the resources are<br> configured to your liking.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nb>cd</span> deploy/examples

<span class=nb>export</span> <span class=nv>ROOK_OPERATOR_NAMESPACE</span><span class=o>=</span><span class=s2>&quot;rook-ceph&quot;</span>
<span class=nb>export</span> <span class=nv>ROOK_CLUSTER_NAMESPACE</span><span class=o>=</span><span class=s2>&quot;rook-ceph&quot;</span>

sed -i.bak <span class=se>\</span>
    -e <span class=s2>&quot;s/\(.*\):.*# namespace:operator/\1: </span><span class=nv>$ROOK_OPERATOR_NAMESPACE</span><span class=s2> # namespace:operator/g&quot;</span> <span class=se>\</span>
    -e <span class=s2>&quot;s/\(.*\):.*# namespace:cluster/\1: </span><span class=nv>$ROOK_CLUSTER_NAMESPACE</span><span class=s2> # namespace:cluster/g&quot;</span> <span class=se>\</span>
    -e <span class=s2>&quot;s/\(.*serviceaccount\):.*:\(.*\) # serviceaccount:namespace:operator/\1:</span><span class=nv>$ROOK_OPERATOR_NAMESPACE</span><span class=s2>:\2 # serviceaccount:namespace:operator/g&quot;</span> <span class=se>\</span>
    -e <span class=s2>&quot;s/\(.*serviceaccount\):.*:\(.*\) # serviceaccount:namespace:cluster/\1:</span><span class=nv>$ROOK_CLUSTER_NAMESPACE</span><span class=s2>:\2 # serviceaccount:namespace:cluster/g&quot;</span> <span class=se>\</span>
    -e <span class=s2>&quot;s/\(.*\): [-_A-Za-z0-9]*\.\(.*\) # driver:namespace:operator/\1: </span><span class=nv>$ROOK_OPERATOR_NAMESPACE</span><span class=s2>.\2 # driver:namespace:operator/g&quot;</span> <span class=se>\</span>
    -e <span class=s2>&quot;s/\(.*\): [-_A-Za-z0-9]*\.\(.*\) # driver:namespace:cluster/\1: </span><span class=nv>$ROOK_CLUSTER_NAMESPACE</span><span class=s2>.\2 # driver:namespace:cluster/g&quot;</span> <span class=se>\</span>
  common.yaml operator.yaml cluster.yaml <span class=c1># add other files or change these as desired for your config</span>

<span class=c1># You need to use `apply` for all Ceph clusters after the first if you have only one Operator</span>
kubectl apply -f common.yaml -f operator.yaml -f cluster.yaml <span class=c1># add other files as desired for yourconfig</span>
</code></pre></div> </td></tr></table> <h2 id=deploying-a-second-cluster>Deploying a second cluster<a class=headerlink href=#deploying-a-second-cluster title="Permanent link">&para;</a></h2> <p>If you wish to create a new CephCluster in a different namespace than <code>rook-ceph</code> while using a single operator to manage both clusters execute the following:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nb>cd</span> deploy/examples

<span class=nv>NAMESPACE</span><span class=o>=</span>rook-ceph-secondary envsubst &lt; common-second-cluster.yaml <span class=p>|</span> kubectl create -f -
</code></pre></div> </td></tr></table> <p>This will create all the necessary RBACs as well as the new namespace. The script assumes that <code>common.yaml</code> was already created.<br> When you create the second CephCluster CR, use the same <code>NAMESPACE</code> and the operator will configure the second cluster.</p> <h2 id=log-collection>Log Collection<a class=headerlink href=#log-collection title="Permanent link">&para;</a></h2> <p>All Rook logs can be collected in a Kubernetes environment with the following command:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span>
<span class=normal>6</span>
<span class=normal>7</span>
<span class=normal>8</span>
<span class=normal>9</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>for p in $(kubectl -n rook-ceph get pods -o jsonpath=&#39;{.items[*].metadata.name}&#39;)</span>
<span class=go>do</span>
<span class=go>    for c in $(kubectl -n rook-ceph get pod ${p} -o jsonpath=&#39;{.spec.containers[*].name}&#39;)</span>
<span class=go>    do</span>
<span class=go>        echo &quot;BEGIN logs from pod: ${p} ${c}&quot;</span>
<span class=go>        kubectl -n rook-ceph logs -c ${c} ${p}</span>
<span class=go>        echo &quot;END logs from pod: ${p} ${c}&quot;</span>
<span class=go>    done</span>
<span class=go>done</span>
</code></pre></div> </td></tr></table> <p>This gets the logs for every container in every Rook pod and then compresses them into a <code>.gz</code> archive<br> for easy sharing. Note that instead of <code>gzip</code>, you could instead pipe to <code>less</code> or to a single text file.</p> <h2 id=osd-information>OSD Information<a class=headerlink href=#osd-information title="Permanent link">&para;</a></h2> <p>Keeping track of OSDs and their underlying storage devices can be<br> difficult. The following scripts will clear things up quickly.</p> <h3 id=kubernetes>Kubernetes<a class=headerlink href=#kubernetes title="Permanent link">&para;</a></h3> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span>
<span class=normal>17</span>
<span class=normal>18</span>
<span class=normal>19</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp># </span>Get OSD Pods
<span class=gp># </span>This uses the example/default cluster name <span class=s2>&quot;rook&quot;</span>
<span class=go>OSD_PODS=$(kubectl get pods --all-namespaces -l \</span>
<span class=go>  app=rook-ceph-osd,rook_cluster=rook-ceph -o jsonpath=&#39;{.items[*].metadata.name}&#39;)</span>

<span class=gp># </span>Find node and drive associations from OSD pods
<span class=go>for pod in $(echo ${OSD_PODS})</span>
<span class=go>do</span>
<span class=go> echo &quot;Pod:  ${pod}&quot;</span>
<span class=go> echo &quot;Node: $(kubectl -n rook-ceph get pod ${pod} -o jsonpath=&#39;{.spec.nodeName}&#39;)&quot;</span>
<span class=go> kubectl -n rook-ceph exec ${pod} -- sh -c &#39;\</span>
<span class=go>  for i in /var/lib/ceph/osd/ceph-*; do</span>
<span class=go>    [ -f ${i}/ready ] || continue</span>
<span class=go>    echo -ne &quot;-$(basename ${i}) &quot;</span>
<span class=go>    echo $(lsblk -n -o NAME,SIZE ${i}/block 2&gt; /dev/null || \</span>
<span class=go>    findmnt -n -v -o SOURCE,SIZE -T ${i}) $(cat ${i}/type)</span>
<span class=go>  done | sort -V</span>
<span class=go>  echo&#39;</span>
<span class=go>done</span>
</code></pre></div> </td></tr></table> <p>The output should look something like this.</p> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span>
<span class=normal>17</span>
<span class=normal>18</span>
<span class=normal>19</span>
<span class=normal>20</span>
<span class=normal>21</span>
<span class=normal>22</span>
<span class=normal>23</span>
<span class=normal>24</span>
<span class=normal>25</span>
<span class=normal>26</span>
<span class=normal>27</span>
<span class=normal>28</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>Pod:  osd-m2fz2
Node: node1.zbrbdl
-osd0  sda3  557.3G  bluestore
-osd1  sdf3  110.2G  bluestore
-osd2  sdd3  277.8G  bluestore
-osd3  sdb3  557.3G  bluestore
-osd4  sde3  464.2G  bluestore
-osd5  sdc3  557.3G  bluestore

Pod:  osd-nxxnq
Node: node3.zbrbdl
-osd6   sda3  110.7G  bluestore
-osd17  sdd3  1.8T    bluestore
-osd18  sdb3  231.8G  bluestore
-osd19  sdc3  231.8G  bluestore

Pod:  osd-tww1h
Node: node2.zbrbdl
-osd7   sdc3  464.2G  bluestore
-osd8   sdj3  557.3G  bluestore
-osd9   sdf3  66.7G   bluestore
-osd10  sdd3  464.2G  bluestore
-osd11  sdb3  147.4G  bluestore
-osd12  sdi3  557.3G  bluestore
-osd13  sdk3  557.3G  bluestore
-osd14  sde3  66.7G   bluestore
-osd15  sda3  110.2G  bluestore
-osd16  sdh3  135.1G  bluestore
</code></pre></div> </td></tr></table> </blockquote> <h2 id=separate-storage-groups>Separate Storage Groups<a class=headerlink href=#separate-storage-groups title="Permanent link">&para;</a></h2> <blockquote> <p><strong>DEPRECATED</strong>: Instead of manually needing to set this, the <code>deviceClass</code> property can be used on Pool structures in <code>CephBlockPool</code>, <code>CephFilesystem</code> and <code>CephObjectStore</code> CRD objects.</p> </blockquote> <p>By default Rook/Ceph puts all storage under one replication rule in the CRUSH<br> Map which provides the maximum amount of storage capacity for a cluster. If you<br> would like to use different storage endpoints for different purposes, you'll<br> have to create separate storage groups.</p> <p>In the following example we will separate SSD drives from spindle-based drives,<br> a common practice for those looking to target certain workloads onto faster<br> (database) or slower (file archive) storage.</p> <h2 id=configuring-pools>Configuring Pools<a class=headerlink href=#configuring-pools title="Permanent link">&para;</a></h2> <h3 id=placement-group-sizing>Placement Group Sizing<a class=headerlink href=#placement-group-sizing title="Permanent link">&para;</a></h3> <blockquote> <p><strong>NOTE</strong>: Since Ceph Nautilus (v14.x), you can use the Ceph MGR <code>pg_autoscaler</code><br> module to auto scale the PGs as needed. If you want to enable this feature,<br> please refer to <a href=../ceph-configuration/#default-pg-and-pgp-counts>Default PG and PGP counts</a>.</p> </blockquote> <p>The general rules for deciding how many PGs your pool(s) should contain is:</p> <ul> <li>Less than 5 OSDs set pg_num to 128</li> <li>Between 5 and 10 OSDs set pg_num to 512</li> <li>Between 10 and 50 OSDs set pg_num to 1024</li> </ul> <p>If you have more than 50 OSDs, you need to understand the tradeoffs and how to<br> calculate the pg_num value by yourself. For calculating pg_num yourself please<br> make use of <a href=http://ceph.com/pgcalc/ >the pgcalc tool</a>.</p> <p>If you're already using a pool it is generally safe to <a href=#setting-pg-count>increase its PG<br> count</a> on-the-fly. Decreasing the PG count is not<br> recommended on a pool that is in use. The safest way to decrease the PG count<br> is to back-up the data, <a href=#deleting-a-pool>delete the pool</a>, and <a href=#creating-a-pool>recreate<br> it</a>. With backups you can try a few potentially unsafe<br> tricks for live pools, documented<br> <a href=http://cephnotes.ksperis.com/blog/2015/04/15/ceph-pool-migration>here</a>.</p> <h3 id=setting-pg-count>Setting PG Count<a class=headerlink href=#setting-pg-count title="Permanent link">&para;</a></h3> <p>Be sure to read the <a href=#placement-group-sizing>placement group sizing</a> section<br> before changing the number of PGs.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp># </span>Set the number of PGs <span class=k>in</span> the rbd pool to <span class=m>512</span>
<span class=go>ceph osd pool set rbd pg_num 512</span>
</code></pre></div> </td></tr></table> <h2 id=custom-cephconf-settings>Custom ceph.conf Settings<a class=headerlink href=#custom-cephconf-settings title="Permanent link">&para;</a></h2> <blockquote> <p><strong>WARNING</strong>: The advised method for controlling Ceph configuration is to manually use the Ceph CLI<br> or the Ceph dashboard because this offers the most flexibility. It is highly recommended that this<br> only be used when absolutely necessary and that the <code>config</code> be reset to an empty string if/when the<br> configurations are no longer necessary. Configurations in the config file will make the Ceph cluster<br> less configurable from the CLI and dashboard and may make future tuning or debugging difficult.</p> </blockquote> <p>Setting configs via Ceph's CLI requires that at least one mon be available for the configs to be<br> set, and setting configs via dashboard requires at least one mgr to be available. Ceph may also have<br> a small number of very advanced settings that aren't able to be modified easily via CLI or<br> dashboard. In order to set configurations before monitors are available or to set problematic<br> configuration settings, the <code>rook-config-override</code> ConfigMap exists, and the <code>config</code> field can be<br> set with the contents of a <code>ceph.conf</code> file. The contents will be propagated to all mon, mgr, OSD,<br> MDS, and RGW daemons as an <code>/etc/ceph/ceph.conf</code> file.</p> <blockquote> <p><strong>WARNING</strong>: Rook performs no validation on the config, so the validity of the settings is the<br> user's responsibility.</p> </blockquote> <p>If the <code>rook-config-override</code> ConfigMap is created before the cluster is started, the Ceph daemons<br> will automatically pick up the settings. If you add the settings to the ConfigMap after the cluster<br> has been initialized, each daemon will need to be restarted where you want the settings applied:</p> <ul> <li>mons: ensure all three mons are online and healthy before restarting each mon pod, one at a time.</li> <li>mgrs: the pods are stateless and can be restarted as needed, but note that this will disrupt the<br> Ceph dashboard during restart.</li> <li>OSDs: restart your the pods by deleting them, one at a time, and running <code>ceph -s</code><br> between each restart to ensure the cluster goes back to "active/clean" state.</li> <li>RGW: the pods are stateless and can be restarted as needed.</li> <li>MDS: the pods are stateless and can be restarted as needed.</li> </ul> <p>After the pod restart, the new settings should be in effect. Note that if the ConfigMap in the Ceph<br> cluster's namespace is created before the cluster is created, the daemons will pick up the settings<br> at first launch.</p> <p>To automate the restart of the Ceph daemon pods, you will need to trigger an update to the pod specs.<br> The simplest way to trigger the update is to add <a href=../ceph-cluster-crd/#annotations-and-labels>annotations or labels</a><br> to the CephCluster CR for the daemons you want to restart. The operator will then proceed with a rolling<br> update, similar to any other update to the cluster.</p> <h3 id=example>Example<a class=headerlink href=#example title="Permanent link">&para;</a></h3> <p>In this example we will set the default pool <code>size</code> to two, and tell OSD<br> daemons not to change the weight of OSDs on startup.</p> <blockquote> <p><strong>WARNING</strong>: Modify Ceph settings carefully. You are leaving the sandbox tested by Rook.<br> Changing the settings could result in unhealthy daemons or even data loss if used incorrectly.</p> </blockquote> <p>When the Rook Operator creates a cluster, a placeholder ConfigMap is created that<br> will allow you to override Ceph configuration settings. When the daemon pods are started, the<br> settings specified in this ConfigMap will be merged with the default settings<br> generated by Rook.</p> <p>The default override settings are blank. Cutting out the extraneous properties,<br> we would see the following defaults after creating a cluster:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph get ConfigMap rook-config-override -o yaml</span>
</code></pre></div> </td></tr></table> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span>
<span class=normal>6</span>
<span class=normal>7</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span><span class=w></span>
<span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-config-override</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>data</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>config</span><span class=p>:</span><span class=w> </span><span class=s>&quot;&quot;</span><span class=w></span>
</code></pre></div> </td></tr></table> <p>To apply your desired configuration, you will need to update this ConfigMap. The next time the<br> daemon pod(s) start, they will use the updated configs.</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph edit configmap rook-config-override</span>
</code></pre></div> </td></tr></table> <p>Modify the settings and save. Each line you add should be indented from the <code>config</code> property as such:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class=w></span>
<span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span><span class=w></span>
<span class=nt>metadata</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-config-override</span><span class=w></span>
<span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w></span>
<span class=nt>data</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>config</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">|</span><span class=w></span>
<span class=w>    </span><span class=no>[global]</span><span class=w></span>
<span class=w>    </span><span class=no>osd crush update on start = false</span><span class=w></span>
<span class=w>    </span><span class=no>osd pool default size = 2</span><span class=w></span>
</code></pre></div> </td></tr></table> <h2 id=custom-csi-cephconf-settings>Custom CSI ceph.conf Settings<a class=headerlink href=#custom-csi-cephconf-settings title="Permanent link">&para;</a></h2> <blockquote> <p><strong>WARNING</strong>: It is highly recommended to use the default setting that comes with<br> CephCSI and this can only be used when absolutely necessary.<br> The <code>ceph.conf</code> should be reset back to default values if/when the configurations are no<br> longer necessary.</p> </blockquote> <p>If the <code>csi-ceph-conf-override</code> ConfigMap is created before the cluster is<br> started, the CephCSI pods will automatically pick up the settings. If you<br> add the settings to the ConfigMap after the cluster has been initialized,<br> you can restart the Rook operator pod and wait for Rook to recreate CSI pods<br> to take immediate effect.</p> <p>After the CSI pods are restarted, the new settings should be in effect.</p> <h3 id=example_1>Example<a class=headerlink href=#example_1 title="Permanent link">&para;</a></h3> <p>In this <a href="https://github.com/rook/rook/tree/{{ branchName }}/deploy/csi-ceph-conf-override.yaml">Example</a> we<br> will set the <code>rbd_validate_pool</code> to <code>false</code> to skip rbd pool validation.</p> <blockquote> <p><strong>WARNING</strong>: Modify Ceph settings carefully to avoid modifying the default<br> configuration.<br> Changing the settings could result in unexpected results if used incorrectly.</p> </blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl create -f csi-ceph-conf-override.yaml</span>
</code></pre></div> </td></tr></table> <p>Restart the Rook operator pod and wait for CSI pods to be recreated.</p> <h2 id=osd-crush-settings>OSD CRUSH Settings<a class=headerlink href=#osd-crush-settings title="Permanent link">&para;</a></h2> <p>A useful view of the <a href=http://docs.ceph.com/docs/master/rados/operations/crush-map/ >CRUSH Map</a><br> is generated with the following command:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd tree</span>
</code></pre></div> </td></tr></table> <p>In this section we will be tweaking some of the values seen in the output.</p> <h3 id=osd-weight>OSD Weight<a class=headerlink href=#osd-weight title="Permanent link">&para;</a></h3> <p>The CRUSH weight controls the ratio of data that should be distributed to each<br> OSD. This also means a higher or lower amount of disk I/O operations for an OSD<br> with higher/lower weight, respectively.</p> <p>By default OSDs get a weight relative to their storage capacity, which maximizes<br> overall cluster capacity by filling all drives at the same rate, even if drive<br> sizes vary. This should work for most use-cases, but the following situations<br> could warrant weight changes:</p> <ul> <li>Your cluster has some relatively slow OSDs or nodes. Lowering their weight can<br> reduce the impact of this bottleneck.</li> <li>You're using bluestore drives provisioned with Rook v0.3.1 or older. In this<br> case you may notice OSD weights did not get set relative to their storage<br> capacity. Changing the weight can fix this and maximize cluster capacity.</li> </ul> <p>This example sets the weight of osd.0 which is 600GiB</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd crush reweight osd.0 .600</span>
</code></pre></div> </td></tr></table> <h3 id=osd-primary-affinity>OSD Primary Affinity<a class=headerlink href=#osd-primary-affinity title="Permanent link">&para;</a></h3> <p>When pools are set with a size setting greater than one, data is replicated<br> between nodes and OSDs. For every chunk of data a Primary OSD is selected to be<br> used for reading that data to be sent to clients. You can control how likely it<br> is for an OSD to become a Primary using the Primary Affinity setting. This is<br> similar to the OSD weight setting, except it only affects reads on the storage<br> device, not capacity or writes.</p> <p>In this example we will make sure <code>osd.0</code> is only selected as Primary if all<br> other OSDs holding replica data are unavailable:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd primary-affinity osd.0 0</span>
</code></pre></div> </td></tr></table> <h2 id=osd-dedicated-network>OSD Dedicated Network<a class=headerlink href=#osd-dedicated-network title="Permanent link">&para;</a></h2> <p>It is possible to configure ceph to leverage a dedicated network for the OSDs to<br> communicate across. A useful overview is the <a href=http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/#ceph-networks>CEPH Networks</a><br> section of the Ceph documentation. If you declare a cluster network, OSDs will<br> route heartbeat, object replication and recovery traffic over the cluster<br> network. This may improve performance compared to using a single network.</p> <p>Two changes are necessary to the configuration to enable this capability:</p> <h3 id=use-hostnetwork-in-the-rook-ceph-cluster-configuration>Use hostNetwork in the rook ceph cluster configuration<a class=headerlink href=#use-hostnetwork-in-the-rook-ceph-cluster-configuration title="Permanent link">&para;</a></h3> <p>Enable the <code>hostNetwork</code> setting in the <a href=../ceph-cluster-crd/#samples>Ceph Cluster CRD configuration</a>.<br> For example,</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=w>  </span><span class=nt>network</span><span class=p>:</span><span class=w></span>
<span class=w>    </span><span class=nt>provider</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">host</span><span class=w></span>
</code></pre></div> </td></tr></table> <blockquote> <p>IMPORTANT: Changing this setting is not supported in a running Rook cluster. Host networking<br> should be configured when the cluster is first created.</p> </blockquote> <h3 id=define-the-subnets-to-use-for-public-and-private-osd-networks>Define the subnets to use for public and private OSD networks<a class=headerlink href=#define-the-subnets-to-use-for-public-and-private-osd-networks title="Permanent link">&para;</a></h3> <p>Edit the <code>rook-config-override</code> configmap to define the custom network<br> configuration:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>kubectl -n rook-ceph edit configmap rook-config-override</span>
</code></pre></div> </td></tr></table> <p>In the editor, add a custom configuration to instruct ceph which subnet is the<br> public network and which subnet is the private network. For example:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span>
<span class=normal>6</span>
<span class=normal>7</span>
<span class=normal>8</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class=w></span>
<span class=nt>data</span><span class=p>:</span><span class=w></span>
<span class=w>  </span><span class=nt>config</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">|</span><span class=w></span>
<span class=w>    </span><span class=no>[global]</span><span class=w></span>
<span class=w>    </span><span class=no>public network =  10.0.7.0/24</span><span class=w></span>
<span class=w>    </span><span class=no>cluster network = 10.0.10.0/24</span><span class=w></span>
<span class=w>    </span><span class=no>public addr = &quot;&quot;</span><span class=w></span>
<span class=w>    </span><span class=no>cluster addr = &quot;&quot;</span><span class=w></span>
</code></pre></div> </td></tr></table> <p>After applying the updated rook-config-override configmap, it will be necessary<br> to restart the OSDs by deleting the OSD pods in order to apply the change.<br> Restart the OSD pods by deleting them, one at a time, and running ceph -s<br> between each restart to ensure the cluster goes back to "active/clean" state.</p> <h2 id=phantom-osd-removal>Phantom OSD Removal<a class=headerlink href=#phantom-osd-removal title="Permanent link">&para;</a></h2> <p>If you have OSDs in which are not showing any disks, you can remove those "Phantom OSDs" by following the instructions below.<br> To check for "Phantom OSDs", you can run:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd tree</span>
</code></pre></div> </td></tr></table> <p>An example output looks like this:</p> <blockquote> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span>
<span class=normal>5</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code>ID  CLASS WEIGHT  TYPE NAME STATUS REWEIGHT PRI-AFF
-1       57.38062 root default
-13        7.17258     host node1.example.com
2   hdd  3.61859         osd.2                up  1.00000 1.00000
-7              0     host node2.example.com   down    0    1.00000
</code></pre></div> </td></tr></table> </blockquote> <p>The host <code>node2.example.com</code> in the output has no disks, so it is most likely a "Phantom OSD".</p> <p>Now to remove it, use the ID in the first column of the output and replace <code>&lt;ID&gt;</code> with it. In the example output above the ID would be <code>-7</code>.<br> The commands are:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=gp>$ </span>ceph osd out &lt;ID&gt;
<span class=gp>$ </span>ceph osd crush remove osd.&lt;ID&gt;
<span class=gp>$ </span>ceph auth del osd.&lt;ID&gt;
<span class=gp>$ </span>ceph osd rm &lt;ID&gt;
</code></pre></div> </td></tr></table> <p>To recheck that the Phantom OSD was removed, re-run the following command and check if the OSD with the ID doesn't show up anymore:</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>ceph osd tree</span>
</code></pre></div> </td></tr></table> <h2 id=auto-expansion-of-osds>Auto Expansion of OSDs<a class=headerlink href=#auto-expansion-of-osds title="Permanent link">&para;</a></h2> <h3 id=prerequisites_1>Prerequisites<a class=headerlink href=#prerequisites_1 title="Permanent link">&para;</a></h3> <p>1) A <a href=../ceph-cluster-crd/#pvc-based-cluster>PVC-based cluster</a> deployed in dynamic provisioning environment with a <code>storageClassDeviceSet</code>.</p> <p>2) Create the Rook <a href=../ceph-toolbox/ >Toolbox</a>.</p> <blockquote> <p>Note: <a href=../ceph-monitoring/#prometheus-operator>Prometheus Operator</a> and <a href=../ceph-monitoring/#prometheus-instances>Prometheus Instances</a> are Prerequisites that are created by the auto-grow-storage script.</p> </blockquote> <h3 id=to-scale-osds-vertically>To scale OSDs Vertically<a class=headerlink href=#to-scale-osds-vertically title="Permanent link">&para;</a></h3> <p>Run the following script to auto-grow the size of OSDs on a PVC-based Rook-Ceph cluster whenever the OSDs have reached the storage near-full threshold.<br> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>tests/scripts/auto-grow-storage.sh size  --max maxSize --growth-rate percent</span>
</code></pre></div> </td></tr></table></p> <blockquote> <p>growth-rate percentage represents the percent increase you want in the OSD capacity and maxSize represent the maximum disk size.</p> </blockquote> <p>For example, if you need to increase the size of OSD by 30% and max disk size is 1Ti<br> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>./auto-grow-storage.sh size  --max 1Ti --growth-rate 30</span>
</code></pre></div> </td></tr></table></p> <h3 id=to-scale-osds-horizontally>To scale OSDs Horizontally<a class=headerlink href=#to-scale-osds-horizontally title="Permanent link">&para;</a></h3> <p>Run the following script to auto-grow the number of OSDs on a PVC-based Rook-Ceph cluster whenever the OSDs have reached the storage near-full threshold.<br> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>tests/scripts/auto-grow-storage.sh count --max maxCount --count rate</span>
</code></pre></div> </td></tr></table></p> <blockquote> <p>Count of OSD represents the number of OSDs you need to add and maxCount represents the number of disks a storage cluster will support.</p> </blockquote> <p>For example, if you need to increase the number of OSDs by 3 and maxCount is 10<br> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=go>./auto-grow-storage.sh count --max 10 --count 3</span>
</code></pre></div> </td></tr></table></p> </article> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> </div> </div> <a href=# class="md-top md-top--hidden md-icon" data-md-component=top> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../authenticated-registry/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Authenticated Registries" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Authenticated Registries </div> </div> </a> <a href=../ceph-block/ class="md-footer__link md-footer__link--next" aria-label="Next: Block Storage" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> Block Storage </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> <a href=/ class=logo> <img src=/images/rook-logo-small.svg alt=rook.io> </a> <p> &#169; Rook Authors {{ site.time | date: '%Y' }}. Documentation distributed under <a href=https://creativecommons.org/licenses/by/4.0>CC-BY-4.0</a>. </p> <p> &#169; {{ site.time | date: '%Y' }} The Linux Foundation. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage/ >Trademark Usage</a> page. </p> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs Insiders </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.tabs.link", "instant", "navigation.expand", "navigation.top", "navigation.tracking", "privacy", "search.highlight", "search.share", "search.suggest", "tabs"], "search": "../assets/javascripts/workers/search.5c9dbbf3.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../assets/javascripts/bundle.10bf1588.min.js></script> </body> </html>